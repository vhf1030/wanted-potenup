### output parser
클라이언트 <-> AI agent(LLM)

fastapi 사용 시 uvicorn을 같이 사용하는 이유: 비동기 처리를 위해

캐싱을 활용하여 반복적인 동일한 요청을 처리하여 비용 절감 필요

기업에서 사용할 때:
1. 오픈 LLM에 회사의 데이터를 학습시킴
2. 랭체인에 연결시켜서 활용

### Ollama

1. ggml-model-Q4_K_M.gguf 다운로드
	https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF/tree/main
2. Modelfile 작성
```
FROM ggml-model-Q4_K_M.gguf

TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER stop <s>
PARAMETER stop </s>
```

models
├─ ggml-model-Q4_K_M.gguf
└─ Modelfile
3. ollama 모델 생성 
	$ ollama create EEVE-Korean-10.8B -f EEVE-Korean-Instruct-10.8B-v1.0-GGUF/Modelfile

ollama run EEVE-Korean-10.8B:latest
