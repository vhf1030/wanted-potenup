{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=data_transform)\n",
    "test_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data - X: (50000, 32, 32, 3) Y: 50000\n",
      "Test Data - X: (10000, 32, 32, 3) Y: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data - X: {train_data.data.shape} Y: {len(train_data.targets)}\")\n",
    "print(f\"Test Data - X: {test_data.data.shape} Y: {len(test_data.targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_integrity', '_format_transform_repr', '_is_protocol', '_load_meta', '_repr_indent', 'base_folder', 'class_to_idx', 'classes', 'data', 'download', 'extra_repr', 'filename', 'meta', 'root', 'target_transform', 'targets', 'test_list', 'tgz_md5', 'train', 'train_list', 'transform', 'transforms', 'url']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(dir(train_data))\n",
    "categories = train_data.classes\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 데이터 1개 그림 그리기 \n",
    "image = train_data.data[0]\n",
    "y = train_data.targets[0]\n",
    "print(y)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"{categories[y]}({y})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 개수: 40000\n",
      "Validation Data 개수: 10000\n",
      "Test Data 개수: 10000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_data, val_data = random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "print(f\"Train Data 개수: {len(train_data)}\")\n",
    "print(f\"Validation Data 개수: {len(val_data)}\")\n",
    "print(f\"Test Data 개수: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 배치 개수: 1250\n",
      "Validation Data 배치 개수: 313\n",
      "Test Data 배치 개수: 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Train Data 배치 개수: {len(train_loader)}\")\n",
    "print(f\"Validation Data 배치 개수: {len(val_loader)}\")\n",
    "print(f\"Test Data 배치 개수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.vgg import vgg16\n",
    "\n",
    "model = vgg16(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.vgg import vgg16\n",
    "\n",
    "model = vgg16(weights='IMAGENET1K_V1')\n",
    "# model = vgg16(pretrained=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-16            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
      "             ReLU-19            [-1, 512, 4, 4]               0\n",
      "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-23            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
      "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.84\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 532.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchsummary import summary \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-16            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 512, 4, 4]       1,180,160\n",
      "             ReLU-19            [-1, 512, 4, 4]               0\n",
      "           Conv2d-20            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "             ReLU-23            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-24            [-1, 512, 2, 2]               0\n",
      "           Conv2d-25            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-26            [-1, 512, 2, 2]               0\n",
      "           Conv2d-27            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-28            [-1, 512, 2, 2]               0\n",
      "           Conv2d-29            [-1, 512, 2, 2]       2,359,808\n",
      "             ReLU-30            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-31            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.84\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 532.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchsummary import summary \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "myclassifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=4096, out_features=10, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = myclassifier\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 2.3589, Train Acc: 10.04% | Val Loss: 2.3064, Val Acc: 9.84%\n",
      "개선 없음 - Early Stopping 카운터: 1/10\n",
      "Epoch: 1\tTrain Loss: 2.3029, Train Acc: 9.80% | Val Loss: 2.3064, Val Acc: 10.03%\n",
      "개선 없음 - Early Stopping 카운터: 2/10\n",
      "Epoch: 2\tTrain Loss: 2.3028, Train Acc: 9.94% | Val Loss: 2.3064, Val Acc: 9.69%\n",
      "Epoch: 3\tTrain Loss: 2.3028, Train Acc: 9.93% | Val Loss: 2.3064, Val Acc: 9.80%\n",
      "개선 없음 - Early Stopping 카운터: 1/10\n",
      "Epoch: 4\tTrain Loss: 2.3028, Train Acc: 9.94% | Val Loss: 2.3065, Val Acc: 9.84%\n",
      "개선 없음 - Early Stopping 카운터: 2/10\n",
      "Epoch: 5\tTrain Loss: 2.3028, Train Acc: 9.74% | Val Loss: 2.3064, Val Acc: 9.69%\n",
      "개선 없음 - Early Stopping 카운터: 3/10\n",
      "Epoch: 6\tTrain Loss: 2.3028, Train Acc: 9.96% | Val Loss: 2.3064, Val Acc: 9.69%\n",
      "개선 없음 - Early Stopping 카운터: 4/10\n",
      "Epoch: 7\tTrain Loss: 2.3028, Train Acc: 9.67% | Val Loss: 2.3065, Val Acc: 9.91%\n",
      "개선 없음 - Early Stopping 카운터: 5/10\n",
      "Epoch: 8\tTrain Loss: 2.3028, Train Acc: 9.99% | Val Loss: 2.3065, Val Acc: 9.69%\n",
      "개선 없음 - Early Stopping 카운터: 6/10\n",
      "Epoch: 9\tTrain Loss: 2.3028, Train Acc: 10.10% | Val Loss: 2.3064, Val Acc: 9.80%\n",
      "개선 없음 - Early Stopping 카운터: 7/10\n",
      "Epoch: 10\tTrain Loss: 2.3028, Train Acc: 10.01% | Val Loss: 2.3065, Val Acc: 9.84%\n",
      "Epoch: 11\tTrain Loss: 2.3028, Train Acc: 9.45% | Val Loss: 2.3063, Val Acc: 10.03%\n",
      "개선 없음 - Early Stopping 카운터: 1/10\n",
      "Epoch: 12\tTrain Loss: 2.3028, Train Acc: 9.70% | Val Loss: 2.3065, Val Acc: 9.69%\n",
      "개선 없음 - Early Stopping 카운터: 2/10\n",
      "Epoch: 13\tTrain Loss: 2.3028, Train Acc: 9.81% | Val Loss: 2.3064, Val Acc: 9.69%\n",
      "개선 없음 - Early Stopping 카운터: 3/10\n",
      "Epoch: 14\tTrain Loss: 2.3028, Train Acc: 9.77% | Val Loss: 2.3065, Val Acc: 9.91%\n",
      "개선 없음 - Early Stopping 카운터: 4/10\n",
      "Epoch: 15\tTrain Loss: 2.3028, Train Acc: 9.73% | Val Loss: 2.3065, Val Acc: 9.84%\n",
      "개선 없음 - Early Stopping 카운터: 5/10\n",
      "Epoch: 16\tTrain Loss: 2.3028, Train Acc: 9.88% | Val Loss: 2.3065, Val Acc: 9.84%\n",
      "개선 없음 - Early Stopping 카운터: 6/10\n",
      "Epoch: 17\tTrain Loss: 2.3027, Train Acc: 9.87% | Val Loss: 2.3065, Val Acc: 9.91%\n",
      "개선 없음 - Early Stopping 카운터: 7/10\n",
      "Epoch: 18\tTrain Loss: 2.3028, Train Acc: 9.78% | Val Loss: 2.3064, Val Acc: 9.84%\n",
      "개선 없음 - Early Stopping 카운터: 8/10\n",
      "Epoch: 19\tTrain Loss: 2.3028, Train Acc: 9.98% | Val Loss: 2.3066, Val Acc: 9.84%\n",
      "개선 없음 - Early Stopping 카운터: 9/10\n",
      "Epoch: 20\tTrain Loss: 2.3028, Train Acc: 9.98% | Val Loss: 2.3066, Val Acc: 9.69%\n",
      "개선 없음 - Early Stopping 카운터: 10/10\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import torch.optim as optim \n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_history = {\"train\": [], \"val\": []}\n",
    "acc_history = {\"train\": [], \"val\": []}\n",
    "\n",
    "best_loss_val = float('inf')\n",
    "patience = 10\n",
    "patience_cnt = 0\n",
    "\n",
    "log_dir = f\"log/cifar10/{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #### train ####\n",
    "    model.train() \n",
    "\n",
    "    loss_train = 0.0\n",
    "    corr_train = 0\n",
    "    for data, target in train_loader:\n",
    "        # GPU 보내기\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # 학습\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(data)\n",
    "        loss = criterion(yhat, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss, correct 계산\n",
    "        loss_train += loss.item() * batch_size\n",
    "        corr_train += (yhat.argmax(dim=1) == target).sum().item()\n",
    "\n",
    "    # loss, acc 저장\n",
    "    train_loss = loss_train / len(train_data)\n",
    "    train_accuracy = corr_train / len(train_data)\n",
    "    \n",
    "    loss_history[\"train\"].append(train_loss)\n",
    "    acc_history[\"train\"].append(train_accuracy)\n",
    "\n",
    "    #### validation ####\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0.0\n",
    "    corr_val = 0\n",
    "    for data, target in val_loader:\n",
    "        # GPU 보내기\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # 예측\n",
    "        with torch.no_grad():\n",
    "            pred = model(data)\n",
    "            loss_val += criterion(pred, target).item() * batch_size\n",
    "            corr_val += (pred.argmax(dim=1) == target).sum().item()\n",
    "\n",
    "    # loss, acc 저장\n",
    "    val_loss = loss_val / len(val_data)\n",
    "    val_accuracy = corr_val / len(val_data)\n",
    "    \n",
    "    loss_history[\"val\"].append(val_loss)\n",
    "    acc_history[\"val\"].append(val_accuracy)\n",
    "    \n",
    "    writer.add_scalars(\"Loss\", {\n",
    "        \"Train\": train_loss,\n",
    "        \"Validation\": val_loss,\n",
    "        },\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    writer.add_scalars(\"Accuracy\", {\n",
    "        \"Train\": train_accuracy,\n",
    "        \"Validation\": val_accuracy,\n",
    "        },\n",
    "        epoch\n",
    "    )\n",
    "\n",
    "    # Earlyl Stopping\n",
    "    if loss_val < best_loss_val:\n",
    "        patience_cnt = 0\n",
    "        best_loss_val = loss_val\n",
    "        torch.save(model.state_dict(), f\"vgg_16_{epoch}.pth\")\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        print(f\"개선 없음 - Early Stopping 카운터: {patience_cnt}/{patience}\")\n",
    "        if patience_cnt == patience:\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "    \n",
    "    # 출력\n",
    "    # if epoch % 1 == 0:\n",
    "        # print(f\"Epoch: {epoch}, Train Loss: {loss_train / len(train_data):.6f}, Train acc: {corr_train / len(train_data):.6f} Validation Loss: {loss_val / len(val_loader):.4f}, Validation acc: {corr_val / len(val_data):.4f}\")\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\\t\"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {100 * train_accuracy:.2f}% | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {100 * val_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': ['Accuracy'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
      "Epoch: 0, Accuracy: 0.10034999996423721\n",
      "Epoch: 1, Accuracy: 0.09799999743700027\n",
      "Epoch: 2, Accuracy: 0.09942500293254852\n",
      "Epoch: 3, Accuracy: 0.09925000369548798\n",
      "Epoch: 4, Accuracy: 0.09942500293254852\n",
      "Epoch: 5, Accuracy: 0.09740000218153\n",
      "Epoch: 6, Accuracy: 0.09965000301599503\n",
      "Epoch: 7, Accuracy: 0.09672500193119049\n",
      "Epoch: 8, Accuracy: 0.09992499649524689\n",
      "Epoch: 9, Accuracy: 0.10095000267028809\n",
      "Epoch: 10, Accuracy: 0.10007499903440475\n",
      "Epoch: 11, Accuracy: 0.09452500194311142\n",
      "Epoch: 12, Accuracy: 0.09702499955892563\n",
      "Epoch: 13, Accuracy: 0.09809999912977219\n",
      "Epoch: 14, Accuracy: 0.09767500311136246\n",
      "Epoch: 15, Accuracy: 0.09732499718666077\n",
      "Epoch: 16, Accuracy: 0.09884999692440033\n",
      "Epoch: 17, Accuracy: 0.09872499853372574\n",
      "Epoch: 18, Accuracy: 0.09782499819993973\n",
      "Epoch: 19, Accuracy: 0.09984999895095825\n",
      "Epoch: 20, Accuracy: 0.09984999895095825\n",
      "Epoch: 21, Accuracy: 0.09787499904632568\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "event_file_path = \"./log/cifar10/20250214-103550/Accuracy_Train/events.out.tfevents.1739497051.BOOK-6IA5E1OIQJ.652.3\"\n",
    "ea = event_accumulator.EventAccumulator(event_file_path)\n",
    "ea.Reload()\n",
    "\n",
    "print(ea.Tags())\n",
    "\n",
    "scalars = ea.Scalars('Accuracy')\n",
    "for scalar in scalars:\n",
    "    print(f\"Epoch: {scalar.step}, Accuracy: {scalar.value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 -- 이 모델의 성능을 평가하기 위한 코드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "corr_test = 0\n",
    "for data, target in test_loader:\n",
    "    # GPU 보내기\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        pred = model(data)\n",
    "        corr_test += (pred.argmax(dim=1) == target).sum().item()\n",
    "\n",
    "print(corr_test / len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
