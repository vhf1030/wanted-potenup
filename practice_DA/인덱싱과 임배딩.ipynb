{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '오늘', '기분이', '좋다', '오늘', '날씨가', '좋다', '나는', '기분이', '나쁘다']\n",
      "{'기분이', '오늘', '나는', '날씨가', '나쁘다', '좋다'}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"나는 오늘 기분이 좋다\",\n",
    "    \"오늘 날씨가 좋다\",\n",
    "    \"나는 기분이 나쁘다\"\n",
    "]\n",
    "\n",
    "# tokens라는 빈 리스트를 만든다\n",
    "tokens = []\n",
    "\n",
    "# 각 문장마다 스페이스를 기준으로 쪼갠 후 tokens에 넣는다 (.split())\n",
    "for s in sentences:\n",
    "    tokens.extend(s.split())\n",
    "print(tokens)\n",
    "\n",
    "# tokens를 집합으로 바꾼다.\n",
    "token_set = set(tokens)\n",
    "print(token_set)\n",
    "\n",
    "# tokens = []\n",
    "# for s in sentences:\n",
    "#     tokens.append(s.split())\n",
    "# print(tokens)\n",
    "# print(sum(tokens, []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 3 2 5]\n",
      "['기분이' '나는' '나쁘다' '날씨가' '오늘' '좋다']\n",
      "<class 'numpy.ndarray'>\n",
      "{0: '기분이', 4: '나는', 1: '나쁘다', 3: '날씨가', 2: '오늘', 5: '좋다'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded = label_encoder.fit_transform(list(token_set))\n",
    "print(encoded)\n",
    "print(label_encoder.classes_)\n",
    "print(type(encoded))\n",
    "\n",
    "word_info = {}\n",
    "for word, index in zip(label_encoder.classes_, encoded):\n",
    "    word_info[int(index)] = str(word)\n",
    "\n",
    "print(word_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기분이' '오늘' '나는' '날씨가' '나쁘다' '좋다']\n",
      "[['기분이']\n",
      " ['오늘']\n",
      " ['나는']\n",
      " ['날씨가']\n",
      " ['나쁘다']\n",
      " ['좋다']]\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# onehot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 집합을 배열로 바꾸기 \n",
    "token_arr_tmp = np.array(list(token_set))\n",
    "print(token_arr_tmp)\n",
    "\n",
    "# 배열 형태 바꾸기\n",
    "token_arr = token_arr_tmp.reshape(-1, 1)\n",
    "print(token_arr)\n",
    "\n",
    "oh_encoder = OneHotEncoder()\n",
    "encoded = oh_encoder.fit_transform(token_arr)\n",
    "print(encoded.toarray())\n",
    "\n",
    "# oh_encoder2 = OneHotEncoder()\n",
    "# encoded2 = oh_encoder2.fit_transform(np.array([sentences]))\n",
    "# print(encoded2.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['나는', '오늘', '기분이', '좋다'], ['오늘', '날씨가', '좋다'], ['나는', '기분이', '나쁘다']]\n",
      "['나는', '오늘', '기분이', '좋다', '오늘', '날씨가', '좋다', '나는', '기분이', '나쁘다']\n",
      "{'기분이', '오늘', '나는', '날씨가', '나쁘다', '좋다'}\n",
      "{'기분이': 2, '오늘': 2, '나는': 2, '날씨가': 1, '나쁘다': 1, '좋다': 2}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"나는 오늘 기분이 좋다\",\n",
    "    \"오늘 날씨가 좋다\",\n",
    "    \"나는 기분이 나쁘다\"\n",
    "]\n",
    "\n",
    "# sentences에서 나올 수 있는 단어들에 대해 몇번 등장하는지 count를 확인\n",
    "words_s = [s.split() for s in sentences]\n",
    "print(words_s)\n",
    "\n",
    "words_a = sum(words_s, [])\n",
    "print(words_a)\n",
    "\n",
    "words_set = set(words_a)\n",
    "print(words_set)\n",
    "\n",
    "# 딕셔너리로 출력\n",
    "words_count_dict = {w: words_a.count(w) for w in words_set}\n",
    "\n",
    "print(words_count_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 1, '오늘': 4, '기분이': 0, '좋다': 5, '날씨가': 3, '나쁘다': 2}\n",
      "[[1 1 0 0 1 1]\n",
      " [0 0 0 1 1 1]\n",
      " [1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"나는 오늘 기분이 좋다\",\n",
    "    \"오늘 날씨가 좋다\",\n",
    "    \"나는 기분이 나쁘다\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 3, '오늘': 7, '기분이': 2, '좋다': 8, '겨울이': 1, '날씨가': 5, '나쁘다': 4, '집에': 9, '가고': 0, '싶다': 6}\n",
      "[[0.         0.39655081 0.31264522 0.62529043 0.         0.\n",
      "  0.         0.31264522 0.5062265  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.70203482\n",
      "  0.         0.55349232 0.44809973 0.        ]\n",
      " [0.         0.         0.52640543 0.52640543 0.66767854 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.5417361  0.         0.         0.         0.         0.\n",
      "  0.5417361  0.         0.34578314 0.5417361 ]]\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 15 stored elements and shape (4, 10)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t0.6252904326448119\n",
      "  (0, 7)\t0.31264521632240594\n",
      "  (0, 2)\t0.31264521632240594\n",
      "  (0, 8)\t0.5062264952069911\n",
      "  (0, 1)\t0.39655081366042105\n",
      "  (1, 7)\t0.5534923152870045\n",
      "  (1, 8)\t0.4480997313625986\n",
      "  (1, 5)\t0.7020348194149619\n",
      "  (2, 3)\t0.5264054336099155\n",
      "  (2, 2)\t0.5264054336099155\n",
      "  (2, 4)\t0.6676785446095399\n",
      "  (3, 8)\t0.3457831381910465\n",
      "  (3, 9)\t0.5417361046803605\n",
      "  (3, 0)\t0.5417361046803605\n",
      "  (3, 6)\t0.5417361046803605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"나는 오늘 기분이 좋다 나는 겨울이 좋다\",\n",
    "    \"오늘 날씨가 좋다\",\n",
    "    \"나는 기분이 나쁘다\",\n",
    "    \"집에 가고 싶다 집 좋다\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(tfidf_matrix.toarray())\n",
    "print(tfidf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 15 stored elements and shape (4, 10)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t0.6252904326448119\n",
      "  (0, 7)\t0.31264521632240594\n",
      "  (0, 2)\t0.31264521632240594\n",
      "  (0, 8)\t0.5062264952069911\n",
      "  (0, 1)\t0.39655081366042105\n",
      "  (1, 7)\t0.5534923152870045\n",
      "  (1, 8)\t0.4480997313625986\n",
      "  (1, 5)\t0.7020348194149619\n",
      "  (2, 3)\t0.5264054336099155\n",
      "  (2, 2)\t0.5264054336099155\n",
      "  (2, 4)\t0.6676785446095399\n",
      "  (3, 8)\t0.3457831381910465\n",
      "  (3, 9)\t0.5417361046803605\n",
      "  (3, 0)\t0.5417361046803605\n",
      "  (3, 6)\t0.5417361046803605\n",
      "[[1.         0.39988668 0.49373442 0.17504459]\n",
      " [0.39988668 1.         0.         0.15494533]\n",
      " [0.49373442 0.         1.         0.        ]\n",
      " [0.17504459 0.15494533 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 유사도 계산\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(tfidf_matrix)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'좋다': 1, '나는': 2, '오늘': 3, '기분이': 4, '겨울이': 5, '날씨가': 6, '나쁘다': 7, '집에': 8, '가고': 9, '싶다': 10, '집': 11}\n",
      "[[2, 3, 4, 1, 2, 5, 1], [3, 6, 1], [2, 4, 7], [8, 9, 10, 11, 1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"나는 오늘 기분이 좋다 나는 겨울이 좋다\",\n",
    "    \"오늘 날씨가 좋다\",\n",
    "    \"나는 기분이 나쁘다\",\n",
    "    \"집에 가고 싶다 집 좋다\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "print(tokenizer.word_index)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 1, 2, 5, 1], [3, 6, 1], [2, 4, 7], [8, 9, 10, 11, 1]]\n",
      "[[ 4  1  2  5  1]\n",
      " [ 0  0  3  6  1]\n",
      " [ 0  0  2  4  7]\n",
      " [ 8  9 10 11  1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(sequence)\n",
    "padded_sequences = pad_sequences(sequence, maxlen=5)\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  1  2  5  1]\n",
      " [ 0  0  3  6  1]\n",
      " [ 0  0  2  4  7]\n",
      " [ 8  9 10 11  1]]\n",
      "[[1.         0.79574493 0.54436252 0.8908486 ]\n",
      " [0.79574493 1.         0.65674725 0.74655152]\n",
      " [0.54436252 0.65674725 1.         0.44617042]\n",
      " [0.8908486  0.74655152 0.44617042 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(padded_sequences)\n",
    "print(padded_sequences)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
