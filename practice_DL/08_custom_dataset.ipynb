{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset 만들기\n",
    "\n",
    "- 목표: pytorch에서 제공하는 mnist 데이터셋을 불러오는 mnist class처럼 내가 수집한 데이터를 불러올 수 있는 클래스를 만들 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, x):\n",
    "        \"\"\"\n",
    "        클래스의 인스턴스가 생성될 때 호출되는 생성자 메서드\n",
    "        객체의 초기 상태를 설정\n",
    "        \"\"\"\n",
    "        print(\"안녕\")\n",
    "        self.x = x\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        객체가 인덱싱 연산(`obj[idx]`)을 수행할 때 호출되는 특수 메서드\n",
    "        데이터셋, 리스트, 딕셔너리와 같은 자료구조에서 개별 요소에 접근할 때 사용\n",
    "        \"\"\"\n",
    "        return self.x[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        객체의 길이를 반환하는 특수 메서드로, `len(obj)` 호출 시 실행\n",
    "        데이터셋의 샘플 개수나 컨테이너 객체(리스트, 튜플 등)의 원소 개수를 정의하는 데 사용\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        객체를 문자열로 표현할 때 호출되는 특수 메서드\n",
    "        `print()` 함수 또는 `str()` 함수 호출 시 반환되는 문자열을 정의\n",
    "        \"\"\"\n",
    "        return \"안녕하세요\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕\n",
      "안녕하세요\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(x=[1, 2, 3, 4, 5])\n",
    "print(dataset)\n",
    "len(dataset)\n",
    "for d in dataset:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(32),\n",
    "        transforms.Normalize((0.5,), (1.0,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.MNIST(root=\"./\", train=True, download=True, transform=data_transform)\n",
    "test_data = datasets.MNIST(root=\"./\", train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=True)\n",
       "               Normalize(mean=(0.5,), std=(1.0,))\n",
       "           )"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data\n",
    "# init - 속성에 data, targets가 있다\n",
    "# len - 전체 데이터의 개수가 출력된다\n",
    "# getitem - index에 맞춰 (data, target)이 출력된다 - transform이 적용됨\n",
    "# str - 데이터셋에 대한 설명이 작성된다\n",
    "\n",
    "# DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# -> train_data의 인덱싱을 이용해서 랜덤으로 나눠준다는 것 을 알 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=True)\n",
      "               Normalize(mean=(0.5,), std=(1.0,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          ...,\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "          [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle data로 CustomDataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알집 풀기\n",
    "import zipfile\n",
    "\n",
    "file_path = \"../cat-and-dog.zip\"\n",
    "output_dir = \"../data/cat-and-dog\"\n",
    "with zipfile.ZipFile(file_path, \"r\") as zip:\n",
    "    zip.extractall(path=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012 1011 3204 3200 801 800\n",
      "6404 1601 2023\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "random.seed(1030)\n",
    "\n",
    "# 미션: 이미지 경로가 담긴 train_list, test_list 만들기\n",
    "# 추가 미션: train_list 만들 때 조건을 걸어서 val_list도 만들기\n",
    "\n",
    "test_dir = Path(\"../data/cat-and-dog/test_set/test_set/\")\n",
    "train_dir = Path(\"../data/cat-and-dog/training_set/training_set/\")\n",
    "\n",
    "test_dogs = [str(p) for p in test_dir.glob(\"dogs/*.jpg\")]\n",
    "test_cats = [str(p) for p in test_dir.glob(\"cats/*.jpg\")]\n",
    "\n",
    "train_dogs = [str(p) for p in train_dir.glob(\"dogs/*.jpg\")]\n",
    "train_cats = [str(p) for p in train_dir.glob(\"cats/*.jpg\")]\n",
    "\n",
    "# Train 데이터를 80:20 비율로 랜덤 샘플링\n",
    "num_val_dogs = int(len(train_dogs) * 0.2)\n",
    "num_val_cats = int(len(train_cats) * 0.2)\n",
    "\n",
    "val_dogs = random.sample(train_dogs, num_val_dogs)  # 개(dog) 검증셋\n",
    "val_cats = random.sample(train_cats, num_val_cats)  # 고양이(cat) 검증셋\n",
    "\n",
    "# Validation 데이터셋을 제외하고 남은 데이터를 Train 데이터셋으로 사용\n",
    "train_dogs = list(set(train_dogs) - set(val_dogs))\n",
    "train_cats = list(set(train_cats) - set(val_cats))\n",
    "\n",
    "print(len(test_dogs), len(test_cats), len(train_dogs), len(train_cats), len(val_dogs), len(val_cats))\n",
    "\n",
    "# 최종 리스트 생성\n",
    "test_list = test_dogs + test_cats\n",
    "train_list = train_dogs + train_cats\n",
    "val_list = val_dogs + val_cats\n",
    "\n",
    "print(len(test_list), len(train_list), len(val_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "# CustomDataSet\n",
    "# 생성자 def __init__(self, root)\n",
    "# 속성 만들기: root\n",
    "#   - 폴더경로를 입력받습니다(ex. \"cat-and-dog\")\n",
    "# 메서드 만들기: def get_datalist(self)\n",
    "#   - train_list, test_list를 클래스의 속성으로 만들어줍니다.\n",
    "\n",
    "class CustomDataset():\n",
    "    \"\"\"\n",
    "    고양이, 개 이미지를 불러오는 클래스\n",
    "    고양이: 0, 개: 1\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, train: bool, transform: transforms.Compose):\n",
    "        self.root = root\n",
    "        self.path = Path(root)\n",
    "        \n",
    "        if train == True:\n",
    "            folder_name = \"training_set\"\n",
    "        else:\n",
    "            folder_name = \"test_set\"\n",
    "            \n",
    "        self.data_list = self.get_data_list(folder_name)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.data_list[index]\n",
    "        target = 0 if \"cat\" in image_path.name else 1\n",
    "        image = Image.open(image_path)\n",
    "        data = self.transform(np.array(image))\n",
    "        return (data, target)\n",
    "    \n",
    "    def get_data_list(self, folder_name):\n",
    "        dogs = list(self.path.glob(f\"{folder_name}/*/dogs/*.jpg\"))\n",
    "        cats = list(self.path.glob(f\"{folder_name}/*/cats/*.jpg\"))\n",
    "        return dogs + cats\n",
    "    \n",
    "    \n",
    "        # # Train 데이터를 80:20 비율로 랜덤 샘플링\n",
    "        # num_val_dogs = int(len(train_dogs) * 0.2)\n",
    "        # num_val_cats = int(len(train_cats) * 0.2)\n",
    "\n",
    "        # val_dogs = random.sample(train_dogs, num_val_dogs)  # 개(dog) 검증셋\n",
    "        # val_cats = random.sample(train_cats, num_val_cats)  # 고양이(cat) 검증셋\n",
    "\n",
    "        # # Validation 데이터셋을 제외하고 남은 데이터를 Train 데이터셋으로 사용\n",
    "        # train_dogs = list(set(train_dogs) - set(val_dogs))\n",
    "        # train_cats = list(set(train_cats) - set(val_cats))\n",
    "\n",
    "        # # 최종 리스트 생성\n",
    "        # self.test_list = test_dogs + test_cats\n",
    "        # self.train_list = train_dogs + train_cats\n",
    "        # self.val_list = val_dogs + val_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cat-and-dog\n",
      "..\\data\\cat-and-dog\\training_set\\training_set\\dogs\\dog.1.jpg\n",
      "8005\n",
      "tensor([[0.9333, 0.6627, 0.5451,  ..., 0.5216, 0.5020, 0.4980],\n",
      "        [0.9804, 0.6784, 0.5216,  ..., 0.4510, 0.4353, 0.4314],\n",
      "        [0.9725, 0.6667, 0.5059,  ..., 0.3333, 0.3255, 0.3294],\n",
      "        ...,\n",
      "        [1.0000, 0.9961, 0.9882,  ..., 0.9373, 0.9333, 0.9333],\n",
      "        [1.0000, 1.0000, 0.9882,  ..., 0.9608, 0.9569, 0.9569],\n",
      "        [1.0000, 1.0000, 0.9961,  ..., 0.9804, 0.9725, 0.9765]])\n",
      "../data/cat-and-dog\n",
      "..\\data\\cat-and-dog\\test_set\\test_set\\dogs\\dog.4001.jpg\n",
      "2023\n",
      "tensor([[0.4902, 0.3020, 0.3020,  ..., 0.5490, 0.5412, 0.5451],\n",
      "        [0.5333, 0.3451, 0.3059,  ..., 0.5490, 0.5451, 0.5490],\n",
      "        [0.5686, 0.3961, 0.2980,  ..., 0.5490, 0.5451, 0.5490],\n",
      "        ...,\n",
      "        [0.6157, 0.6039, 0.6314,  ..., 0.6392, 0.6157, 0.5961],\n",
      "        [0.6157, 0.6039, 0.6314,  ..., 0.6471, 0.6157, 0.5882],\n",
      "        [0.6157, 0.6039, 0.6314,  ..., 0.6471, 0.6118, 0.5804]])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "        # RGB로 되어있는 (0~255) 이미지를 0과 1 사이의 tensor로 변환\n",
    "    ]\n",
    ")\n",
    "\n",
    "mydata = CustomDataset(\"../data/cat-and-dog\", train=True, transform=data_transform)\n",
    "print(mydata.root)\n",
    "print(mydata.data_list[0])\n",
    "print(len(mydata))\n",
    "print(mydata[0][0][0])\n",
    "\n",
    "mydata = CustomDataset(\"../data/cat-and-dog\", train=False, transform=data_transform)\n",
    "print(mydata.root)\n",
    "print(mydata.data_list[0])\n",
    "print(len(mydata))\n",
    "print(mydata[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1529, 0.1529, 0.1569,  ..., 0.8118, 0.7922, 0.7882],\n",
      "         [0.1569, 0.1569, 0.1569,  ..., 0.7961, 0.7804, 0.7725],\n",
      "         [0.1569, 0.1569, 0.1569,  ..., 0.7804, 0.7804, 0.7804],\n",
      "         ...,\n",
      "         [0.1255, 0.1176, 0.1020,  ..., 0.1412, 0.1608, 0.2235],\n",
      "         [0.1255, 0.1176, 0.1020,  ..., 0.1647, 0.1725, 0.2157],\n",
      "         [0.1255, 0.1137, 0.0980,  ..., 0.2000, 0.1647, 0.1569]],\n",
      "\n",
      "        [[0.1725, 0.1725, 0.1725,  ..., 0.8000, 0.7804, 0.7765],\n",
      "         [0.1765, 0.1765, 0.1725,  ..., 0.7843, 0.7686, 0.7608],\n",
      "         [0.1765, 0.1765, 0.1725,  ..., 0.7647, 0.7608, 0.7608],\n",
      "         ...,\n",
      "         [0.1176, 0.1098, 0.0941,  ..., 0.0902, 0.1176, 0.1804],\n",
      "         [0.1176, 0.1098, 0.0941,  ..., 0.1137, 0.1333, 0.1765],\n",
      "         [0.1176, 0.1059, 0.0902,  ..., 0.1490, 0.1255, 0.1176]],\n",
      "\n",
      "        [[0.1569, 0.1569, 0.1686,  ..., 0.6784, 0.6510, 0.6392],\n",
      "         [0.1608, 0.1608, 0.1686,  ..., 0.6627, 0.6392, 0.6235],\n",
      "         [0.1608, 0.1608, 0.1686,  ..., 0.6510, 0.6431, 0.6353],\n",
      "         ...,\n",
      "         [0.1216, 0.1137, 0.0980,  ..., 0.0667, 0.0941, 0.1569],\n",
      "         [0.1216, 0.1137, 0.0980,  ..., 0.0824, 0.0980, 0.1412],\n",
      "         [0.1216, 0.1098, 0.0941,  ..., 0.1176, 0.0902, 0.0824]]])\n"
     ]
    }
   ],
   "source": [
    "# 클래스 CustomDataSet를 활용하여 데이터를 train_data라는 변수에 저장\n",
    "# train_data[0] -> 첫 번째 train_data의 transform이 적용된 데이터와 라벨을 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
