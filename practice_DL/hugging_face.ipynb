{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìš”ì•½ëœ í…ìŠ¤íŠ¸: Artificial Intelligence (AI) is a field of computer science that aims to create machines capable of intelligent behavior. AI technology has rapidly evolved over the past decade, making significant contributions in areas such as healthcare, finance, and autonomous systems. The primary goal of AI research is to develop a deeper understanding of human intelligence and create machines that can perform complex tasks autonomously. \n",
      "\n",
      "0ë²ˆì§¸ AIì˜ ë‹µë³€: healthcare, finance, and autonomous systems\n",
      "1ë²ˆì§¸ AIì˜ ë‹µë³€: healthcare, finance, and autonomous systems.\n",
      "2ë²ˆì§¸ AIì˜ ë‹µë³€: healthcare, finance\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# âœ… ìµœì‹  ëª¨ë¸ ì„ íƒ\n",
    "summarization_model_name = \"facebook/bart-large-cnn\"\n",
    "qa_model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_model_name)\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "\n",
    "# âœ… GPU ì„¤ì • (ì—†ìœ¼ë©´ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarization_model = summarization_model.to(device)\n",
    "qa_model = qa_model.to(device)\n",
    "\n",
    "# âœ… íŒŒì´í”„ë¼ì¸ ì„¤ì • (ì¶”ë¡  ë°©ì‹ ê°œì„ )\n",
    "summarizer = pipeline(\"summarization\", model=summarization_model, tokenizer=summarization_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "question_answerer = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# âœ… ê¸´ ë¬¸ì„œ ì…ë ¥\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) is a field of computer science that aims to create machines capable of intelligent behavior.\n",
    "AI technology has rapidly evolved over the past decade, making significant contributions in areas such as healthcare, finance, and autonomous systems.\n",
    "In healthcare, AI assists doctors in diagnosing diseases and recommending personalized treatments.\n",
    "In finance, AI-driven algorithms optimize stock trading and risk management.\n",
    "AI-powered self-driving cars are revolutionizing transportation by improving safety and efficiency.\n",
    "The primary goal of AI research is to develop a deeper understanding of human intelligence and to create machines that can perform complex tasks autonomously.\n",
    "\"\"\"\n",
    "\n",
    "# âœ… í…ìŠ¤íŠ¸ ìš”ì•½ ì‹¤í–‰ (ê°œì„ ëœ ìš”ì•½)\n",
    "summary = summarizer(long_text, max_length=100, min_length=50, do_sample=True, temperature=0.7)\n",
    "summary_text = summary[0]['summary_text'].strip()\n",
    "print(\"ìš”ì•½ëœ í…ìŠ¤íŠ¸:\", summary_text, \"\\n\")\n",
    "\n",
    "# âœ… ì§ˆë¬¸ ì…ë ¥ ë° ë‹µë³€ ìƒì„± (ì§ˆë¬¸ ê°œì„ )\n",
    "question = \"What are the main applications of AI?\"\n",
    "answer = question_answerer(question=question, context=summary_text, top_k=3)\n",
    "for i, ans in enumerate(answer):\n",
    "    print(f\"{i}ë²ˆì§¸ AIì˜ ë‹µë³€:\", ans['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì„œ: Artificial intelligence is a computer system that mimics human intelligence and performs functions such as learning, inference, recognition. Recently, AI has led to innovations in many fields, particularly in the field of natural language processing, where large-scale language models have emerged that demonstrate human-like levels of language comprehension and generation, and these technologies are being used in many applications, including chatbots, translation, emotion analysis.\n",
      "ğŸ“¢ ë²ˆì—­ëœ í•œêµ­ì–´ ë¬¸ì„œ: ì¸ê³µ ì§€ëŠ¥ì€ ì¸ê°„ ì§€ëŠ¥ì„ í‰ë‚´ë‚´ê³  í•™ìŠµ, ì¶”ë¡ , ì¸ì‹ ê°™ì€ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ì»´í“¨í„° ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ìµœê·¼ AIëŠ” ë§ì€ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ ìì—° ì–¸ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë§ì´ì£ . ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì–¸ì–´ ì´í•´ì™€ ìƒì„±ì„ ë³´ì—¬ì£¼ëŠ” ëŒ€ê·œëª¨ì˜ ì–¸ì–´ ëª¨ë¸ì´ ë‚˜íƒ€ë‚˜ê³ , ì´ ê¸°ìˆ ì€ ì±„íŠ¸ë´‡, ë²ˆì—­, ê°ì • ë¶„ì„ ë“± ë§ì€ ì‘ìš©ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# ë²ˆì—­ ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ â†” ì˜ì–´)\n",
    "ko_to_en_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "en_to_ko_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "ko_to_en_tokenizer = AutoTokenizer.from_pretrained(ko_to_en_model_name)\n",
    "ko_to_en_model = AutoModelForSeq2SeqLM.from_pretrained(ko_to_en_model_name)\n",
    "\n",
    "en_to_ko_tokenizer = AutoTokenizer.from_pretrained(en_to_ko_model_name)\n",
    "en_to_ko_model = AutoModelForSeq2SeqLM.from_pretrained(en_to_ko_model_name)\n",
    "\n",
    "# GPU ì„¤ì • (ì—†ìœ¼ë©´ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ko_to_en_model = ko_to_en_model.to(device)\n",
    "en_to_ko_model = en_to_ko_model.to(device)\n",
    "\n",
    "# ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì„¤ì • (src_lang, tgt_lang ëª…ì‹œì  ì„¤ì •)\n",
    "translator_ko_to_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=ko_to_en_model,\n",
    "    tokenizer=ko_to_en_tokenizer,\n",
    "    src_lang=\"ko_KR\",  # í•œêµ­ì–´ â†’ ì˜ì–´ ë³€í™˜ ì‹œ ì›ë³¸ ì–¸ì–´\n",
    "    tgt_lang=\"en_XX\",  # ë³€í™˜í•  ëŒ€ìƒ ì–¸ì–´ (ì˜ì–´)\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "translator_en_to_ko = pipeline(\n",
    "    \"translation\",\n",
    "    model=en_to_ko_model,\n",
    "    tokenizer=en_to_ko_tokenizer,\n",
    "    src_lang=\"en_XX\",  # ì˜ì–´ â†’ í•œêµ­ì–´ ë³€í™˜ ì‹œ ì›ë³¸ ì–¸ì–´\n",
    "    tgt_lang=\"ko_KR\",  # ë³€í™˜í•  ëŒ€ìƒ ì–¸ì–´ (í•œêµ­ì–´)\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "long_text_ko = \"\"\"\n",
    "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ í•™ìŠµ, ì¶”ë¡ , ì¸ì‹ ë“±ì˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ì»´í“¨í„° ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "ìµœê·¼ AI ê¸°ìˆ ì€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.\n",
    "íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ë“±ì¥í•˜ì—¬ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì–¸ì–´ ì´í•´ ë° ìƒì„± ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì±—ë´‡, ë²ˆì—­, ê°ì • ë¶„ì„ ë“± ì—¬ëŸ¬ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: í•œêµ­ì–´ ë¬¸ì„œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­\n",
    "translated_text_en = translator_ko_to_en(long_text_ko)[0]['translation_text']\n",
    "print(\"ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì„œ:\", translated_text_en)\n",
    "\n",
    "# Step 2: ì˜ì–´ ë¬¸ì„œë¥¼ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­\n",
    "translated_text_ko = translator_en_to_ko(translated_text_en)[0]['translation_text']\n",
    "print(\"ë²ˆì—­ëœ í•œêµ­ì–´ ë¬¸ì„œ:\", translated_text_ko)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Your input_length: 700 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì„œ: But the developers also revealed in other interviews that they had a lot of requests from their users for their previous heroes to appear, so they left a lot of room for new heroes to be added. In the previous game, the character hire narrator talked about the character's appearance, characteristics, origin, etc., but in the original game, the character hire narrator talks about the character's past. Unlike in the previous game, each hero had a lot of weight in the past, and you could pay for the hero's past and the character's strength through past echoes in the rehearsal chambers that would appear at random on the map, except that all of the heroes had no known connections with the player and the instructor, and you couldn't figure out why they were in charge of keeping the torch of hope. All of the heroes except the \n",
      "\n",
      "ì˜ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸: In the previous game, the character hire narrator talked about the character's appearance, characteristics, origin, etc., but in the original game, it talked about their past. The developers also revealed in other interviews that they had a lot of requests from their users for their previous heroes to appear. \n",
      "\n",
      "í•œêµ­ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸: ì´ì „ì˜ ê²Œì„ì—ì„œëŠ” ìºë¦­í„°ì˜ ì™¸ëª¨, íŠ¹ì§•, ê¸°ì› ë“±ì— ëŒ€í•´ ì´ì•¼ê¸° í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì›ë˜ ê²Œì„ì—ì„œëŠ” ê·¸ë“¤ì˜ ê³¼ê±°ì— ëŒ€í•´ì„œ ì´ì•¼ê¸° í–ˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì€ ë‹¤ë¥¸ ì¸í„°ë·°ì—ì„œ ì´ì „ì˜ ì˜ì›…ë“¤ì´ ë‚˜íƒ€ë‚˜ê¸° ìœ„í•´ ì‚¬ìš©ìë“¤ë¡œë¶€í„° ë§ì€ ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. \n",
      "\n",
      "ë²ˆì—­ëœ ì˜ì–´ ì§ˆë¬¸: What are the characteristics of a predatory system versus a different one? \n",
      "\n",
      "AIì˜ ì²« ë²ˆì§¸ ë‹µë³€: ì™¸ëª¨, íŠ¹ì„±, ê¸°ì›\n",
      "0ë²ˆì§¸ AIì˜ ë‹µë³€: ì™¸ëª¨, íŠ¹ì„±, ê¸°ì›\n",
      "1ë²ˆì§¸ AIì˜ ë‹µë³€: ì™¸ëª¨\n",
      "2ë²ˆì§¸ AIì˜ ë‹µë³€: íŠ¹ì§•, ê¸°ì›\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# ìµœì‹  ëª¨ë¸ ì„ íƒ\n",
    "summarization_model_name = \"facebook/bart-large-cnn\"\n",
    "qa_model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_model_name)\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "\n",
    "# ë²ˆì—­ ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ â†” ì˜ì–´)\n",
    "ko_to_en_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "en_to_ko_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "ko_to_en_tokenizer = AutoTokenizer.from_pretrained(ko_to_en_model_name)\n",
    "ko_to_en_model = AutoModelForSeq2SeqLM.from_pretrained(ko_to_en_model_name)\n",
    "\n",
    "en_to_ko_tokenizer = AutoTokenizer.from_pretrained(en_to_ko_model_name)\n",
    "en_to_ko_model = AutoModelForSeq2SeqLM.from_pretrained(en_to_ko_model_name)\n",
    "\n",
    "# GPU ì„¤ì • (ì—†ìœ¼ë©´ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarization_model = summarization_model.to(device)\n",
    "qa_model = qa_model.to(device)\n",
    "ko_to_en_model = ko_to_en_model.to(device)\n",
    "en_to_ko_model = en_to_ko_model.to(device)\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì„¤ì • (src_lang, tgt_lang ëª…ì‹œì  ì„¤ì •)\n",
    "summarizer = pipeline(\"summarization\", model=summarization_model, tokenizer=summarization_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "question_answerer = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "translator_ko_to_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=ko_to_en_model,\n",
    "    tokenizer=ko_to_en_tokenizer,\n",
    "    src_lang=\"ko_KR\",\n",
    "    tgt_lang=\"en_XX\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "translator_en_to_ko = pipeline(\n",
    "    \"translation\",\n",
    "    model=en_to_ko_model,\n",
    "    tokenizer=en_to_ko_tokenizer,\n",
    "    src_lang=\"en_XX\",\n",
    "    tgt_lang=\"ko_KR\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ê¸´ ë¬¸ì„œ ì…ë ¥ (í•œêµ­ì–´)\n",
    "long_text_ko = \"\"\"\n",
    "ì „ì‘ì— ë‚˜ì˜¨ ì˜ì›…ë“¤ì˜ ê¸°ìˆ ë“¤ì€ ì „ì²´ì ìœ¼ë¡œ ì „ì‘ì˜ í˜•íƒœë¥¼ ìœ ì§€í•˜ê³  ìˆì§€ë§Œ ê¸°ìˆ  ê°œìˆ˜ê°€ 7ê°œì—ì„œ 11ê°œë¡œ ì¦ê°€í•´ ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë§ì´ ìƒê²¼ìœ¼ë©°, í† í° ì‹œìŠ¤í…œì´ ì ìš©ë˜ê³  íšŒë³µ ê³„ì—´ ìŠ¤í‚¬ë“¤ì´ ëŒ€ë¶€ë¶„ ì‹œì „ ì²´ë ¥ ì¡°ê±´ì´ë‚˜ 2~3íšŒ ì •ë„ì˜ íšŸìˆ˜ ì œí•œ ë“± í˜ë„í‹°ê°€ ìƒê²¼ìœ¼ë©° ì¥ê¸°ì ì¸ ë²„í”„ íš¨ê³¼ë„ ë§ì´ ì—†ì–´ì§„ í¸ì´ë¼ ì „ì‘ì˜ ê°ê°ìœ¼ë¡œ ë‹¨í¸ì ì¸ í™œìš©ì„ í•´ì„œëŠ” ê³ ì „í•˜ê¸° ì‹­ìƒì´ë‹¤. íŠ¹íˆ í† í° ì‹œìŠ¤í…œì— ìµìˆ™í•´ì§€ì§€ ëª»í•œë‹¤ë©´ ë”ìš± ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.\n",
    "ë˜í•œ ì „ì‘ì— ì¡´ì¬í•˜ë˜ ì‹œìŠ¤í…œì¸ ë¬´ê¸°ì™€ ë°©ì–´êµ¬ ì‹œìŠ¤í…œ ìì²´ê°€ ì‚¬ë¼ì ¸ ê³µê²© ê¸°ìˆ ì—ëŠ” ìì²´ì ìœ¼ë¡œ ê³µê²©ë ¥ì´ ë¶™ì–´ ìˆë‹¤. ë˜í•œ íšŒí”¼ìœ¨ì´ë‚˜ ëª…ì¤‘ë¥  ì—­ì‹œ í† í°ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— í† í°ì´ ì—†ë‹¤ëŠ” ì „ì œ í•˜ì— ê¸°ë³¸ì ì¸ íšŒí”¼ìœ¨ì€ 0%, ëª…ì¤‘ë¥ ì€ 100%ì´ë‹¤.\n",
    "1í¸ì—ì„œ ë“±ì¥í–ˆë˜ ì˜ì›…ë“¤ì´ ê²Œì„ ë‚´ì— ëª¨ë‘ ë“±ì¥í•˜ì§€ëŠ” ì•Šì•˜ìœ¼ë©°, ì£¼ì¸ê³µ ì¼í–‰ì„ ë°°ì‹ í•˜ì—¬ ë„ì ë“¤ê³¼ í•©ë¥˜í•œ ìœ ë¬¼ìˆ˜ì§‘ìƒ ë“± ìŠ¤í† ë¦¬ìƒìœ¼ë¡œ ì¶œì—°ì´ ë°°ì œëœ ìºë¦­í„°ë“¤ë„ ì¡´ì¬í•œë‹¤. í—ˆë‚˜ ê°œë°œì ë˜í•œ ë‹¤ë¥¸ ì¸í„°ë·°ì—ì„œ ìœ ì €ë“¤ì˜ ì „ì‘ ì˜ì›…ë“¤ì´ ì¶œì—°í–ˆìœ¼ë©´ í•˜ëŠ” ìš”ì²­ì´ ë§ì€ ê²ƒì„ ì•Œê³  ìˆì–´ ê¸°ì¡´ ìºë¦­í„°ë¥¼ ë“±ì¥í•˜ê²Œ í•  ìƒê°ì´ ìˆìŒì„ ë°í˜”ê¸°ì— ìƒˆë¡œìš´ ì˜ì›…ë“¤ì´ ì¶”ê°€ë  ì—¬ì§€ë¥¼ ë‚¨ê²¨ë‘ì—ˆë‹¤.\n",
    "ì „ì‘ì—ì„œëŠ” ì˜ì›… ê³ ìš© ì‹œ ë‚˜ë ˆì´í„°ê°€ í•´ë‹¹ ì˜ì›…ì˜ ëª¨ìŠµê³¼ íŠ¹ì§•, ì¶œì‹  ë“±ì„ ì´ì•¼ê¸°í–ˆë˜ ê²ƒê³¼ ë‹¤ë¥´ê²Œ ë³¸ì‘ì—ì„œëŠ” í•´ë‹¹ ì˜ì›…ì˜ ê³¼ê±°ì‚¬ë¥¼ ë¹„ì¶”ëŠ” ë§ì„ í•œë‹¤. ì „ì‘ê³¼ ë‹¬ë¦¬ ê° ì˜ì›…ì˜ ê³¼ê±°ì— ë¹„ì¤‘ì„ ë§ì´ ë‘ì—ˆëŠ”ë°, ë§µì—ì„œ ë¬´ì‘ìœ„ë¡œ ë“±ì¥í•˜ëŠ” íšŒê³ ì˜ ì„±ì†Œì—ì„œ ê³¼ê±°ì˜ ë©”ì•„ë¦¬ë¥¼ í†µí•´ ì˜ì›…ì˜ ê³¼ê±°ì‚¬ì™€ ìŠ¤í‚¬ì„ í•´ê¸ˆí•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ëª¨ë“  ì˜ì›…ì´ ì „ì‘ê³¼ëŠ” ë‹¬ë¦¬ í”Œë ˆì´ì–´ì™€ ìŠ¤ìŠ¹ê³¼ì˜ ì—°ê²°ì ì´ ì•Œë ¤ì ¸ ìˆì§€ ì•Šì•„, ì–´ì§¸ì„œ í¬ë§ì˜ íšƒë¶ˆì„ ì§€í‚¤ëŠ” ì…ì¥ì´ ë˜ì—ˆëŠ”ì§€ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤.\n",
    "í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ì„ ì œì™¸í•œ ì˜ì›…ë“¤ì€ ì˜ì›…ì˜ ê¸¸ì„ 4ê°œì”© ê°€ì§€ê³  ìˆëŠ”ë° í•˜ë‚˜ëŠ” ë°©ë‘ìë¼ëŠ” ê³µí†µëœ ì˜ì›…ì˜ ê¸¸ì´ê³  ë‚˜ë¨¸ì§€ 3ê°œëŠ” ì˜ì›…ë³„ë¡œ ì •í•´ì§„ íŠ¹ì„±í™”ëœ ì˜ì›…ì˜ ê¸¸ì´ë‹¤. í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ë„ ì¼ë‹¨ì€ ì˜ì›…ì˜ ê¸¸ ìì²´ëŠ” ì¡´ì¬í•˜ëŠ”ë° ì „ë¬¸ê°€ë¼ëŠ” ì˜ì›…ì˜ ê¸¸ë¡œ ê³ ì •ë˜ì–´ ìˆë‹¤. ì˜ì›…ì˜ ê¸¸ì€ í¬ë§ì˜ ì œë‹¨ì˜ ì‚´ì•„ìˆëŠ” ë„ì‹œì—ì„œ ì´›ë¶ˆë¡œ í•´ê¸ˆí•´ì•¼ í•œë‹¤. ì›ë˜ëŠ” ì›ì • ì‹œì‘ ì‹œ ì˜ì›…ì„ ê²°ì •í•˜ëŠ” 'êµì°¨ë¡œ'ì—ì„œ ì˜ì›…ê³¼ ì‚¬ìš©í•  ì˜ì›…ì˜ ê¸¸ì„ ì •í•˜ë©´ ì±•í„°ê°€ ëë‚˜ë„ë¡ ê³„ì† ê³ ì •ëœ ì˜ì›…ì˜ ê¸¸ì„ ì‚¬ìš©í•´ì•¼ í–ˆì§€ë§Œ 1.02.54580 ë²„ì „ë¶€í„° ì—¬ê´€ì—ì„œ ì˜ì›…ì˜ ê¸¸ì„ ë³€ê²½í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ê³„ê³¡ ì§€ì—­ì˜ ì²« ì—¬ê´€ì—ì„œëŠ” ìœ ë¬¼ 4, ê·¸ ë’¤ë¡œëŠ” ìœ ë¬¼ 32ê°€ í•„ìš”í•˜ë©°, ì˜ì›…ì˜ ê¸¸ ë³€ê²½ ì‹œ ê·¸ ì˜ì›…ì—ê²Œ íˆ¬ìí–ˆë˜ ìˆ™ë ¨ í¬ì¸íŠ¸ëŠ” ëª¨ë‘ ë°˜ë‚©ë°›ëŠ”ë‹¤.\n",
    "ì „ìš© ì¥ì‹ êµ¬ëŠ” ì˜ì›…ë‹¹ 3ê°œì´ë©° ì „ì‘ê³¼ ë‹¤ë¥´ê²Œ ëª¨ë‘ ê°™ì€ ë“±ê¸‰ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ ì „ìš© ì¥ì‹ êµ¬ëŠ” ì˜ì›…ì˜ ê³¼ê±°ì™€ ê´€ë ¨ì´ ìˆë‹¤. ë˜í•œ ì„ì‹œ ê³ ìš© ì˜ì›…ì¸ í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ì—ê²ŒëŠ” ì „ìš© ì—¬ê´€ ì•„ì´í…œì´ ì¡´ì¬í•˜ë©° í•´ê¸ˆì´ ë˜ì—ˆì„ ì‹œ ì—¬ê´€ì— ì…ì¥í•  ë•Œ í™•ë¥ ì ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: í•œêµ­ì–´ ë¬¸ì„œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­\n",
    "translated_text_en = translator_ko_to_en(long_text_ko)[0]['translation_text']\n",
    "print(\"ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì„œ:\", translated_text_en, \"\\n\")\n",
    "\n",
    "# Step 2: ì˜ì–´ ë¬¸ì„œ ìš”ì•½\n",
    "summary_en = summarizer(translated_text_en, max_length=100, min_length=50, do_sample=True, temperature=0.7)[0]['summary_text']\n",
    "print(\"ì˜ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸:\", summary_en, \"\\n\")\n",
    "\n",
    "# Step 3: ì˜ì–´ ìš”ì•½ì„ í•œêµ­ì–´ë¡œ ë³€í™˜\n",
    "summary_ko = translator_en_to_ko(summary_en)[0]['translation_text']\n",
    "print(\"í•œêµ­ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸:\", summary_ko, \"\\n\")\n",
    "\n",
    "# Step 4: í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­\n",
    "question_ko = \"ì „ì‘ê³¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "translated_question_en = translator_ko_to_en(question_ko)[0]['translation_text']\n",
    "print(\"ë²ˆì—­ëœ ì˜ì–´ ì§ˆë¬¸:\", translated_question_en, \"\\n\")\n",
    "\n",
    "# Step 5: ì˜ì–´ ì§ˆë¬¸ì„ ì˜ì–´ ìš”ì•½ëœ ë¬¸ì„œì—ì„œ ì§ˆë¬¸-ì‘ë‹µ ìˆ˜í–‰\n",
    "answer_en = question_answerer(question=translated_question_en, context=summary_en, top_k=3)\n",
    "\n",
    "# Step 6: ì˜ì–´ ë‹µë³€ì„ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­\n",
    "translated_answers_ko = [translator_en_to_ko(ans['answer'])[0]['translation_text'] for ans in answer_en]\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë‹µë³€ ì¶œë ¥\n",
    "print(\"AIì˜ ì²« ë²ˆì§¸ ë‹µë³€:\", translated_answers_ko[0])\n",
    "\n",
    "# ëª¨ë“  ë‹µë³€ ì¶œë ¥\n",
    "for i, ans in enumerate(translated_answers_ko):\n",
    "    print(f\"{i}ë²ˆì§¸ AIì˜ ë‹µë³€:\", ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(text, max_length=512):\n",
    "    \"\"\"ë¬¸ì¥ì„ ì¼ì • ê¸¸ì´ë¡œ ë¶„í• \"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_length:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def translate_large_text(translator, text, max_length=512):\n",
    "    \"\"\"ê¸´ ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ ë²ˆì—­ í›„ í•©ì¹˜ê¸°\"\"\"\n",
    "    chunks = split_text(text, max_length=max_length)\n",
    "    translated_chunks = [translator(chunk)[0]['translation_text'] for chunk in chunks]\n",
    "    return \" \".join(translated_chunks).replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Your input_length: 248 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 253 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 205 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ í•œê¸€ ë¬¸ì„œ: \n",
      "ì „ì‘ì— ë‚˜ì˜¨ ì˜ì›…ë“¤ì˜ ê¸°ìˆ ë“¤ì€ ì „ì²´ì ìœ¼ë¡œ ì „ì‘ì˜ í˜•íƒœë¥¼ ìœ ì§€í•˜ê³  ìˆì§€ë§Œ ê¸°ìˆ  ê°œìˆ˜ê°€ 7ê°œì—ì„œ 11ê°œë¡œ ì¦ê°€í•´ ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë§ì´ ìƒê²¼ìœ¼ë©°, í† í° ì‹œìŠ¤í…œì´ ì ìš©ë˜ê³  íšŒë³µ ê³„ì—´ ìŠ¤í‚¬ë“¤ì´ ëŒ€ë¶€ë¶„ ì‹œì „ ì²´ë ¥ ì¡°ê±´ì´ë‚˜ 2~3íšŒ ì •ë„ì˜ íšŸìˆ˜ ì œí•œ ë“± í˜ë„í‹°ê°€ ìƒê²¼ìœ¼ë©° ì¥ê¸°ì ì¸ ë²„í”„ íš¨ê³¼ë„ ë§ì´ ì—†ì–´ì§„ í¸ì´ë¼ ì „ì‘ì˜ ê°ê°ìœ¼ë¡œ ë‹¨í¸ì ì¸ í™œìš©ì„ í•´ì„œëŠ” ê³ ì „í•˜ê¸° ì‹­ìƒì´ë‹¤. íŠ¹íˆ í† í° ì‹œìŠ¤í…œì— ìµìˆ™í•´ì§€ì§€ ëª»í•œë‹¤ë©´ ë”ìš± ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.\n",
      "ë˜í•œ ì „ì‘ì— ì¡´ì¬í•˜ë˜ ì‹œìŠ¤í…œì¸ ë¬´ê¸°ì™€ ë°©ì–´êµ¬ ì‹œìŠ¤í…œ ìì²´ê°€ ì‚¬ë¼ì ¸ ê³µê²© ê¸°ìˆ ì—ëŠ” ìì²´ì ìœ¼ë¡œ ê³µê²©ë ¥ì´ ë¶™ì–´ ìˆë‹¤. ë˜í•œ íšŒí”¼ìœ¨ì´ë‚˜ ëª…ì¤‘ë¥  ì—­ì‹œ í† í°ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— í† í°ì´ ì—†ë‹¤ëŠ” ì „ì œ í•˜ì— ê¸°ë³¸ì ì¸ íšŒí”¼ìœ¨ì€ 0%, ëª…ì¤‘ë¥ ì€ 100%ì´ë‹¤.\n",
      "1í¸ì—ì„œ ë“±ì¥í–ˆë˜ ì˜ì›…ë“¤ì´ ê²Œì„ ë‚´ì— ëª¨ë‘ ë“±ì¥í•˜ì§€ëŠ” ì•Šì•˜ìœ¼ë©°, ì£¼ì¸ê³µ ì¼í–‰ì„ ë°°ì‹ í•˜ì—¬ ë„ì ë“¤ê³¼ í•©ë¥˜í•œ ìœ ë¬¼ìˆ˜ì§‘ìƒ ë“± ìŠ¤í† ë¦¬ìƒìœ¼ë¡œ ì¶œì—°ì´ ë°°ì œëœ ìºë¦­í„°ë“¤ë„ ì¡´ì¬í•œë‹¤. í—ˆë‚˜ ê°œë°œì ë˜í•œ ë‹¤ë¥¸ ì¸í„°ë·°ì—ì„œ ìœ ì €ë“¤ì˜ ì „ì‘ ì˜ì›…ë“¤ì´ ì¶œì—°í–ˆìœ¼ë©´ í•˜ëŠ” ìš”ì²­ì´ ë§ì€ ê²ƒì„ ì•Œê³  ìˆì–´ ê¸°ì¡´ ìºë¦­í„°ë¥¼ ë“±ì¥í•˜ê²Œ í•  ìƒê°ì´ ìˆìŒì„ ë°í˜”ê¸°ì— ìƒˆë¡œìš´ ì˜ì›…ë“¤ì´ ì¶”ê°€ë  ì—¬ì§€ë¥¼ ë‚¨ê²¨ë‘ì—ˆë‹¤.\n",
      "ì „ì‘ì—ì„œëŠ” ì˜ì›… ê³ ìš© ì‹œ ë‚˜ë ˆì´í„°ê°€ í•´ë‹¹ ì˜ì›…ì˜ ëª¨ìŠµê³¼ íŠ¹ì§•, ì¶œì‹  ë“±ì„ ì´ì•¼ê¸°í–ˆë˜ ê²ƒê³¼ ë‹¤ë¥´ê²Œ ë³¸ì‘ì—ì„œëŠ” í•´ë‹¹ ì˜ì›…ì˜ ê³¼ê±°ì‚¬ë¥¼ ë¹„ì¶”ëŠ” ë§ì„ í•œë‹¤. ì „ì‘ê³¼ ë‹¬ë¦¬ ê° ì˜ì›…ì˜ ê³¼ê±°ì— ë¹„ì¤‘ì„ ë§ì´ ë‘ì—ˆëŠ”ë°, ë§µì—ì„œ ë¬´ì‘ìœ„ë¡œ ë“±ì¥í•˜ëŠ” íšŒê³ ì˜ ì„±ì†Œì—ì„œ ê³¼ê±°ì˜ ë©”ì•„ë¦¬ë¥¼ í†µí•´ ì˜ì›…ì˜ ê³¼ê±°ì‚¬ì™€ ìŠ¤í‚¬ì„ í•´ê¸ˆí•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ëª¨ë“  ì˜ì›…ì´ ì „ì‘ê³¼ëŠ” ë‹¬ë¦¬ í”Œë ˆì´ì–´ì™€ ìŠ¤ìŠ¹ê³¼ì˜ ì—°ê²°ì ì´ ì•Œë ¤ì ¸ ìˆì§€ ì•Šì•„, ì–´ì§¸ì„œ í¬ë§ì˜ íšƒë¶ˆì„ ì§€í‚¤ëŠ” ì…ì¥ì´ ë˜ì—ˆëŠ”ì§€ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤.\n",
      "í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ì„ ì œì™¸í•œ ì˜ì›…ë“¤ì€ ì˜ì›…ì˜ ê¸¸ì„ 4ê°œì”© ê°€ì§€ê³  ìˆëŠ”ë° í•˜ë‚˜ëŠ” ë°©ë‘ìë¼ëŠ” ê³µí†µëœ ì˜ì›…ì˜ ê¸¸ì´ê³  ë‚˜ë¨¸ì§€ 3ê°œëŠ” ì˜ì›…ë³„ë¡œ ì •í•´ì§„ íŠ¹ì„±í™”ëœ ì˜ì›…ì˜ ê¸¸ì´ë‹¤. í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ë„ ì¼ë‹¨ì€ ì˜ì›…ì˜ ê¸¸ ìì²´ëŠ” ì¡´ì¬í•˜ëŠ”ë° ì „ë¬¸ê°€ë¼ëŠ” ì˜ì›…ì˜ ê¸¸ë¡œ ê³ ì •ë˜ì–´ ìˆë‹¤. ì˜ì›…ì˜ ê¸¸ì€ í¬ë§ì˜ ì œë‹¨ì˜ ì‚´ì•„ìˆëŠ” ë„ì‹œì—ì„œ ì´›ë¶ˆë¡œ í•´ê¸ˆí•´ì•¼ í•œë‹¤. ì›ë˜ëŠ” ì›ì • ì‹œì‘ ì‹œ ì˜ì›…ì„ ê²°ì •í•˜ëŠ” 'êµì°¨ë¡œ'ì—ì„œ ì˜ì›…ê³¼ ì‚¬ìš©í•  ì˜ì›…ì˜ ê¸¸ì„ ì •í•˜ë©´ ì±•í„°ê°€ ëë‚˜ë„ë¡ ê³„ì† ê³ ì •ëœ ì˜ì›…ì˜ ê¸¸ì„ ì‚¬ìš©í•´ì•¼ í–ˆì§€ë§Œ 1.02.54580 ë²„ì „ë¶€í„° ì—¬ê´€ì—ì„œ ì˜ì›…ì˜ ê¸¸ì„ ë³€ê²½í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ê³„ê³¡ ì§€ì—­ì˜ ì²« ì—¬ê´€ì—ì„œëŠ” ìœ ë¬¼ 4, ê·¸ ë’¤ë¡œëŠ” ìœ ë¬¼ 32ê°€ í•„ìš”í•˜ë©°, ì˜ì›…ì˜ ê¸¸ ë³€ê²½ ì‹œ ê·¸ ì˜ì›…ì—ê²Œ íˆ¬ìí–ˆë˜ ìˆ™ë ¨ í¬ì¸íŠ¸ëŠ” ëª¨ë‘ ë°˜ë‚©ë°›ëŠ”ë‹¤.\n",
      "ì „ìš© ì¥ì‹ êµ¬ëŠ” ì˜ì›…ë‹¹ 3ê°œì´ë©° ì „ì‘ê³¼ ë‹¤ë¥´ê²Œ ëª¨ë‘ ê°™ì€ ë“±ê¸‰ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ ì „ìš© ì¥ì‹ êµ¬ëŠ” ì˜ì›…ì˜ ê³¼ê±°ì™€ ê´€ë ¨ì´ ìˆë‹¤. ë˜í•œ ì„ì‹œ ê³ ìš© ì˜ì›…ì¸ í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ì—ê²ŒëŠ” ì „ìš© ì—¬ê´€ ì•„ì´í…œì´ ì¡´ì¬í•˜ë©° í•´ê¸ˆì´ ë˜ì—ˆì„ ì‹œ ì—¬ê´€ì— ì…ì¥í•  ë•Œ í™•ë¥ ì ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
      "\n",
      "ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì„œ: Now, the technology of the heroes in the game remains the same overall, but the number of technologies has grown from seven to eleven, and there's a lot of new technology coming in, and the token system has been implemented, and the recovery-style skills are mostly limited to physical conditioning, limited to two or three turns, and there's not a lot of long-term buffer effect, so it's a good idea to try to use it as a single sensor in the game, especially if you're not used to the token system, and it's even harder. Also, the weapons and defense systems that existed in the game have disappeared, and the attacking technology has its own attacking power, and the probability of avoidance and the number of turns is also affected by the token, so the basic probability of avoidance is zero, the number of turns But the developer also revealed in other interviews that he knew that there were a lot of requests from users for the heroes to appear in the prequel, and that he thought he had an idea for an existing character, so he left a lot of room for new heroes. Unlike in the prequel, where the character hired by the narrator talked about the character's appearance, characteristics, origins, and so on, in the main story, the narrator talks about the history of the character, and unlike in the prequel, each character had a lot of weight on the history of the character, and you could rehearse the history and the skill of the character through a rehearsal hallway that appears randomly on the map, but not all of the heroes are known to have connections with the player and the teacher, and we don't know why they became the guardians of the The hero's path has to be redeemed by candles in the living city of the Order of Hope. Originally, at the intersection that determines the hero at the start of the mission, you had to use the hero's path to go with the hero at the end of the chapter, but now with version 1.02.54580, you can change the hero's path from a labyrinth to a labyrinth. The first labyrinth in the valley area requires 4 treasures, then 32 treasures, and when you change the hero's path, all the skill points that you invested in the hero are redeemed. The dedicated costumes are three for each hero, and unlike the previous one, they're all the same grade. Most of the dedicated costumes are related to the hero's past, and for the rest, except for the temporary hired hero, the \n",
      "\n",
      "ì˜ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸: The hero's path has to be redeemed by candles in the living city of the Order of Hope. The dedicated costumes are three for each hero, and unlike the previous one, they're all the same grade. Unlike in the prequel, each character had a lot of weight on the history of the character. Not all of the heroes are known to have connections with the player and the teacher. \n",
      "\n",
      "í•œêµ­ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸: ì˜ì›…ì˜ ê¸¸ì€ í¬ë§ì˜ ê³„ë‹¨ì˜ ì‚´ì•„ìˆëŠ” ë„ì‹œì—ì„œ ì´›ë¶ˆìœ¼ë¡œ êµ¬í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê° ì˜ì›…ì„ ìœ„í•œ í—Œì‹ ì ì¸ ì˜ìƒì€ 3ê°œì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ì „ê³¼ëŠ” ë‹¬ë¦¬, ê·¸ê²ƒë“¤ì€ ëª¨ë‘ ê°™ì€ ë“±ê¸‰ì…ë‹ˆë‹¤. ì „ì„¤ê³¼ëŠ” ë‹¬ë¦¬, ê° ìºë¦­í„°ëŠ” ìºë¦­í„°ì˜ ì—­ì‚¬ì— í° ì˜í–¥ì„ ë¼ì³¤ìŠµë‹ˆë‹¤. ëª¨ë“  ì˜ì›…ë“¤ì€ í”Œë ˆì´ì–´ì™€ ì„ ìƒë‹˜ê³¼ ì—°ê²°ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œì§€ ëª»í•©ë‹ˆë‹¤. \n",
      "\n",
      "ê¸°ì¡´ í•œê¸€ ì§ˆë¬¸: ì „ì‘ê³¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ë²ˆì—­ëœ ì˜ì–´ ì§ˆë¬¸: What are the characteristics of a predatory system versus a different one? \n",
      "\n",
      "AIì˜ ì²« ë²ˆì§¸ ë‹µë³€: ëª¨ë‘ ê°™ì€ ì„±ì ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "1ë²ˆì§¸ AIì˜ ë‹µë³€: ëª¨ë‘ ê°™ì€ ì„±ì ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "2ë²ˆì§¸ AIì˜ ë‹µë³€: ê·¸ë“¤ì€ ëª¨ë‘ ê°™ì€ ì„±ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "3ë²ˆì§¸ AIì˜ ë‹µë³€: ê°™ì€ ì„±ì ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "4ë²ˆì§¸ AIì˜ ë‹µë³€: ëª¨ë‘ ê°™ì€ ë“±ê¸‰ì…ë‹ˆë‹¤.\n",
      "5ë²ˆì§¸ AIì˜ ë‹µë³€: ê·¸ë“¤ì€ ëª¨ë‘ ê°™ì€ ì„±ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# ìµœì‹  ëª¨ë¸ ì„ íƒ\n",
    "summarization_model_name = \"facebook/bart-large-cnn\"\n",
    "qa_model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_model_name)\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "\n",
    "# ë²ˆì—­ ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ â†” ì˜ì–´)\n",
    "ko_to_en_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "en_to_ko_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "ko_to_en_tokenizer = AutoTokenizer.from_pretrained(ko_to_en_model_name)\n",
    "ko_to_en_model = AutoModelForSeq2SeqLM.from_pretrained(ko_to_en_model_name)\n",
    "\n",
    "en_to_ko_tokenizer = AutoTokenizer.from_pretrained(en_to_ko_model_name)\n",
    "en_to_ko_model = AutoModelForSeq2SeqLM.from_pretrained(en_to_ko_model_name)\n",
    "\n",
    "# GPU ì„¤ì • (ì—†ìœ¼ë©´ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarization_model = summarization_model.to(device)\n",
    "qa_model = qa_model.to(device)\n",
    "ko_to_en_model = ko_to_en_model.to(device)\n",
    "en_to_ko_model = en_to_ko_model.to(device)\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì„¤ì • (src_lang, tgt_lang ëª…ì‹œì  ì„¤ì •)\n",
    "summarizer = pipeline(\"summarization\", model=summarization_model, tokenizer=summarization_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "question_answerer = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "translator_ko_to_en = pipeline(\n",
    "    \"translation\",\n",
    "    model=ko_to_en_model,\n",
    "    tokenizer=ko_to_en_tokenizer,\n",
    "    src_lang=\"ko_KR\",\n",
    "    tgt_lang=\"en_XX\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "translator_en_to_ko = pipeline(\n",
    "    \"translation\",\n",
    "    model=en_to_ko_model,\n",
    "    tokenizer=en_to_ko_tokenizer,\n",
    "    src_lang=\"en_XX\",\n",
    "    tgt_lang=\"ko_KR\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ê¸´ ë¬¸ì„œ ì…ë ¥ (í•œêµ­ì–´)\n",
    "long_text_ko = \"\"\"\n",
    "ì „ì‘ì— ë‚˜ì˜¨ ì˜ì›…ë“¤ì˜ ê¸°ìˆ ë“¤ì€ ì „ì²´ì ìœ¼ë¡œ ì „ì‘ì˜ í˜•íƒœë¥¼ ìœ ì§€í•˜ê³  ìˆì§€ë§Œ ê¸°ìˆ  ê°œìˆ˜ê°€ 7ê°œì—ì„œ 11ê°œë¡œ ì¦ê°€í•´ ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë§ì´ ìƒê²¼ìœ¼ë©°, í† í° ì‹œìŠ¤í…œì´ ì ìš©ë˜ê³  íšŒë³µ ê³„ì—´ ìŠ¤í‚¬ë“¤ì´ ëŒ€ë¶€ë¶„ ì‹œì „ ì²´ë ¥ ì¡°ê±´ì´ë‚˜ 2~3íšŒ ì •ë„ì˜ íšŸìˆ˜ ì œí•œ ë“± í˜ë„í‹°ê°€ ìƒê²¼ìœ¼ë©° ì¥ê¸°ì ì¸ ë²„í”„ íš¨ê³¼ë„ ë§ì´ ì—†ì–´ì§„ í¸ì´ë¼ ì „ì‘ì˜ ê°ê°ìœ¼ë¡œ ë‹¨í¸ì ì¸ í™œìš©ì„ í•´ì„œëŠ” ê³ ì „í•˜ê¸° ì‹­ìƒì´ë‹¤. íŠ¹íˆ í† í° ì‹œìŠ¤í…œì— ìµìˆ™í•´ì§€ì§€ ëª»í•œë‹¤ë©´ ë”ìš± ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.\n",
    "ë˜í•œ ì „ì‘ì— ì¡´ì¬í•˜ë˜ ì‹œìŠ¤í…œì¸ ë¬´ê¸°ì™€ ë°©ì–´êµ¬ ì‹œìŠ¤í…œ ìì²´ê°€ ì‚¬ë¼ì ¸ ê³µê²© ê¸°ìˆ ì—ëŠ” ìì²´ì ìœ¼ë¡œ ê³µê²©ë ¥ì´ ë¶™ì–´ ìˆë‹¤. ë˜í•œ íšŒí”¼ìœ¨ì´ë‚˜ ëª…ì¤‘ë¥  ì—­ì‹œ í† í°ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— í† í°ì´ ì—†ë‹¤ëŠ” ì „ì œ í•˜ì— ê¸°ë³¸ì ì¸ íšŒí”¼ìœ¨ì€ 0%, ëª…ì¤‘ë¥ ì€ 100%ì´ë‹¤.\n",
    "1í¸ì—ì„œ ë“±ì¥í–ˆë˜ ì˜ì›…ë“¤ì´ ê²Œì„ ë‚´ì— ëª¨ë‘ ë“±ì¥í•˜ì§€ëŠ” ì•Šì•˜ìœ¼ë©°, ì£¼ì¸ê³µ ì¼í–‰ì„ ë°°ì‹ í•˜ì—¬ ë„ì ë“¤ê³¼ í•©ë¥˜í•œ ìœ ë¬¼ìˆ˜ì§‘ìƒ ë“± ìŠ¤í† ë¦¬ìƒìœ¼ë¡œ ì¶œì—°ì´ ë°°ì œëœ ìºë¦­í„°ë“¤ë„ ì¡´ì¬í•œë‹¤. í—ˆë‚˜ ê°œë°œì ë˜í•œ ë‹¤ë¥¸ ì¸í„°ë·°ì—ì„œ ìœ ì €ë“¤ì˜ ì „ì‘ ì˜ì›…ë“¤ì´ ì¶œì—°í–ˆìœ¼ë©´ í•˜ëŠ” ìš”ì²­ì´ ë§ì€ ê²ƒì„ ì•Œê³  ìˆì–´ ê¸°ì¡´ ìºë¦­í„°ë¥¼ ë“±ì¥í•˜ê²Œ í•  ìƒê°ì´ ìˆìŒì„ ë°í˜”ê¸°ì— ìƒˆë¡œìš´ ì˜ì›…ë“¤ì´ ì¶”ê°€ë  ì—¬ì§€ë¥¼ ë‚¨ê²¨ë‘ì—ˆë‹¤.\n",
    "ì „ì‘ì—ì„œëŠ” ì˜ì›… ê³ ìš© ì‹œ ë‚˜ë ˆì´í„°ê°€ í•´ë‹¹ ì˜ì›…ì˜ ëª¨ìŠµê³¼ íŠ¹ì§•, ì¶œì‹  ë“±ì„ ì´ì•¼ê¸°í–ˆë˜ ê²ƒê³¼ ë‹¤ë¥´ê²Œ ë³¸ì‘ì—ì„œëŠ” í•´ë‹¹ ì˜ì›…ì˜ ê³¼ê±°ì‚¬ë¥¼ ë¹„ì¶”ëŠ” ë§ì„ í•œë‹¤. ì „ì‘ê³¼ ë‹¬ë¦¬ ê° ì˜ì›…ì˜ ê³¼ê±°ì— ë¹„ì¤‘ì„ ë§ì´ ë‘ì—ˆëŠ”ë°, ë§µì—ì„œ ë¬´ì‘ìœ„ë¡œ ë“±ì¥í•˜ëŠ” íšŒê³ ì˜ ì„±ì†Œì—ì„œ ê³¼ê±°ì˜ ë©”ì•„ë¦¬ë¥¼ í†µí•´ ì˜ì›…ì˜ ê³¼ê±°ì‚¬ì™€ ìŠ¤í‚¬ì„ í•´ê¸ˆí•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ëª¨ë“  ì˜ì›…ì´ ì „ì‘ê³¼ëŠ” ë‹¬ë¦¬ í”Œë ˆì´ì–´ì™€ ìŠ¤ìŠ¹ê³¼ì˜ ì—°ê²°ì ì´ ì•Œë ¤ì ¸ ìˆì§€ ì•Šì•„, ì–´ì§¸ì„œ í¬ë§ì˜ íšƒë¶ˆì„ ì§€í‚¤ëŠ” ì…ì¥ì´ ë˜ì—ˆëŠ”ì§€ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤.\n",
    "í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ì„ ì œì™¸í•œ ì˜ì›…ë“¤ì€ ì˜ì›…ì˜ ê¸¸ì„ 4ê°œì”© ê°€ì§€ê³  ìˆëŠ”ë° í•˜ë‚˜ëŠ” ë°©ë‘ìë¼ëŠ” ê³µí†µëœ ì˜ì›…ì˜ ê¸¸ì´ê³  ë‚˜ë¨¸ì§€ 3ê°œëŠ” ì˜ì›…ë³„ë¡œ ì •í•´ì§„ íŠ¹ì„±í™”ëœ ì˜ì›…ì˜ ê¸¸ì´ë‹¤. í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ë„ ì¼ë‹¨ì€ ì˜ì›…ì˜ ê¸¸ ìì²´ëŠ” ì¡´ì¬í•˜ëŠ”ë° ì „ë¬¸ê°€ë¼ëŠ” ì˜ì›…ì˜ ê¸¸ë¡œ ê³ ì •ë˜ì–´ ìˆë‹¤. ì˜ì›…ì˜ ê¸¸ì€ í¬ë§ì˜ ì œë‹¨ì˜ ì‚´ì•„ìˆëŠ” ë„ì‹œì—ì„œ ì´›ë¶ˆë¡œ í•´ê¸ˆí•´ì•¼ í•œë‹¤. ì›ë˜ëŠ” ì›ì • ì‹œì‘ ì‹œ ì˜ì›…ì„ ê²°ì •í•˜ëŠ” 'êµì°¨ë¡œ'ì—ì„œ ì˜ì›…ê³¼ ì‚¬ìš©í•  ì˜ì›…ì˜ ê¸¸ì„ ì •í•˜ë©´ ì±•í„°ê°€ ëë‚˜ë„ë¡ ê³„ì† ê³ ì •ëœ ì˜ì›…ì˜ ê¸¸ì„ ì‚¬ìš©í•´ì•¼ í–ˆì§€ë§Œ 1.02.54580 ë²„ì „ë¶€í„° ì—¬ê´€ì—ì„œ ì˜ì›…ì˜ ê¸¸ì„ ë³€ê²½í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ê³„ê³¡ ì§€ì—­ì˜ ì²« ì—¬ê´€ì—ì„œëŠ” ìœ ë¬¼ 4, ê·¸ ë’¤ë¡œëŠ” ìœ ë¬¼ 32ê°€ í•„ìš”í•˜ë©°, ì˜ì›…ì˜ ê¸¸ ë³€ê²½ ì‹œ ê·¸ ì˜ì›…ì—ê²Œ íˆ¬ìí–ˆë˜ ìˆ™ë ¨ í¬ì¸íŠ¸ëŠ” ëª¨ë‘ ë°˜ë‚©ë°›ëŠ”ë‹¤.\n",
    "ì „ìš© ì¥ì‹ êµ¬ëŠ” ì˜ì›…ë‹¹ 3ê°œì´ë©° ì „ì‘ê³¼ ë‹¤ë¥´ê²Œ ëª¨ë‘ ê°™ì€ ë“±ê¸‰ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ ì „ìš© ì¥ì‹ êµ¬ëŠ” ì˜ì›…ì˜ ê³¼ê±°ì™€ ê´€ë ¨ì´ ìˆë‹¤. ë˜í•œ ì„ì‹œ ê³ ìš© ì˜ì›…ì¸ í˜„ìƒê¸ˆ ì‚¬ëƒ¥ê¾¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ì—ê²ŒëŠ” ì „ìš© ì—¬ê´€ ì•„ì´í…œì´ ì¡´ì¬í•˜ë©° í•´ê¸ˆì´ ë˜ì—ˆì„ ì‹œ ì—¬ê´€ì— ì…ì¥í•  ë•Œ í™•ë¥ ì ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: í•œêµ­ì–´ ë¬¸ì„œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ (ê¸´ ë¬¸ì¥ ì§€ì›)\n",
    "translated_text_en = translate_large_text(translator_ko_to_en, long_text_ko)\n",
    "print(\"ê¸°ì¡´ í•œê¸€ ë¬¸ì„œ:\", long_text_ko)\n",
    "print(\"ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì„œ:\", translated_text_en, \"\\n\")\n",
    "\n",
    "# Step 2: ì˜ì–´ ë¬¸ì„œ ìš”ì•½ (ìš”ì•½ ê¸¸ì´ ìµœì í™”)\n",
    "summary_en = summarizer(translated_text_en, max_length=150, min_length=70, do_sample=True, temperature=0.7)[0]['summary_text']\n",
    "print(\"ì˜ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸:\", summary_en, \"\\n\")\n",
    "\n",
    "# Step 3: ì˜ì–´ ìš”ì•½ì„ í•œêµ­ì–´ë¡œ ë³€í™˜ (ê¸´ ë¬¸ì¥ ì§€ì›)\n",
    "summary_ko = translate_large_text(translator_en_to_ko, summary_en)\n",
    "print(\"í•œêµ­ì–´ ìš”ì•½ëœ í…ìŠ¤íŠ¸:\", summary_ko, \"\\n\")\n",
    "\n",
    "# Step 4: í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­\n",
    "question_ko = \"ì „ì‘ê³¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "translated_question_en = translator_ko_to_en(question_ko)[0]['translation_text']\n",
    "print(\"ê¸°ì¡´ í•œê¸€ ì§ˆë¬¸:\", question_ko)\n",
    "print(\"ë²ˆì—­ëœ ì˜ì–´ ì§ˆë¬¸:\", translated_question_en, \"\\n\")\n",
    "\n",
    "# Step 5: ì˜ì–´ ì§ˆë¬¸ì„ ì˜ì–´ ìš”ì•½ëœ ë¬¸ì„œì—ì„œ ì§ˆë¬¸-ì‘ë‹µ ìˆ˜í–‰ (ì •í™•ë„ í–¥ìƒ)\n",
    "answer_en = question_answerer(question=translated_question_en, context=summary_en, top_k=5)\n",
    "\n",
    "# Step 6: ì˜ì–´ ë‹µë³€ì„ ë‹¤ì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ê¸´ ë¬¸ì¥ ì§€ì›)\n",
    "translated_answers_ko = [translate_large_text(translator_en_to_ko, ans['answer']) for ans in answer_en]\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë‹µë³€ ì¶œë ¥\n",
    "print(\"AIì˜ ì²« ë²ˆì§¸ ë‹µë³€:\", translated_answers_ko[0])\n",
    "\n",
    "# ëª¨ë“  ë‹µë³€ ì¶œë ¥\n",
    "for i, ans in enumerate(translated_answers_ko):\n",
    "    print(f\"{i+1}ë²ˆì§¸ AIì˜ ë‹µë³€:\", ans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
