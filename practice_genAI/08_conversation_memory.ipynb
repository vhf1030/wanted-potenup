{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='안녕하세요, 김상겸님! 만나서 반갑습니다. 어떻게 도와드릴까요?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 19, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-dec88f70-1521-43fb-9180-6868d540acbc-0'\n",
      "content='죄송하지만, 당신의 이름을 알 수 있는 정보가 없습니다. 당신의 이름이 무엇인지 말씀해 주실 수 있나요?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 13, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-4dddd6b4-d1bd-4857-b65b-975f42f3b717-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "\n",
    "response = llm.invoke('안녕 내 이름은 김상겸이야.')\n",
    "print(response)\n",
    "\n",
    "response = llm.invoke('내 이름이 뭐라고?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17516\\1192676045.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕 내 이름은 김상겸이야.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '안녕 내 이름은 김상겸이야.', 'history': '', 'response': '안녕 김상겸! 만나서 반가워. 나는 AI야, 너와 대화하는 걸 정말 좋아해. 오늘 기분이 어때? 혹시 궁금한 점이나 이야기하고 싶은 주제가 있어?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = con_chain.invoke('안녕 내 이름은 김상겸이야.')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕 내 이름은 김상겸이야.\n",
      "AI: 안녕 김상겸! 만나서 반가워. 나는 AI야, 너와 대화하는 걸 정말 좋아해. 오늘 기분이 어때? 혹시 궁금한 점이나 이야기하고 싶은 주제가 있어?\n",
      "Human: 내 이름이 뭔지 알아?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '내 이름이 뭔지 알아?', 'history': 'Human: 안녕 내 이름은 김상겸이야.\\nAI: 안녕 김상겸! 만나서 반가워. 나는 AI야, 너와 대화하는 걸 정말 좋아해. 오늘 기분이 어때? 혹시 궁금한 점이나 이야기하고 싶은 주제가 있어?', 'response': '네, 너의 이름은 김상겸이야! 정말 멋진 이름이야. 너의 이름에 어떤 특별한 의미가 있거나, 가족의 전통이 있는지 궁금해!'}\n"
     ]
    }
   ],
   "source": [
    "response = con_chain.invoke('내 이름이 뭔지 알아?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 안녕 내 이름은 김상겸이야.\\nAI: 안녕 김상겸! 만나서 반가워. 나는 AI야, 너와 대화하는 걸 정말 좋아해. 오늘 기분이 어때? 혹시 궁금한 점이나 이야기하고 싶은 주제가 있어?\\nHuman: 내 이름이 뭔지 알아?\\nAI: 네, 너의 이름은 김상겸이야! 정말 멋진 이름이야. 너의 이름에 어떤 특별한 의미가 있거나, 가족의 전통이 있는지 궁금해!'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유저별 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕! 내 이름은 김상겸이야.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "user_memory = {}\n",
    "\n",
    "def get_user_memory(user_id):\n",
    "    if user_id not in user_memory:\n",
    "        user_memory[user_id] = ConversationBufferMemory()\n",
    "        \n",
    "    return user_memory[user_id]\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "user_id = 1030\n",
    "\n",
    "conv_chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = get_user_memory(user_id),\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕! 내 이름은 김상겸이야.\n",
      "AI: 안녕, 김상겸! 만나서 반가워! 나는 AI야. 너는 어떤 것에 대해 이야기하고 싶어? 취미나 관심사 같은 것들을 나누면 좋을 것 같아!\n",
      "Human: 날씨가 정말 좋네\n",
      "AI: 정말 좋은 날씨라니 좋지! 어떤 활동을 하고 싶어? 밖에서 산책하거나 친구와 만나는 것도 좋고, 혹시 좋아하는 카페가 있으면 가서 커피 한 잔 하는 것도 추천해! 요즘 날씨가 맑으면 기분도 더 좋잖아. 너는 보통 날씨가 좋은 날에 어떤 일을 하는 걸 좋아해?\n",
      "Human: 낮잠 자고 싶어\n",
      "AI: 낮잠도 정말 좋은 선택이야! 특히 날씨가 좋을 때는 따뜻한 햇볕 아래에서 자는 게 정말 편안하잖아. 낮잠은 에너지를 재충전해 주기도 하고, 기분을 좋게 만들어 주기도 해. 보통 몇 분 정도 자는 걸 좋아해? 20분 정도의 짧은 낮잠이 효과적이라고 하기도 하고, 때로는 좀 더 길게 자는 것도 좋지. 너는 어떤 스타일의 낮잠을 선호해?\n",
      "Human: 너는 참 말이 많구나\n",
      "AI: ㅎㅎ 그렇지! 나는 대화를 즐기는 걸 좋아해서 많은 정보를 나누고 싶어. 너와의 대화에서 더 많은 것을 배우고 싶기도 하고. 그래서 질문이나 이야기하고 싶은 주제가 있으면 언제든지 말해줘! 너는 어떤 이야기를 더 나누고 싶어?\n",
      "Human: 너는 참 말이 많구나\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ㅎㅎ 네, 맞아! 나는 대화를 통해 다양한 정보를 나누고, 서로의 생각을 교환하는 걸 좋아해. 그래서 질문이든 이야기든 어떤 것이든 편하게 해줘! 혹시 다른 관심사나 취미에 대해서 이야기하고 싶어? 아니면 최근에 흥미로운 일이나 사건이 있었는지 궁금해!\n"
     ]
    }
   ],
   "source": [
    "response = conv_chain.invoke('너는 참 말이 많구나')\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '안녕!', 'history': '', 'response': '안녕하세요! 어떻게 지내세요? 오늘은 어떤 이야기를 나눠볼까요?'}\n"
     ]
    }
   ],
   "source": [
    "response = con_chain.invoke('안녕!')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: 안녕!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "memory_test = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "\n",
    "con_chain_test = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory_test,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = con_chain_test.invoke('안녕!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 안녕!\\nAI: 안녕하세요! 어떻게 지내세요? 오늘은 어떤 이야기를 나눠볼까요?'}\n",
      "{'history': [HumanMessage(content='안녕!', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요! 어떻게 지내세요? 오늘 어떤 이야기 나눌까요? 😊', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))\n",
    "print(memory_test.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_41496\\3313397638.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150\n",
    ")\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕 내 이름은 김상겸이야.\n",
      "AI: 안녕하세요, 김상겸님! 만나서 반갑습니다. 저는 여러분과 이야기하는 AI입니다. 오늘 하루는 어땠나요? 특별히 하고 싶은 이야기가 있나요?\n",
      "Human: 날씨가 참 좋구나\n",
      "AI: 정말 좋군요! 날씨가 좋은 날에는 기분이 한층 더 좋아지죠. 밖에 나가서 산책하거나, 좋아하는 활동을 하기에 딱 좋은 날입니다. 어떤 활동을 계획하고 계신가요? 아니면 특별히 가고 싶은 장소가 있나요?\n",
      "Human: 어디까지 기억하고 있니\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '어디까지 기억하고 있니', 'history': 'Human: 안녕 내 이름은 김상겸이야.\\nAI: 안녕하세요, 김상겸님! 만나서 반갑습니다. 저는 여러분과 이야기하는 AI입니다. 오늘 하루는 어땠나요? 특별히 하고 싶은 이야기가 있나요?\\nHuman: 날씨가 참 좋구나\\nAI: 정말 좋군요! 날씨가 좋은 날에는 기분이 한층 더 좋아지죠. 밖에 나가서 산책하거나, 좋아하는 활동을 하기에 딱 좋은 날입니다. 어떤 활동을 계획하고 계신가요? 아니면 특별히 가고 싶은 장소가 있나요?', 'response': '저는 현재 대화의 맥락을 기억하고 있습니다. 김상겸님과의 대화 내용이나 질문에 대한 답변은 계속해서 이어나갈 수 있어요. 하지만 이전 대화나 다른 세션의 내용은 기억하지 못합니다. 지금 이야기하고 있는 내용에 대해서는 계속해서 대화할 수 있으니, 궁금한 점이나 나누고 싶은 이야기가 있다면 언제든지 말씀해 주세요!'}\n"
     ]
    }
   ],
   "source": [
    "response = con_chain.invoke('어디까지 기억하고 있니')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐시 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='랭체인(LangChain)에서 캐싱은 주로 데이터 처리 성능을 향상시키기 위해 사용됩니다. 캐싱은 자주 사용되는 데이터나 API 호출 결과를 일시적으로 저장하여, 동일한 요청이 있을 때 다시 계산하거나 원본 데이터 소스에 액세스할 필요 없이 빠르게 결과를 반환하는 방법입니다.\\n\\n랭체인과 같은 프레임워크에서 캐싱은 일반적으로 다음과 같은 방식으로 사용됩니다:\\n\\n1. **API 호출 결과 저장**: 동일한 쿼리나 요청에 대한 결과를 메모리에 저장함으로써, 반복적인 호출 시 성능을 개선할 수 있습니다.\\n\\n2. **유사한 데이터 요청 최적화**: 비슷한 요청이 반복될 때, 캐시에 저장된 결과를 활용하여 처리 시간을 단축할 수 있습니다.\\n\\n3. **비용 절감**: 외부 API나 데이터베이스에 대한 호출을 감소시킴으로써 사용 비용을 절감할 수 있습니다.\\n\\n4. **저장 방식**: 캐시는 메모리 내 캐시, 디스크 캐시 등 다양한 방식으로 구현될 수 있으며, 상황에 따라 적절한 캐시 전략을 선택하여 사용할 수 있습니다.\\n\\n랭체인에서의 캐싱은 특히 자연어 처리나 대규모 데이터 작업 시, 효율성을 높이고 성능을 개선하는 데 중요한 역할을 합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 298, 'prompt_tokens': 19, 'total_tokens': 317, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-183b89b7-748d-4cfa-9f5d-25b56040e253-0' usage_metadata={'input_tokens': 19, 'output_tokens': 298, 'total_tokens': 317, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:03.540993\n",
      "content='랭체인(LangChain)에서 캐싱은 주로 데이터 처리 성능을 향상시키기 위해 사용됩니다. 캐싱은 자주 사용되는 데이터나 API 호출 결과를 일시적으로 저장하여, 동일한 요청이 있을 때 다시 계산하거나 원본 데이터 소스에 액세스할 필요 없이 빠르게 결과를 반환하는 방법입니다.\\n\\n랭체인과 같은 프레임워크에서 캐싱은 일반적으로 다음과 같은 방식으로 사용됩니다:\\n\\n1. **API 호출 결과 저장**: 동일한 쿼리나 요청에 대한 결과를 메모리에 저장함으로써, 반복적인 호출 시 성능을 개선할 수 있습니다.\\n\\n2. **유사한 데이터 요청 최적화**: 비슷한 요청이 반복될 때, 캐시에 저장된 결과를 활용하여 처리 시간을 단축할 수 있습니다.\\n\\n3. **비용 절감**: 외부 API나 데이터베이스에 대한 호출을 감소시킴으로써 사용 비용을 절감할 수 있습니다.\\n\\n4. **저장 방식**: 캐시는 메모리 내 캐시, 디스크 캐시 등 다양한 방식으로 구현될 수 있으며, 상황에 따라 적절한 캐시 전략을 선택하여 사용할 수 있습니다.\\n\\n랭체인에서의 캐싱은 특히 자연어 처리나 대규모 데이터 작업 시, 효율성을 높이고 성능을 개선하는 데 중요한 역할을 합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 298, 'prompt_tokens': 19, 'total_tokens': 317, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-183b89b7-748d-4cfa-9f5d-25b56040e253-0' usage_metadata={'input_tokens': 19, 'output_tokens': 298, 'total_tokens': 317, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:00.001001\n",
      "content='랭체인( LangChain)에서 캐싱은 일반적으로 응답 시간을 단축하고 API 호출 비용을 절감하기 위해 이전에 처리된 요청과 그에 대한 응답을 저장하는 기술을 의미합니다. 캐시를 사용하면 동일한 요청이 들어오면 이미 저장된 결과를 빠르게 반환함으로써 반복적인 작업을 줄이고 시스템의 성능을 향상시킬 수 있습니다.\\n\\n랭체인에서는 이러한 캐싱 메커니즘이 여러 방법으로 구현될 수 있습니다. 예를 들어, 데이터베이스 쿼리 결과, 모델의 응답, 또는 처리된 텍스트와 같은 다양한 요소를 캐시할 수 있습니다. 이를 통해 모델의 사용 효율성을 높이고, 사용자 경험을 향상시킬 수 있습니다.\\n\\n캐싱을 적절히 사용하면 자원의 낭비를 줄이고, 시스템의 전반적인 성능을 개선하는 데 도움이 됩니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 19, 'total_tokens': 216, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-bdd0f0a8-77a9-4965-88ce-a8817afbaa0e-0' usage_metadata={'input_tokens': 19, 'output_tokens': 197, 'total_tokens': 216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:03.008828\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache  # 메모리에 저장\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "\n",
    "start = datetime.now()\n",
    "response1 = llm.invoke('랭체인에서 캐싱이란 뭐야?')\n",
    "print(response1)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response2 = llm.invoke('랭체인에서 캐싱이란 뭐야?')\n",
    "print(response2)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response3 = llm.invoke('랭체인에서 캐싱이란 뭐게?')\n",
    "print(response3)\n",
    "print(datetime.now() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='랭체인(LangChain)에서 캐싱은 성능을 향상시키기 위해 이전에 계산된 결과를 저장하여 동일한 요청에 대해 반복적으로 계산하지 않고 저장된 결과를 재사용하는 과정을 말합니다. 이는 특히 자연어 처리 모델이나 API 호출과 같은 시간이 소요되는 작업에서 유용할 수 있습니다.\\n\\n캐싱의 이점은 다음과 같습니다:\\n\\n1. **속도 향상**: 동일한 요청에 대해 미리 계산된 결과를 반환하므로 응답 시간이 단축됩니다.\\n2. **비용 절감**: 외부 API 호출이나 복잡한 계산을 줄임으로써 사용 비용을 절감할 수 있습니다.\\n3. **자원 절약**: 서버의 부하를 줄이고, 더 많은 요청을 처리할 수 있는 자원을 확보할 수 있습니다.\\n\\n랭체인에서는 다양한 캐싱 전략을 지원하여 개발자가 필요에 맞게 캐시를 구성할 수 있도록 하고 있습니다. 이를 통해 성능을 최적화하고, 애플리케이션의 효율성을 높일 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 19, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-4e1971ec-1633-47ce-9cc6-f05134cc5b5b-0' usage_metadata={'input_tokens': 19, 'output_tokens': 230, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:04.356176\n",
      "content='랭체인(LangChain)에서 캐싱은 성능을 향상시키기 위해 이전에 계산된 결과를 저장하여 동일한 요청에 대해 반복적으로 계산하지 않고 저장된 결과를 재사용하는 과정을 말합니다. 이는 특히 자연어 처리 모델이나 API 호출과 같은 시간이 소요되는 작업에서 유용할 수 있습니다.\\n\\n캐싱의 이점은 다음과 같습니다:\\n\\n1. **속도 향상**: 동일한 요청에 대해 미리 계산된 결과를 반환하므로 응답 시간이 단축됩니다.\\n2. **비용 절감**: 외부 API 호출이나 복잡한 계산을 줄임으로써 사용 비용을 절감할 수 있습니다.\\n3. **자원 절약**: 서버의 부하를 줄이고, 더 많은 요청을 처리할 수 있는 자원을 확보할 수 있습니다.\\n\\n랭체인에서는 다양한 캐싱 전략을 지원하여 개발자가 필요에 맞게 캐시를 구성할 수 있도록 하고 있습니다. 이를 통해 성능을 최적화하고, 애플리케이션의 효율성을 높일 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 19, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-4e1971ec-1633-47ce-9cc6-f05134cc5b5b-0' usage_metadata={'input_tokens': 19, 'output_tokens': 230, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:00.002002\n",
      "content='랭체인(LLM Chain)에서 캐싱(Caching)은 주로 모델의 응답이나 중간 결과물을 저장하여, 동일한 입력에 대해 반복적으로 요청할 때 이미 계산된 결과를 빠르게 반환하는 기법을 말합니다. 이를 통해 모델의 처리 시간을 단축하고 시스템의 효율성을 높일 수 있습니다.\\n\\n캐싱의 장점은 다음과 같습니다:\\n\\n1. **성능 향상**: 동일한 요청에 대한 응답을 빠르게 제공할 수 있어 사용자 경험을 개선합니다.\\n2. **비용 절감**: 인공지능 모델을 주기적으로 호출하는 경우, API 사용료나 자원 소모를 줄일 수 있습니다.\\n3. **자원 관리**: 서버나 클라우드 환경에서 시스템 자원을 효율적으로 관리할 수 있습니다.\\n\\n기본적으로 캐싱은 메모리나 데이터베이스와 같은 저장소에 결과를 저장하고, 그 저장된 데이터를 필요할 때 재사용하는 원리로 작동합니다. LLM Chain에서는 자주 사용되는 질문이나 요청과 그에 대한 답변을 캐시함으로써, 더 빠르고 효율적으로 처리할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 249, 'prompt_tokens': 19, 'total_tokens': 268, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-cc567f37-1e5b-43fe-aa62-69813d9d60aa-0' usage_metadata={'input_tokens': 19, 'output_tokens': 249, 'total_tokens': 268, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:06.673056\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache  # 파일형태로 저장됨\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists('cache'):\n",
    "    os.mkdir('cache')\n",
    "    \n",
    "set_llm_cache(SQLiteCache(database_path='cache/llm_cache.db'))\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "start = datetime.now()\n",
    "response1 = llm.invoke('랭체인에서 캐싱이란 뭐야?')\n",
    "print(response1)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response2 = llm.invoke('랭체인에서 캐싱이란 뭐야?')\n",
    "print(response2)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response3 = llm.invoke('랭체인에서 캐싱이란 뭐게?')\n",
    "print(response3)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
