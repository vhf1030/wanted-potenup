{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-ai/DeepSeek-R1\n",
      "deepseek-ai/Janus-Pro-7B\n",
      "deepseek-ai/DeepSeek-V3\n",
      "unsloth/DeepSeek-R1-GGUF\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "deepseek-ai/deepseek-vl2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# API 인스턴스 생성\n",
    "api = HfApi()\n",
    "\n",
    "# 모델 검색 (예: 'deepseek' 관련 모델 찾기)\n",
    "models = api.list_models(search=\"deepseek\", limit=10)\n",
    "\n",
    "# 결과 출력\n",
    "for model in models:\n",
    "    print(model.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "# # model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 모델 다운로드\n",
    "# $ huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir ./models/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "\n",
    "# 로컬 모델 경로 지정\n",
    "local_model_path = \"../models/\"\n",
    "model_name = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# 로컬에서 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path + model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path + model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's weather is going to be tricky. You have a weather app that you need to check. You have a weather app that you need to check. You have a weather app that you need to check. You have a weather app that you\n"
     ]
    }
   ],
   "source": [
    "# 입력 텍스트 준비\n",
    "prompt = \"Today's weather is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # 입력 데이터도 GPU로 이동\n",
    "\n",
    "# 모델을 이용한 텍스트 생성\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=50,  # 생성할 최대 토큰 길이\n",
    "        num_return_sequences=1,  # 생성할 문장 수\n",
    "        do_sample=True,  # 샘플링 여부\n",
    "        top_p=0.95,  # nucleus sampling\n",
    "        top_k=50     # top-k 샘플링\n",
    "    )\n",
    "\n",
    "# 결과 디코딩 및 출력\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Chatbot: Tell me a joke about AI. It needs to be surprising and have something to do with AI. Maybe it's about how AI is changing jobs? Or something like that.\n",
      "\n",
      "Okay, I'm thinking about something funny. Maybe a joke that plays on the idea that AI is moving jobs around, but the punchline is that AI is so good at it that it doesn't need the jobs anymore. Or maybe it's about how AI is just as human as humans and is constantly evolving. Or\n"
     ]
    }
   ],
   "source": [
    "# 사용자 입력 예시\n",
    "user_input = \"Tell me a joke about AI.\"\n",
    "\n",
    "# 입력을 토크나이징\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델로 응답 생성\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=100,  # 답변 길이 제한\n",
    "        temperature=0.8,  # 다양성 조절\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "# 응답 출력\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"🤖 Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📢 Review Summary: \n",
      "This phone has a great camera and an amazing battery life. \n",
      "However, the screen sometimes flickers, which is quite annoying. \n",
      "Overall, it is a good purchase for the price.\n",
      "But the problem is that the battery life is not\n"
     ]
    }
   ],
   "source": [
    "# 긴 리뷰 입력\n",
    "review_text = \"\"\"\n",
    "This phone has a great camera and an amazing battery life. \n",
    "However, the screen sometimes flickers, which is quite annoying. \n",
    "Overall, it is a good purchase for the price.\n",
    "\"\"\"\n",
    "\n",
    "# 입력을 토크나이징\n",
    "inputs = tokenizer(review_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델로 요약 생성\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "# 요약된 텍스트 출력\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"📢 Review Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🇰🇷 Translated: Translate this to Korean: The restaurant serves delicious pasta with fresh ingredients. The chef is really talented. The food is prepared with traditional techniques.\n",
      "\n",
      "Okay, I have a problem with this. The problem is that the restaurant is so busy, and I can't get a seat. I need to figure out a way to get a table, but I don't know the restaurant's seating arrangements.\n",
      "\n",
      "The manager is a bit unprofessional. I don't think they pay attention to the customer's needs.\n"
     ]
    }
   ],
   "source": [
    "# 영어 문장 입력\n",
    "english_text = \"The restaurant serves delicious pasta with fresh ingredients.\"\n",
    "\n",
    "# 입력을 토크나이징\n",
    "inputs = tokenizer(\"Translate this to Korean: \" + english_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델로 번역 생성\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "# 번역된 결과 출력\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"🇰🇷 Translated:\", translated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
