{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플코드 체험하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"  # 임베딩할 문장\n",
    "encoded_input = tokenizer(text, return_tensors='pt')  # 단어 토크나이징\n",
    "output = model(**encoded_input)  # 임베딩 벡터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[  101, 72337, 72654, 10911, 10155, 11178, 15541, 13028,   112,   172,\n",
      "         11850,   119,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 탐색 1. Tokenizer\n",
    "print(type(encoded_input))\n",
    "print(encoded_input.keys())\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print(encoded_input[\"token_type_ids\"])\n",
    "print(encoded_input[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐색 2. model에 넣어줘야 하는 값\n",
    "# model(**encoded_input)\n",
    "# 입력값이 딕셔너리여야 하며 그 키는 input_ids, token_type_ids, attention_mask\n",
    "# 즉, 모델에 대한 입력값은 tokenizer의 결과 출력값임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 탐색 3. model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2524, -0.5321,  0.4496,  ...,  1.1428, -0.6238, -0.0745],\n",
      "         [ 0.5374,  0.0701,  0.4880,  ...,  1.0800, -0.5560, -0.5564],\n",
      "         [ 0.4686, -0.3286,  0.4782,  ...,  1.1656, -0.7389, -0.3920],\n",
      "         ...,\n",
      "         [ 0.4822, -1.0822,  0.9026,  ...,  1.8029, -1.1343, -0.1034],\n",
      "         [ 0.1283, -0.6550,  0.3723,  ...,  0.7817, -0.9173, -0.0401],\n",
      "         [ 0.1645, -0.5239,  0.6638,  ...,  0.7964, -0.7326, -0.2642]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.3479, -0.0466,  0.4564, -0.2464, -0.0563,  0.5865,  0.4807,  0.2475,\n",
      "         -0.5663,  0.4260,  0.0136, -0.3356, -0.3281,  0.0391,  0.3874, -0.4077,\n",
      "          0.8111,  0.2171,  0.4523, -0.5184, -0.9998, -0.5465, -0.1472, -0.5278,\n",
      "         -0.5858,  0.3390, -0.4160,  0.3991,  0.4012, -0.4298,  0.1618, -0.9998,\n",
      "          0.7481,  0.7902,  0.3325, -0.4037,  0.1348,  0.2374,  0.2554, -0.1603,\n",
      "         -0.3249,  0.0911, -0.4920,  0.0966, -0.0886, -0.4324, -0.3835,  0.3972,\n",
      "         -0.5554,  0.2169,  0.1148,  0.3260,  0.4943,  0.4352,  0.3529,  0.2036,\n",
      "          0.3909,  0.1889,  0.5300, -0.4648, -0.0521,  0.5751,  0.2526, -0.0612,\n",
      "         -0.2219, -0.4394,  0.0356, -0.3328,  0.5685, -0.3036, -0.2594, -0.4530,\n",
      "         -0.2713,  0.1009,  0.2301, -0.3336,  0.5517,  0.4013,  0.1831, -0.3138,\n",
      "         -0.5992, -0.5676, -0.5336,  0.3356, -0.2990,  0.5137,  0.2741, -0.5048,\n",
      "          0.2117, -0.0015,  0.3309,  0.6298, -0.2695,  0.2346, -0.3509, -0.4789,\n",
      "         -0.8909, -0.3698, -0.4985, -0.5266, -0.4290,  0.1720, -0.2786, -0.3678,\n",
      "         -0.1528, -0.5241,  0.3222,  0.3790, -0.3511,  0.4393,  0.3300, -0.5506,\n",
      "         -0.2684,  0.2293, -0.4590,  0.9924, -0.5686,  0.5286,  0.0708, -0.2181,\n",
      "         -0.6942,  0.9998,  0.1417, -0.2129,  0.2732,  0.3654, -0.6268,  0.2804,\n",
      "          0.4681,  0.4806,  0.4627, -0.2392, -0.3762, -0.4162, -0.9208, -0.3348,\n",
      "         -0.5620,  0.2746, -0.5168, -0.4194,  0.2401,  0.3975,  0.2670, -0.1696,\n",
      "         -0.3278, -0.0907,  0.5337, -0.3277,  0.9998,  0.6998, -0.3795, -0.0534,\n",
      "          0.7170, -0.7487, -0.3792, -0.3239, -0.3306, -0.7024,  0.3410,  0.3298,\n",
      "          0.3016, -0.3909, -0.5636, -0.3942,  0.2903, -0.7446, -0.3552,  0.5690,\n",
      "          0.4293,  0.4615, -0.2927,  0.5457,  0.4178, -0.2542, -0.3220,  0.5368,\n",
      "          0.3799, -0.2040, -0.4458, -0.1065,  0.3341, -0.3019, -0.6492,  0.1437,\n",
      "         -0.2294, -0.5867,  0.2758, -0.3071, -0.0754,  0.2141, -0.5020,  0.3125,\n",
      "         -0.5100,  0.4330,  0.5638,  0.2154, -0.4454,  0.3568,  0.5825,  0.5002,\n",
      "          0.4783,  0.1944,  0.3854,  0.3343, -0.4531, -0.8202,  0.5794,  0.3621,\n",
      "          0.4884, -0.2074, -0.5437, -0.5644,  0.7088,  0.4359, -0.3276,  0.5428,\n",
      "          0.4145, -0.4658, -0.3687,  0.3062, -0.1896, -0.5597, -0.5004, -0.4566,\n",
      "         -0.1157,  0.4746,  0.2691,  0.1165,  0.1806, -0.2733, -0.2113, -0.4532,\n",
      "          0.2552,  0.2581, -0.1887,  0.9240, -0.3420,  0.1755, -0.5977, -0.2486,\n",
      "          0.3691, -0.2093,  0.3388,  0.9882,  0.2615, -0.4248,  0.1601,  0.2186,\n",
      "          0.2783, -0.3623,  0.1934, -0.7431,  0.8718,  0.5192,  0.3146, -0.9998,\n",
      "          0.3282,  0.1694,  0.2693,  0.3528,  0.3718,  0.2386,  0.3505,  0.9561,\n",
      "         -0.6181, -0.5637, -0.6078, -0.3900, -0.6018, -0.3526, -0.4228, -0.3809,\n",
      "         -0.3002, -0.2034, -0.2592,  0.3707,  0.3969, -0.9981,  0.9230,  0.2382,\n",
      "         -0.2377, -0.0136,  0.4744, -0.9998,  0.4307, -0.2486, -0.4819,  0.4280,\n",
      "         -0.6735, -0.3429,  0.2829,  0.2688,  0.4592,  0.3448,  0.1427,  0.5622,\n",
      "         -0.1548,  0.0800,  0.2396, -0.2551,  0.7024, -0.0637,  0.2483,  0.4902,\n",
      "         -0.0604,  0.4824, -0.4325,  0.5296,  0.5423,  0.1549,  0.2383, -0.3534,\n",
      "          0.4392, -0.8413,  0.4954, -0.0289, -0.1962, -0.0201,  0.3012, -0.2925,\n",
      "         -0.2265,  0.2662, -0.6172,  0.9998,  0.4016, -0.4316, -0.6277,  0.5666,\n",
      "          0.7398, -0.3263, -0.8886, -0.2034,  0.6460,  0.4737,  0.2391,  0.3137,\n",
      "          0.2394,  0.2663, -0.1722, -0.2366, -0.0762, -0.5351,  0.4793, -0.2669,\n",
      "         -0.3655,  0.4261, -0.4167, -0.3533, -0.9110,  0.5237,  0.4789,  0.2668,\n",
      "          0.2215,  0.2964, -0.3993,  0.7121,  0.6148, -0.3470, -0.2579, -0.3197,\n",
      "         -0.3302,  0.0703, -0.3236, -0.3757,  0.3013, -0.7673,  0.2004, -0.1767,\n",
      "         -0.3236, -0.4728,  0.5218, -0.9998, -0.1971,  0.2426, -0.3627,  0.5291,\n",
      "         -0.4952, -0.1579,  0.1851,  0.3242,  0.0453,  0.4090, -0.4824,  0.3272,\n",
      "         -0.2402,  0.3627,  0.8409,  0.7796,  0.2317, -0.4103,  0.3347, -0.5352,\n",
      "         -0.1233,  0.5097,  0.4353, -0.1352,  0.3606,  0.4346,  0.3564, -0.3667,\n",
      "          0.4498, -0.2681, -0.3671,  0.3424,  0.1276, -0.2811, -0.5382,  0.2924,\n",
      "         -0.5711,  0.4475,  0.3389,  0.5766,  0.3297,  0.4118, -0.3788, -0.3290,\n",
      "         -0.1866, -0.1894, -0.5205, -0.5004, -0.3591,  0.9998,  0.4138,  0.4436,\n",
      "         -0.5978,  0.4752,  0.2903, -0.3527,  0.3584,  0.5775,  0.2543, -0.0140,\n",
      "          0.2847,  0.4611,  0.3658,  0.5154,  0.5331,  0.6123, -0.4101,  0.7353,\n",
      "         -0.3032, -0.5571, -0.9984,  0.3908,  0.6632, -0.4190, -0.8705,  0.3718,\n",
      "         -0.3502,  0.2786, -0.4557,  0.0131,  0.3753, -0.3060,  0.5765, -0.2056,\n",
      "          0.9997, -0.2336,  0.3872,  0.4641,  0.3707, -0.2764, -0.4731, -0.4289,\n",
      "          0.4056, -0.4280,  0.1517, -0.9720,  0.4526,  0.0424,  0.3430, -0.2254,\n",
      "          0.5788, -0.5043,  0.4951, -0.1597, -0.4305, -0.3821,  0.4350, -0.5796,\n",
      "          0.5389, -0.3024,  0.1203, -0.4169,  0.3182, -0.1258,  0.5588, -0.2346,\n",
      "          0.0680, -0.3068, -0.3093, -0.4831,  0.1124, -0.3321,  0.9998, -0.1196,\n",
      "          0.5176, -0.4022,  0.4818, -0.4652,  0.7185,  0.7791, -0.2603,  0.4564,\n",
      "          0.4134, -0.7492,  0.4928, -0.1487, -0.8899, -0.0349,  0.9843,  0.2287,\n",
      "          0.2933,  0.7056,  0.4445,  0.5416, -0.3607,  0.2753,  0.9175,  0.2576,\n",
      "          0.2043,  0.3301,  0.1690, -0.2744, -0.4724,  0.9998,  0.9997, -0.0657,\n",
      "          0.3945, -0.3565, -0.3675, -0.3862,  0.2491,  0.1906,  0.3696, -0.2029,\n",
      "          0.0978, -0.4928, -0.2283, -0.2088, -0.3244, -0.3628,  0.2238, -0.5318,\n",
      "          0.6601,  0.4634,  0.3248,  0.5380,  0.4754,  0.3531, -0.2502, -0.4468,\n",
      "          0.5027, -0.4210, -0.2843, -0.3626,  0.2704, -0.9997, -0.3237, -0.3378,\n",
      "         -0.3674,  0.6854,  0.3904,  0.1491, -0.3761, -0.2340, -0.4994,  0.3760,\n",
      "          0.0038,  0.3809, -0.3853, -0.3365,  0.5422, -0.7120,  0.2767, -0.4304,\n",
      "         -0.4116, -0.6568, -0.4123, -0.3864,  0.4490, -0.3626, -0.3944,  0.5106,\n",
      "          0.4320,  0.3441, -0.4078,  0.3311, -0.2481,  0.0281,  0.5284,  0.3057,\n",
      "          0.3534, -0.4360, -0.4805, -0.3101, -0.4968, -0.2919,  0.1183, -0.3756,\n",
      "          0.3986, -0.3174,  0.3155, -0.3968,  0.0747,  0.4007,  0.5872, -0.2769,\n",
      "          0.6463,  0.5545, -0.2109,  0.6229,  0.3648, -0.3702, -0.4417,  0.9998,\n",
      "          0.5258,  0.2709, -0.0111, -0.1960,  0.3316,  0.3600,  0.6899, -0.4168,\n",
      "          0.8816, -0.4170,  0.3142,  0.5387,  0.5203,  0.0401,  0.4016,  0.3202,\n",
      "          0.8566,  0.4476,  0.5010,  0.5407,  0.1573,  0.5883,  0.4044,  0.3473,\n",
      "          0.4646,  0.4544, -0.4164,  0.4577, -0.3750, -0.2294, -0.0772, -0.1680,\n",
      "         -0.3458,  0.0120, -0.2115, -0.2273, -0.1055,  0.5045, -0.3115,  0.1304,\n",
      "         -0.1694, -0.3133,  0.6474, -0.6750,  0.4520, -0.1844,  0.1244, -0.8716,\n",
      "          0.4054, -0.2088, -0.5018, -0.3058, -0.6826,  0.3779,  0.4047, -0.4849,\n",
      "          0.4524, -0.1988,  0.3167, -0.3646, -0.3537,  0.2657, -0.9998,  0.2529,\n",
      "          0.3900, -0.4465,  0.3725,  0.0813,  0.3133,  0.4863, -0.5000, -0.5793,\n",
      "         -0.3430,  0.4808, -0.3672,  0.1632,  0.4605, -0.5725, -0.2863,  0.1940,\n",
      "         -0.3025,  0.3865,  0.5675, -0.3394,  0.5261, -0.4529,  0.1571, -0.4972,\n",
      "          0.2650, -0.5463, -0.2356,  0.4190, -0.4152, -0.6399, -0.0933,  0.3661,\n",
      "         -0.2407,  0.2780,  0.4938, -0.1811,  0.4346, -0.4499,  0.5624, -0.3171,\n",
      "          0.3187, -0.8939, -0.3868, -0.6404, -0.1275,  0.3002,  0.6523,  0.1678,\n",
      "          0.4010, -0.2698,  0.0510, -0.1523,  0.3995,  0.4114, -0.2815,  0.0711,\n",
      "         -0.3346,  0.2971, -0.5793,  0.0084, -0.9974, -0.4989,  0.2654,  0.5173,\n",
      "          0.4936, -0.3613, -0.2462, -0.4643, -0.2649,  0.2476,  0.3840,  0.3994,\n",
      "          0.4215,  0.4807, -0.2922, -0.2126,  0.8593, -0.3262,  0.2572,  0.4530,\n",
      "          0.5275,  0.9433,  0.3779,  0.3931,  0.3170, -0.4342,  0.5104,  0.4348]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 유사도: 0.9452\n",
      "문장 유사도: 0.9084\n",
      "문장 유사도: 0.8817\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 비교할 문장\n",
    "sentence1 = \"나는 오늘 날씨가 좋아서 기분이 좋다.\"\n",
    "sentence2 = \"맑은 하늘을 보니 기분이 상쾌하다.\"\n",
    "sentence3 = \"허깅페이스 모델 활용 방법을 배우고 있다.\"\n",
    "sentence4 = \"BERT는 구글에서 개발한 자연어 처리 모델로, Transformer 기반의 사전 학습된 모델이다.\"\n",
    "\n",
    "# 토큰화 및 임베딩 추출 함수\n",
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    return output.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "# 문장 임베딩 생성\n",
    "embedding1 = get_embedding(sentence1)\n",
    "embedding2 = get_embedding(sentence2)\n",
    "embedding3 = get_embedding(sentence3)\n",
    "embedding4 = get_embedding(sentence4)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "print(f\"문장 유사도: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity([embedding1], [embedding3])[0][0]\n",
    "print(f\"문장 유사도: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity([embedding1], [embedding4])[0][0]\n",
    "print(f\"문장 유사도: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>갑자기 로그아웃 되더니 비밀번호 변경 실패 메세지가 계속 뜨네요. 휴대폰 번호로 인...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>기사님이 상품 픽업을 하셨는지 표시되면 더 좋을 것 같습니다. 가게에서 조리가 늦게...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>요즘 요기요 보다 배민을 많이 쓰는 사람입니다 전화보다 앱을 써서 좀더 간편하고 다...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>취소 됐으면 적어도 전화 주는 제도는 있어야하는거 아닌가요? 주문해놓고 다른거 하는...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>배달의 민족 참 잘 사용하고 있어요. 별 다섯 개를 드리고 싶지만 한 가지 아쉬운 ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  score\n",
       "0    배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...      4\n",
       "1    내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...      5\n",
       "2    검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...      1\n",
       "3    배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...      2\n",
       "4    최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...      3\n",
       "..                                                 ...    ...\n",
       "995  갑자기 로그아웃 되더니 비밀번호 변경 실패 메세지가 계속 뜨네요. 휴대폰 번호로 인...      1\n",
       "996  기사님이 상품 픽업을 하셨는지 표시되면 더 좋을 것 같습니다. 가게에서 조리가 늦게...      3\n",
       "997  요즘 요기요 보다 배민을 많이 쓰는 사람입니다 전화보다 앱을 써서 좀더 간편하고 다...      3\n",
       "998  취소 됐으면 적어도 전화 주는 제도는 있어야하는거 아닌가요? 주문해놓고 다른거 하는...      1\n",
       "999  배달의 민족 참 잘 사용하고 있어요. 별 다섯 개를 드리고 싶지만 한 가지 아쉬운 ...      4\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./data/배달의민족댓글2.csv\", index_col=0).dropna().reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>배달의민족 주문시 리뷰를 자주 참고하는 편입니다  한가지 건의사항이 있다면 최신순 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요   분명 이 가게에 시킨 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>검색 화면에서 전체 배달 포장 탭 중 배달 탭을 스크롤 내리면서 볼 때  아래로 스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>배달팁 낮은 순으로 정렬하면 0 4000원 이런식으로 된 가게가 가장 위로 올라옵니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다   배민 어플 실행시 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>갑자기 로그아웃 되더니 비밀번호 변경 실패 메세지가 계속 뜨네요. 휴대폰 번호로 인...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>갑자기 로그아웃 되더니 비밀번호 변경 실패 메세지가 계속 뜨네요  휴대폰 번호로 인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>기사님이 상품 픽업을 하셨는지 표시되면 더 좋을 것 같습니다. 가게에서 조리가 늦게...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>기사님이 상품 픽업을 하셨는지 표시되면 더 좋을 것 같습니다  가게에서 조리가 늦게...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>요즘 요기요 보다 배민을 많이 쓰는 사람입니다 전화보다 앱을 써서 좀더 간편하고 다...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>요즘 요기요 보다 배민을 많이 쓰는 사람입니다 전화보다 앱을 써서 좀더 간편하고 다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>취소 됐으면 적어도 전화 주는 제도는 있어야하는거 아닌가요? 주문해놓고 다른거 하는...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>취소 됐으면 적어도 전화 주는 제도는 있어야하는거 아닌가요  주문해놓고 다른거 하는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>배달의 민족 참 잘 사용하고 있어요. 별 다섯 개를 드리고 싶지만 한 가지 아쉬운 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>배달의 민족 참 잘 사용하고 있어요  별 다섯 개를 드리고 싶지만 한 가지 아쉬운 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  score  label  \\\n",
       "0    배달의민족 주문시 리뷰를 자주 참고하는 편입니다. 한가지 건의사항이 있다면 최신순,...      4      1   \n",
       "1    내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요.. 분명 이 가게에 시킨 ...      5      1   \n",
       "2    검색 화면에서 전체/배달/포장 탭 중 배달 탭을 스크롤 내리면서 볼 때, 아래로 스...      1      0   \n",
       "3    배달팁 낮은 순으로 정렬하면 0~4000원 이런식으로 된 가게가 가장 위로 올라옵니...      2      0   \n",
       "4    최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다.. 배민 어플 실행시 ...      3      1   \n",
       "..                                                 ...    ...    ...   \n",
       "995  갑자기 로그아웃 되더니 비밀번호 변경 실패 메세지가 계속 뜨네요. 휴대폰 번호로 인...      1      0   \n",
       "996  기사님이 상품 픽업을 하셨는지 표시되면 더 좋을 것 같습니다. 가게에서 조리가 늦게...      3      1   \n",
       "997  요즘 요기요 보다 배민을 많이 쓰는 사람입니다 전화보다 앱을 써서 좀더 간편하고 다...      3      1   \n",
       "998  취소 됐으면 적어도 전화 주는 제도는 있어야하는거 아닌가요? 주문해놓고 다른거 하는...      1      0   \n",
       "999  배달의 민족 참 잘 사용하고 있어요. 별 다섯 개를 드리고 싶지만 한 가지 아쉬운 ...      4      1   \n",
       "\n",
       "                                                review  \n",
       "0    배달의민족 주문시 리뷰를 자주 참고하는 편입니다  한가지 건의사항이 있다면 최신순 ...  \n",
       "1    내가 주문했던 과거목록에서도 검색기능이 있었으면 좋겠어요   분명 이 가게에 시킨 ...  \n",
       "2    검색 화면에서 전체 배달 포장 탭 중 배달 탭을 스크롤 내리면서 볼 때  아래로 스...  \n",
       "3    배달팁 낮은 순으로 정렬하면 0 4000원 이런식으로 된 가게가 가장 위로 올라옵니...  \n",
       "4    최근 업데이트가 안드로이드5사양 정도에서는 안되는것 같습니다   배민 어플 실행시 ...  \n",
       "..                                                 ...  \n",
       "995  갑자기 로그아웃 되더니 비밀번호 변경 실패 메세지가 계속 뜨네요  휴대폰 번호로 인...  \n",
       "996  기사님이 상품 픽업을 하셨는지 표시되면 더 좋을 것 같습니다  가게에서 조리가 늦게...  \n",
       "997  요즘 요기요 보다 배민을 많이 쓰는 사람입니다 전화보다 앱을 써서 좀더 간편하고 다...  \n",
       "998  취소 됐으면 적어도 전화 주는 제도는 있어야하는거 아닌가요  주문해놓고 다른거 하는...  \n",
       "999  배달의 민족 참 잘 사용하고 있어요  별 다섯 개를 드리고 싶지만 한 가지 아쉬운 ...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# score 3이상이면 긍정(1), 그 외에는 부정(0)\n",
    "data[\"label\"] = np.where(data[\"score\"] >= 3, 1, 0)\n",
    "data[\"review\"] = data[\"text\"].apply(lambda x: re.sub(\"[^0-9a-zA-Z가-힣\\s+]\", \" \", x))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    data[\"review\"], data[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "trainX, valX, trainY, valY = train_test_split(\n",
    "    trainX, trainY, test_size=0.1, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX: (720,), TrainY: (720,)\n",
      "ValX: (80,), ValY: (80,)\n",
      "TestX: (200,), TestY: (200,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"TrainX: {trainX.shape}, TrainY: {trainY.shape}\")\n",
    "print(f\"ValX: {valX.shape}, ValY: {valY.shape}\")\n",
    "print(f\"TestX: {testX.shape}, TestY: {testY.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 만들기\n",
    "# CustomDataset Class를 만든다\n",
    "# __len__: 데이터의 개수 반환\n",
    "# __getitem__: 학습데이터 1개의 x,y쌍을 반환\n",
    "#               -> tokenizer()의 출력(딕셔너리) + label키를 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트1 - 샘플코드 그대로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "torch.Size([74])\n",
      "torch.Size([74])\n",
      "torch.Size([74])\n",
      "torch.Size([42])\n",
      "torch.Size([42])\n",
      "torch.Size([42])\n",
      "torch.Size([65])\n",
      "torch.Size([65])\n",
      "torch.Size([65])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer)\n",
    "print(len(train_dataset))\n",
    "# print(train_dataset[0])\n",
    "print(train_dataset[0][\"input_ids\"].shape)\n",
    "print(train_dataset[0][\"attention_mask\"].shape)\n",
    "print(train_dataset[0][\"token_type_ids\"].shape)\n",
    "print(train_dataset[1][\"input_ids\"].shape)\n",
    "print(train_dataset[1][\"attention_mask\"].shape)\n",
    "print(train_dataset[1][\"token_type_ids\"].shape)\n",
    "print(train_dataset[2][\"input_ids\"].shape)\n",
    "print(train_dataset[2][\"attention_mask\"].shape)\n",
    "print(train_dataset[2][\"token_type_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트2 -조건 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            max_length = 128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"],\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer)\n",
    "print(len(train_dataset))\n",
    "# print(train_dataset[0])\n",
    "print(train_dataset[0][\"input_ids\"].shape)\n",
    "print(train_dataset[0][\"attention_mask\"].shape)\n",
    "print(train_dataset[0][\"token_type_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n",
      "torch.Size([8, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for x in train_loader:\n",
    "    print(x[\"input_ids\"].shape)\n",
    "    print(x[\"attention_mask\"].shape)\n",
    "    print(x[\"token_type_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts.tolist() \n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded_output = self.tokenizer(\n",
    "            text, \n",
    "            max_length = self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_output\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 가져오기\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 토크나이징\n",
    "        encoding = self.tokenize(text)\n",
    "\n",
    "        # 라벨 추가\n",
    "        # encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "tensor([   101,   9689,  25934,  10739,    100,    100,   9524,  26737, 119221,\n",
      "         21711, 119217,   9330,  89851,   9576,  16605,   9485,  96618,   9706,\n",
      "         16439,  11664,  10244,  37712,  43739,   9706,  16439,  12424,   9258,\n",
      "         34907,  10530,   9665,  18227,  35506,  25503,   9524,  14153, 118800,\n",
      "         77884,  48549,   9995, 118992, 118965,  11018,   8898,  16605,  12092,\n",
      "          9357, 119199,  12453,  48549,  62849,  15303,   9580,  46520,  12030,\n",
      "        119219,   9524, 119118,  41850,   9435,  35465,  43739,  80956,   9641,\n",
      "         10739,  90587,   9668,  34907,   9536,  10622,   9521,   9511,  14153,\n",
      "           100,    102,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer, max_length=128)\n",
    "print(len(train_dataset))\n",
    "# print(train_dataset[0])\n",
    "print(train_dataset[0][\"input_ids\"])\n",
    "print(train_dataset[0][\"attention_mask\"])  # 패딩 여부 표시\n",
    "print(train_dataset[0][\"token_type_ids\"])  # 여러 문장인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for x in train_loader:\n",
    "    print(x[\"input_ids\"].shape)\n",
    "    print(x[\"attention_mask\"].shape)\n",
    "    print(x[\"token_type_ids\"].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "max_length = 128\n",
    "train_dataset = CustomDataset(trainX, trainY, tokenizer, max_length)\n",
    "val_dataset = CustomDataset(valX, valY, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(testX, testY, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  9946, 13764,  ...,     0,     0,     0],\n",
      "        [  101,  9689, 25934,  ...,     0,     0,     0],\n",
      "        [  101,  9599, 12310,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  9654, 12945,  ..., 11018, 71439,   102],\n",
      "        [  101,  9930, 10622,  ..., 28188, 77884,   102],\n",
      "        [  101,  9926, 22333,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'label': tensor([1, 1, 1, 1, 0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for x in train_loader:\n",
    "    print(x)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전이학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved: bert_0\n",
      "Epoch: 0\tTrain Loss: 0.1027, Train Acc: 0.9792 | Val Loss: 1.6662, Val Acc: 0.7625\n",
      "Best model saved: bert_1\n",
      "Epoch: 1\tTrain Loss: 0.1441, Train Acc: 0.9750 | Val Loss: 1.5497, Val Acc: 0.7625\n",
      "개선 없음 - Early Stopping 카운터: 1/5\n",
      "Epoch: 2\tTrain Loss: 0.0534, Train Acc: 0.9903 | Val Loss: 1.7842, Val Acc: 0.7250\n",
      "Best model saved: bert_3\n",
      "Epoch: 3\tTrain Loss: 0.0660, Train Acc: 0.9875 | Val Loss: 1.3020, Val Acc: 0.8250\n",
      "개선 없음 - Early Stopping 카운터: 1/5\n",
      "Epoch: 4\tTrain Loss: 0.0433, Train Acc: 0.9917 | Val Loss: 1.6420, Val Acc: 0.7875\n",
      "개선 없음 - Early Stopping 카운터: 2/5\n",
      "Epoch: 5\tTrain Loss: 0.0615, Train Acc: 0.9917 | Val Loss: 1.8206, Val Acc: 0.7125\n",
      "개선 없음 - Early Stopping 카운터: 3/5\n",
      "Epoch: 6\tTrain Loss: 0.0788, Train Acc: 0.9861 | Val Loss: 2.2907, Val Acc: 0.7125\n",
      "Epoch: 7\tTrain Loss: 0.0311, Train Acc: 0.9944 | Val Loss: 2.1132, Val Acc: 0.7375\n",
      "Epoch: 8\tTrain Loss: 0.0309, Train Acc: 0.9958 | Val Loss: 1.8129, Val Acc: 0.7625\n",
      "개선 없음 - Early Stopping 카운터: 1/5\n",
      "Epoch: 9\tTrain Loss: 0.0207, Train Acc: 0.9972 | Val Loss: 3.0796, Val Acc: 0.6375\n",
      "Epoch: 10\tTrain Loss: 0.0527, Train Acc: 0.9917 | Val Loss: 2.5055, Val Acc: 0.6750\n",
      "Epoch: 11\tTrain Loss: 0.0195, Train Acc: 0.9958 | Val Loss: 1.8142, Val Acc: 0.7750\n",
      "개선 없음 - Early Stopping 카운터: 1/5\n",
      "Epoch: 12\tTrain Loss: 0.0351, Train Acc: 0.9931 | Val Loss: 2.0957, Val Acc: 0.7500\n",
      "개선 없음 - Early Stopping 카운터: 2/5\n",
      "Epoch: 13\tTrain Loss: 0.0566, Train Acc: 0.9917 | Val Loss: 3.0869, Val Acc: 0.6250\n",
      "Epoch: 14\tTrain Loss: 0.0574, Train Acc: 0.9931 | Val Loss: 2.0382, Val Acc: 0.7375\n",
      "개선 없음 - Early Stopping 카운터: 1/5\n",
      "Epoch: 15\tTrain Loss: 0.0722, Train Acc: 0.9875 | Val Loss: 2.7421, Val Acc: 0.6750\n",
      "개선 없음 - Early Stopping 카운터: 2/5\n",
      "Epoch: 16\tTrain Loss: 0.0018, Train Acc: 0.9986 | Val Loss: 3.2154, Val Acc: 0.6250\n",
      "Epoch: 17\tTrain Loss: 0.0527, Train Acc: 0.9931 | Val Loss: 2.4725, Val Acc: 0.7125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 손실 및 정확도 계산\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m     62\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m correct_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "epochs = 100\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 스케쥴러 설정\n",
    "# 워밍업 단계(학습률을 선형적으로 증가) / 학습 단계(학습률을 선형적으로 감소)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 손실 및 정확도 저장용\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "patience = 5\n",
    "patience_cnt = 0\n",
    "last_loss_val = float('inf')\n",
    "best_loss_val = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Train ###\n",
    "    model.train()\n",
    "    \n",
    "    loss_train = 0.0\n",
    "    correct_train = 0  # 훈련 데이터 정확도 계산\n",
    "    total_train = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # GPU 보내기\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device, dtype=torch.long)\n",
    "\n",
    "        # 학습과정\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)  # 기울기 폭발 방지\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 손실 및 정확도 계산\n",
    "        loss_train += loss.item() * batch_size\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    # 평균 손실 및 정확도 저장\n",
    "    train_loss = loss_train / len(train_dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_accuracy)\n",
    "    \n",
    "    ### Validation ###\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val = 0.0\n",
    "    correct_val = 0  # 검증 데이터 정확도 계산\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # GPU 보내기\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device, dtype=torch.long)\n",
    "\n",
    "            # 예측\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # 손실 및 정확도 계산\n",
    "            loss_val += loss.item() * batch_size\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    # 평균 손실 및 정확도 저장\n",
    "    val_loss = loss_val / len(val_dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_accuracy)\n",
    "\n",
    "    ### Early Stopping ###\n",
    "    if val_loss < last_loss_val:\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        print(f\"개선 없음 - Early Stopping 카운터: {patience_cnt}/{patience}\")\n",
    "        if patience_cnt == patience:\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "    last_loss_val = val_loss\n",
    "    \n",
    "    # Best 모델 저장\n",
    "    if val_loss < best_loss_val:\n",
    "        best_loss_val = val_loss\n",
    "        model_name = f\"bert_{epoch}\"\n",
    "        torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "        print(f\"Best model saved: {model_name}\")\n",
    "        \n",
    "    # 출력\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\\t\"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {100 * train_accuracy:.2f}% | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {100 * val_accuracy:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6225, Test Acc: 78.00%\n"
     ]
    }
   ],
   "source": [
    "# 예측 -- 이 모델의 성능을 평가하기 위한 코드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "loss_test = 0.0\n",
    "corr_test = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # GPU 보내기\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # 예측\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss_test += loss.item() * batch_size\n",
    "        \n",
    "        pred = outputs.logits.argmax(dim=1)\n",
    "        corr_test += (pred == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {loss_test / len(test_dataset):.4f}, Test Acc: {100 * corr_test / len(test_dataset):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 가져오기\n",
    "\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"bert_3.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# text = \"이번엔 정말 맛있었어요!!\"\n",
    "# text = \"배달이 너무 느려요. 다시는 안시킬것 같아요.\"\n",
    "# text = \"참 대단한 집입니다. 떡볶이에서 고무장화 맛이 나요 :)\"\n",
    "# text = \"맛있으면 짖는 개 왈왈왈왈 와라와라왈왈 으르르르르르르르르왈왈\"\n",
    "text = \"맛없으면 짖는 개 왈왈왈왈 와라와라왈왈 으르르르르르르르르왈왈\"\n",
    "max_length = 128\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    text, \n",
    "    max_length=max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_input = encoded_input.to(device)\n",
    "output = model(**encoded_input)\n",
    "print(output.logits.argmax(dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
