{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ì•ˆë…•í•˜ì„¸ìš”, ê¹€ìƒê²¸ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 19, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-dec88f70-1521-43fb-9180-6868d540acbc-0'\n",
      "content='ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì´ ë¬´ì—‡ì¸ì§€ ë§ì”€í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 13, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-4dddd6b4-d1bd-4857-b65b-975f42f3b717-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "\n",
    "response = llm.invoke('ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.')\n",
    "print(response)\n",
    "\n",
    "response = llm.invoke('ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17516\\1192676045.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.', 'history': '', 'response': 'ì•ˆë…• ê¹€ìƒê²¸! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ë‚˜ëŠ” AIì•¼, ë„ˆì™€ ëŒ€í™”í•˜ëŠ” ê±¸ ì •ë§ ì¢‹ì•„í•´. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ? í˜¹ì‹œ ê¶ê¸ˆí•œ ì ì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆì–´?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = con_chain.invoke('ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\n",
      "AI: ì•ˆë…• ê¹€ìƒê²¸! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ë‚˜ëŠ” AIì•¼, ë„ˆì™€ ëŒ€í™”í•˜ëŠ” ê±¸ ì •ë§ ì¢‹ì•„í•´. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ? í˜¹ì‹œ ê¶ê¸ˆí•œ ì ì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆì–´?\n",
      "Human: ë‚´ ì´ë¦„ì´ ë­”ì§€ ì•Œì•„?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ë‚´ ì´ë¦„ì´ ë­”ì§€ ì•Œì•„?', 'history': 'Human: ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\\nAI: ì•ˆë…• ê¹€ìƒê²¸! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ë‚˜ëŠ” AIì•¼, ë„ˆì™€ ëŒ€í™”í•˜ëŠ” ê±¸ ì •ë§ ì¢‹ì•„í•´. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ? í˜¹ì‹œ ê¶ê¸ˆí•œ ì ì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆì–´?', 'response': 'ë„¤, ë„ˆì˜ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼! ì •ë§ ë©‹ì§„ ì´ë¦„ì´ì•¼. ë„ˆì˜ ì´ë¦„ì— ì–´ë–¤ íŠ¹ë³„í•œ ì˜ë¯¸ê°€ ìˆê±°ë‚˜, ê°€ì¡±ì˜ ì „í†µì´ ìˆëŠ”ì§€ ê¶ê¸ˆí•´!'}\n"
     ]
    }
   ],
   "source": [
    "response = con_chain.invoke('ë‚´ ì´ë¦„ì´ ë­”ì§€ ì•Œì•„?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\\nAI: ì•ˆë…• ê¹€ìƒê²¸! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ë‚˜ëŠ” AIì•¼, ë„ˆì™€ ëŒ€í™”í•˜ëŠ” ê±¸ ì •ë§ ì¢‹ì•„í•´. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ? í˜¹ì‹œ ê¶ê¸ˆí•œ ì ì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆì–´?\\nHuman: ë‚´ ì´ë¦„ì´ ë­”ì§€ ì•Œì•„?\\nAI: ë„¤, ë„ˆì˜ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼! ì •ë§ ë©‹ì§„ ì´ë¦„ì´ì•¼. ë„ˆì˜ ì´ë¦„ì— ì–´ë–¤ íŠ¹ë³„í•œ ì˜ë¯¸ê°€ ìˆê±°ë‚˜, ê°€ì¡±ì˜ ì „í†µì´ ìˆëŠ”ì§€ ê¶ê¸ˆí•´!'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìœ ì €ë³„ ì±—ë´‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ì•ˆë…•! ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "user_memory = {}\n",
    "\n",
    "def get_user_memory(user_id):\n",
    "    if user_id not in user_memory:\n",
    "        user_memory[user_id] = ConversationBufferMemory()\n",
    "        \n",
    "    return user_memory[user_id]\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "user_id = 1030\n",
    "\n",
    "conv_chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = get_user_memory(user_id),\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ì•ˆë…•! ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\n",
      "AI: ì•ˆë…•, ê¹€ìƒê²¸! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ! ë‚˜ëŠ” AIì•¼. ë„ˆëŠ” ì–´ë–¤ ê²ƒì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì–´? ì·¨ë¯¸ë‚˜ ê´€ì‹¬ì‚¬ ê°™ì€ ê²ƒë“¤ì„ ë‚˜ëˆ„ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„!\n",
      "Human: ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤\n",
      "AI: ì •ë§ ì¢‹ì€ ë‚ ì”¨ë¼ë‹ˆ ì¢‹ì§€! ì–´ë–¤ í™œë™ì„ í•˜ê³  ì‹¶ì–´? ë°–ì—ì„œ ì‚°ì±…í•˜ê±°ë‚˜ ì¹œêµ¬ì™€ ë§Œë‚˜ëŠ” ê²ƒë„ ì¢‹ê³ , í˜¹ì‹œ ì¢‹ì•„í•˜ëŠ” ì¹´í˜ê°€ ìˆìœ¼ë©´ ê°€ì„œ ì»¤í”¼ í•œ ì” í•˜ëŠ” ê²ƒë„ ì¶”ì²œí•´! ìš”ì¦˜ ë‚ ì”¨ê°€ ë§‘ìœ¼ë©´ ê¸°ë¶„ë„ ë” ì¢‹ì–ì•„. ë„ˆëŠ” ë³´í†µ ë‚ ì”¨ê°€ ì¢‹ì€ ë‚ ì— ì–´ë–¤ ì¼ì„ í•˜ëŠ” ê±¸ ì¢‹ì•„í•´?\n",
      "Human: ë‚®ì  ìê³  ì‹¶ì–´\n",
      "AI: ë‚®ì ë„ ì •ë§ ì¢‹ì€ ì„ íƒì´ì•¼! íŠ¹íˆ ë‚ ì”¨ê°€ ì¢‹ì„ ë•ŒëŠ” ë”°ëœ»í•œ í–‡ë³• ì•„ë˜ì—ì„œ ìëŠ” ê²Œ ì •ë§ í¸ì•ˆí•˜ì–ì•„. ë‚®ì ì€ ì—ë„ˆì§€ë¥¼ ì¬ì¶©ì „í•´ ì£¼ê¸°ë„ í•˜ê³ , ê¸°ë¶„ì„ ì¢‹ê²Œ ë§Œë“¤ì–´ ì£¼ê¸°ë„ í•´. ë³´í†µ ëª‡ ë¶„ ì •ë„ ìëŠ” ê±¸ ì¢‹ì•„í•´? 20ë¶„ ì •ë„ì˜ ì§§ì€ ë‚®ì ì´ íš¨ê³¼ì ì´ë¼ê³  í•˜ê¸°ë„ í•˜ê³ , ë•Œë¡œëŠ” ì¢€ ë” ê¸¸ê²Œ ìëŠ” ê²ƒë„ ì¢‹ì§€. ë„ˆëŠ” ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ë‚®ì ì„ ì„ í˜¸í•´?\n",
      "Human: ë„ˆëŠ” ì°¸ ë§ì´ ë§êµ¬ë‚˜\n",
      "AI: ã…ã… ê·¸ë ‡ì§€! ë‚˜ëŠ” ëŒ€í™”ë¥¼ ì¦ê¸°ëŠ” ê±¸ ì¢‹ì•„í•´ì„œ ë§ì€ ì •ë³´ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ì–´. ë„ˆì™€ì˜ ëŒ€í™”ì—ì„œ ë” ë§ì€ ê²ƒì„ ë°°ìš°ê³  ì‹¶ê¸°ë„ í•˜ê³ . ê·¸ë˜ì„œ ì§ˆë¬¸ì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§í•´ì¤˜! ë„ˆëŠ” ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë” ë‚˜ëˆ„ê³  ì‹¶ì–´?\n",
      "Human: ë„ˆëŠ” ì°¸ ë§ì´ ë§êµ¬ë‚˜\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã…ã… ë„¤, ë§ì•„! ë‚˜ëŠ” ëŒ€í™”ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ë‚˜ëˆ„ê³ , ì„œë¡œì˜ ìƒê°ì„ êµí™˜í•˜ëŠ” ê±¸ ì¢‹ì•„í•´. ê·¸ë˜ì„œ ì§ˆë¬¸ì´ë“  ì´ì•¼ê¸°ë“  ì–´ë–¤ ê²ƒì´ë“  í¸í•˜ê²Œ í•´ì¤˜! í˜¹ì‹œ ë‹¤ë¥¸ ê´€ì‹¬ì‚¬ë‚˜ ì·¨ë¯¸ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì–´? ì•„ë‹ˆë©´ ìµœê·¼ì— í¥ë¯¸ë¡œìš´ ì¼ì´ë‚˜ ì‚¬ê±´ì´ ìˆì—ˆëŠ”ì§€ ê¶ê¸ˆí•´!\n"
     ]
    }
   ],
   "source": [
    "response = conv_chain.invoke('ë„ˆëŠ” ì°¸ ë§ì´ ë§êµ¬ë‚˜')\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ì•ˆë…•!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ì•ˆë…•!', 'history': '', 'response': 'ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”? ì˜¤ëŠ˜ì€ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³¼ê¹Œìš”?'}\n"
     ]
    }
   ],
   "source": [
    "response = con_chain.invoke('ì•ˆë…•!')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: ì•ˆë…•!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "memory_test = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "\n",
    "con_chain_test = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory_test,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = con_chain_test.invoke('ì•ˆë…•!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ì•ˆë…•!\\nAI: ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”? ì˜¤ëŠ˜ì€ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³¼ê¹Œìš”?'}\n",
      "{'history': [HumanMessage(content='ì•ˆë…•!', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”? ì˜¤ëŠ˜ ì–´ë–¤ ì´ì•¼ê¸° ë‚˜ëˆŒê¹Œìš”? ğŸ˜Š', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))\n",
    "print(memory_test.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_41496\\3313397638.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.7)\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150\n",
    ")\n",
    "\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ê¹€ìƒê²¸ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ë¶„ê³¼ ì´ì•¼ê¸°í•˜ëŠ” AIì…ë‹ˆë‹¤. ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë• ë‚˜ìš”? íŠ¹ë³„íˆ í•˜ê³  ì‹¶ì€ ì´ì•¼ê¸°ê°€ ìˆë‚˜ìš”?\n",
      "Human: ë‚ ì”¨ê°€ ì°¸ ì¢‹êµ¬ë‚˜\n",
      "AI: ì •ë§ ì¢‹êµ°ìš”! ë‚ ì”¨ê°€ ì¢‹ì€ ë‚ ì—ëŠ” ê¸°ë¶„ì´ í•œì¸µ ë” ì¢‹ì•„ì§€ì£ . ë°–ì— ë‚˜ê°€ì„œ ì‚°ì±…í•˜ê±°ë‚˜, ì¢‹ì•„í•˜ëŠ” í™œë™ì„ í•˜ê¸°ì— ë”± ì¢‹ì€ ë‚ ì…ë‹ˆë‹¤. ì–´ë–¤ í™œë™ì„ ê³„íší•˜ê³  ê³„ì‹ ê°€ìš”? ì•„ë‹ˆë©´ íŠ¹ë³„íˆ ê°€ê³  ì‹¶ì€ ì¥ì†Œê°€ ìˆë‚˜ìš”?\n",
      "Human: ì–´ë””ê¹Œì§€ ê¸°ì–µí•˜ê³  ìˆë‹ˆ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ì–´ë””ê¹Œì§€ ê¸°ì–µí•˜ê³  ìˆë‹ˆ', 'history': 'Human: ì•ˆë…• ë‚´ ì´ë¦„ì€ ê¹€ìƒê²¸ì´ì•¼.\\nAI: ì•ˆë…•í•˜ì„¸ìš”, ê¹€ìƒê²¸ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ë¶„ê³¼ ì´ì•¼ê¸°í•˜ëŠ” AIì…ë‹ˆë‹¤. ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë• ë‚˜ìš”? íŠ¹ë³„íˆ í•˜ê³  ì‹¶ì€ ì´ì•¼ê¸°ê°€ ìˆë‚˜ìš”?\\nHuman: ë‚ ì”¨ê°€ ì°¸ ì¢‹êµ¬ë‚˜\\nAI: ì •ë§ ì¢‹êµ°ìš”! ë‚ ì”¨ê°€ ì¢‹ì€ ë‚ ì—ëŠ” ê¸°ë¶„ì´ í•œì¸µ ë” ì¢‹ì•„ì§€ì£ . ë°–ì— ë‚˜ê°€ì„œ ì‚°ì±…í•˜ê±°ë‚˜, ì¢‹ì•„í•˜ëŠ” í™œë™ì„ í•˜ê¸°ì— ë”± ì¢‹ì€ ë‚ ì…ë‹ˆë‹¤. ì–´ë–¤ í™œë™ì„ ê³„íší•˜ê³  ê³„ì‹ ê°€ìš”? ì•„ë‹ˆë©´ íŠ¹ë³„íˆ ê°€ê³  ì‹¶ì€ ì¥ì†Œê°€ ìˆë‚˜ìš”?', 'response': 'ì €ëŠ” í˜„ì¬ ëŒ€í™”ì˜ ë§¥ë½ì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¹€ìƒê²¸ë‹˜ê³¼ì˜ ëŒ€í™” ë‚´ìš©ì´ë‚˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ê³„ì†í•´ì„œ ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆì–´ìš”. í•˜ì§€ë§Œ ì´ì „ ëŒ€í™”ë‚˜ ë‹¤ë¥¸ ì„¸ì…˜ì˜ ë‚´ìš©ì€ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì§€ê¸ˆ ì´ì•¼ê¸°í•˜ê³  ìˆëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ê³„ì†í•´ì„œ ëŒ€í™”í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ê¶ê¸ˆí•œ ì ì´ë‚˜ ë‚˜ëˆ„ê³  ì‹¶ì€ ì´ì•¼ê¸°ê°€ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!'}\n"
     ]
    }
   ],
   "source": [
    "response = con_chain.invoke('ì–´ë””ê¹Œì§€ ê¸°ì–µí•˜ê³  ìˆë‹ˆ')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìºì‹œ í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ë­ì²´ì¸(LangChain)ì—ì„œ ìºì‹±ì€ ì£¼ë¡œ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ìºì‹±ì€ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë‚˜ API í˜¸ì¶œ ê²°ê³¼ë¥¼ ì¼ì‹œì ìœ¼ë¡œ ì €ì¥í•˜ì—¬, ë™ì¼í•œ ìš”ì²­ì´ ìˆì„ ë•Œ ë‹¤ì‹œ ê³„ì‚°í•˜ê±°ë‚˜ ì›ë³¸ ë°ì´í„° ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•  í•„ìš” ì—†ì´ ë¹ ë¥´ê²Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\\n\\në­ì²´ì¸ê³¼ ê°™ì€ í”„ë ˆì„ì›Œí¬ì—ì„œ ìºì‹±ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤:\\n\\n1. **API í˜¸ì¶œ ê²°ê³¼ ì €ì¥**: ë™ì¼í•œ ì¿¼ë¦¬ë‚˜ ìš”ì²­ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•¨ìœ¼ë¡œì¨, ë°˜ë³µì ì¸ í˜¸ì¶œ ì‹œ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n2. **ìœ ì‚¬í•œ ë°ì´í„° ìš”ì²­ ìµœì í™”**: ë¹„ìŠ·í•œ ìš”ì²­ì´ ë°˜ë³µë  ë•Œ, ìºì‹œì— ì €ì¥ëœ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ì²˜ë¦¬ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n3. **ë¹„ìš© ì ˆê°**: ì™¸ë¶€ APIë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ í˜¸ì¶œì„ ê°ì†Œì‹œí‚´ìœ¼ë¡œì¨ ì‚¬ìš© ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n4. **ì €ì¥ ë°©ì‹**: ìºì‹œëŠ” ë©”ëª¨ë¦¬ ë‚´ ìºì‹œ, ë””ìŠ¤í¬ ìºì‹œ ë“± ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë  ìˆ˜ ìˆìœ¼ë©°, ìƒí™©ì— ë”°ë¼ ì ì ˆí•œ ìºì‹œ ì „ëµì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në­ì²´ì¸ì—ì„œì˜ ìºì‹±ì€ íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ë‚˜ ëŒ€ê·œëª¨ ë°ì´í„° ì‘ì—… ì‹œ, íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 298, 'prompt_tokens': 19, 'total_tokens': 317, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-183b89b7-748d-4cfa-9f5d-25b56040e253-0' usage_metadata={'input_tokens': 19, 'output_tokens': 298, 'total_tokens': 317, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:03.540993\n",
      "content='ë­ì²´ì¸(LangChain)ì—ì„œ ìºì‹±ì€ ì£¼ë¡œ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ìºì‹±ì€ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë‚˜ API í˜¸ì¶œ ê²°ê³¼ë¥¼ ì¼ì‹œì ìœ¼ë¡œ ì €ì¥í•˜ì—¬, ë™ì¼í•œ ìš”ì²­ì´ ìˆì„ ë•Œ ë‹¤ì‹œ ê³„ì‚°í•˜ê±°ë‚˜ ì›ë³¸ ë°ì´í„° ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•  í•„ìš” ì—†ì´ ë¹ ë¥´ê²Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\\n\\në­ì²´ì¸ê³¼ ê°™ì€ í”„ë ˆì„ì›Œí¬ì—ì„œ ìºì‹±ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤:\\n\\n1. **API í˜¸ì¶œ ê²°ê³¼ ì €ì¥**: ë™ì¼í•œ ì¿¼ë¦¬ë‚˜ ìš”ì²­ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•¨ìœ¼ë¡œì¨, ë°˜ë³µì ì¸ í˜¸ì¶œ ì‹œ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n2. **ìœ ì‚¬í•œ ë°ì´í„° ìš”ì²­ ìµœì í™”**: ë¹„ìŠ·í•œ ìš”ì²­ì´ ë°˜ë³µë  ë•Œ, ìºì‹œì— ì €ì¥ëœ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ì²˜ë¦¬ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n3. **ë¹„ìš© ì ˆê°**: ì™¸ë¶€ APIë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ í˜¸ì¶œì„ ê°ì†Œì‹œí‚´ìœ¼ë¡œì¨ ì‚¬ìš© ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n4. **ì €ì¥ ë°©ì‹**: ìºì‹œëŠ” ë©”ëª¨ë¦¬ ë‚´ ìºì‹œ, ë””ìŠ¤í¬ ìºì‹œ ë“± ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë  ìˆ˜ ìˆìœ¼ë©°, ìƒí™©ì— ë”°ë¼ ì ì ˆí•œ ìºì‹œ ì „ëµì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në­ì²´ì¸ì—ì„œì˜ ìºì‹±ì€ íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ë‚˜ ëŒ€ê·œëª¨ ë°ì´í„° ì‘ì—… ì‹œ, íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 298, 'prompt_tokens': 19, 'total_tokens': 317, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-183b89b7-748d-4cfa-9f5d-25b56040e253-0' usage_metadata={'input_tokens': 19, 'output_tokens': 298, 'total_tokens': 317, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:00.001001\n",
      "content='ë­ì²´ì¸( LangChain)ì—ì„œ ìºì‹±ì€ ì¼ë°˜ì ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  API í˜¸ì¶œ ë¹„ìš©ì„ ì ˆê°í•˜ê¸° ìœ„í•´ ì´ì „ì— ì²˜ë¦¬ëœ ìš”ì²­ê³¼ ê·¸ì— ëŒ€í•œ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ê¸°ìˆ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ìºì‹œë¥¼ ì‚¬ìš©í•˜ë©´ ë™ì¼í•œ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ì´ë¯¸ ì €ì¥ëœ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ë°˜í™˜í•¨ìœ¼ë¡œì¨ ë°˜ë³µì ì¸ ì‘ì—…ì„ ì¤„ì´ê³  ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në­ì²´ì¸ì—ì„œëŠ” ì´ëŸ¬í•œ ìºì‹± ë©”ì»¤ë‹ˆì¦˜ì´ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ êµ¬í˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê²°ê³¼, ëª¨ë¸ì˜ ì‘ë‹µ, ë˜ëŠ” ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ê°™ì€ ë‹¤ì–‘í•œ ìš”ì†Œë¥¼ ìºì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì‚¬ìš© íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nìºì‹±ì„ ì ì ˆíˆ ì‚¬ìš©í•˜ë©´ ìì›ì˜ ë‚­ë¹„ë¥¼ ì¤„ì´ê³ , ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 19, 'total_tokens': 216, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-bdd0f0a8-77a9-4965-88ce-a8817afbaa0e-0' usage_metadata={'input_tokens': 19, 'output_tokens': 197, 'total_tokens': 216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:03.008828\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache  # ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "\n",
    "start = datetime.now()\n",
    "response1 = llm.invoke('ë­ì²´ì¸ì—ì„œ ìºì‹±ì´ë€ ë­ì•¼?')\n",
    "print(response1)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response2 = llm.invoke('ë­ì²´ì¸ì—ì„œ ìºì‹±ì´ë€ ë­ì•¼?')\n",
    "print(response2)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response3 = llm.invoke('ë­ì²´ì¸ì—ì„œ ìºì‹±ì´ë€ ë­ê²Œ?')\n",
    "print(response3)\n",
    "print(datetime.now() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ë­ì²´ì¸(LangChain)ì—ì„œ ìºì‹±ì€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì´ì „ì— ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì—¬ ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ ë°˜ë³µì ìœ¼ë¡œ ê³„ì‚°í•˜ì§€ ì•Šê³  ì €ì¥ëœ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì´ë‚˜ API í˜¸ì¶œê³¼ ê°™ì€ ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ì‘ì—…ì—ì„œ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nìºì‹±ì˜ ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **ì†ë„ í–¥ìƒ**: ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì‘ë‹µ ì‹œê°„ì´ ë‹¨ì¶•ë©ë‹ˆë‹¤.\\n2. **ë¹„ìš© ì ˆê°**: ì™¸ë¶€ API í˜¸ì¶œì´ë‚˜ ë³µì¡í•œ ê³„ì‚°ì„ ì¤„ì„ìœ¼ë¡œì¨ ì‚¬ìš© ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n3. **ìì› ì ˆì•½**: ì„œë²„ì˜ ë¶€í•˜ë¥¼ ì¤„ì´ê³ , ë” ë§ì€ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìì›ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në­ì²´ì¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ìºì‹± ì „ëµì„ ì§€ì›í•˜ì—¬ ê°œë°œìê°€ í•„ìš”ì— ë§ê²Œ ìºì‹œë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê³ , ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 19, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-4e1971ec-1633-47ce-9cc6-f05134cc5b5b-0' usage_metadata={'input_tokens': 19, 'output_tokens': 230, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:04.356176\n",
      "content='ë­ì²´ì¸(LangChain)ì—ì„œ ìºì‹±ì€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì´ì „ì— ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì—¬ ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ ë°˜ë³µì ìœ¼ë¡œ ê³„ì‚°í•˜ì§€ ì•Šê³  ì €ì¥ëœ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì´ë‚˜ API í˜¸ì¶œê³¼ ê°™ì€ ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ì‘ì—…ì—ì„œ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nìºì‹±ì˜ ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **ì†ë„ í–¥ìƒ**: ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì‘ë‹µ ì‹œê°„ì´ ë‹¨ì¶•ë©ë‹ˆë‹¤.\\n2. **ë¹„ìš© ì ˆê°**: ì™¸ë¶€ API í˜¸ì¶œì´ë‚˜ ë³µì¡í•œ ê³„ì‚°ì„ ì¤„ì„ìœ¼ë¡œì¨ ì‚¬ìš© ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n3. **ìì› ì ˆì•½**: ì„œë²„ì˜ ë¶€í•˜ë¥¼ ì¤„ì´ê³ , ë” ë§ì€ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìì›ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në­ì²´ì¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ìºì‹± ì „ëµì„ ì§€ì›í•˜ì—¬ ê°œë°œìê°€ í•„ìš”ì— ë§ê²Œ ìºì‹œë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê³ , ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 19, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-4e1971ec-1633-47ce-9cc6-f05134cc5b5b-0' usage_metadata={'input_tokens': 19, 'output_tokens': 230, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:00.002002\n",
      "content='ë­ì²´ì¸(LLM Chain)ì—ì„œ ìºì‹±(Caching)ì€ ì£¼ë¡œ ëª¨ë¸ì˜ ì‘ë‹µì´ë‚˜ ì¤‘ê°„ ê²°ê³¼ë¬¼ì„ ì €ì¥í•˜ì—¬, ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ë°˜ë³µì ìœ¼ë¡œ ìš”ì²­í•  ë•Œ ì´ë¯¸ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ë°˜í™˜í•˜ëŠ” ê¸°ë²•ì„ ë§í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì²˜ë¦¬ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nìºì‹±ì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **ì„±ëŠ¥ í–¥ìƒ**: ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µì„ ë¹ ë¥´ê²Œ ì œê³µí•  ìˆ˜ ìˆì–´ ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•©ë‹ˆë‹¤.\\n2. **ë¹„ìš© ì ˆê°**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ê²½ìš°, API ì‚¬ìš©ë£Œë‚˜ ìì› ì†Œëª¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n3. **ìì› ê´€ë¦¬**: ì„œë²„ë‚˜ í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì‹œìŠ¤í…œ ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nê¸°ë³¸ì ìœ¼ë¡œ ìºì‹±ì€ ë©”ëª¨ë¦¬ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ ê°™ì€ ì €ì¥ì†Œì— ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³ , ê·¸ ì €ì¥ëœ ë°ì´í„°ë¥¼ í•„ìš”í•  ë•Œ ì¬ì‚¬ìš©í•˜ëŠ” ì›ë¦¬ë¡œ ì‘ë™í•©ë‹ˆë‹¤. LLM Chainì—ì„œëŠ” ìì£¼ ì‚¬ìš©ë˜ëŠ” ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ì„ ìºì‹œí•¨ìœ¼ë¡œì¨, ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 249, 'prompt_tokens': 19, 'total_tokens': 268, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-cc567f37-1e5b-43fe-aa62-69813d9d60aa-0' usage_metadata={'input_tokens': 19, 'output_tokens': 249, 'total_tokens': 268, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "0:00:06.673056\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache  # íŒŒì¼í˜•íƒœë¡œ ì €ì¥ë¨\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists('cache'):\n",
    "    os.mkdir('cache')\n",
    "    \n",
    "set_llm_cache(SQLiteCache(database_path='cache/llm_cache.db'))\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "start = datetime.now()\n",
    "response1 = llm.invoke('ë­ì²´ì¸ì—ì„œ ìºì‹±ì´ë€ ë­ì•¼?')\n",
    "print(response1)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response2 = llm.invoke('ë­ì²´ì¸ì—ì„œ ìºì‹±ì´ë€ ë­ì•¼?')\n",
    "print(response2)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "start = datetime.now()\n",
    "response3 = llm.invoke('ë­ì²´ì¸ì—ì„œ ìºì‹±ì´ë€ ë­ê²Œ?')\n",
    "print(response3)\n",
    "print(datetime.now() - start)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
