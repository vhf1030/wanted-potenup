2025-01-09 10:17:35,340:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-01-09 10:17:35,341:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-01-09 10:17:35,341:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-01-09 10:17:35,341:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-01-09 10:20:16,355:INFO:PyCaret ClassificationExperiment
2025-01-09 10:20:16,355:INFO:Logging name: clf-default-name
2025-01-09 10:20:16,364:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-01-09 10:20:16,364:INFO:version 3.3.2
2025-01-09 10:20:16,364:INFO:Initializing setup()
2025-01-09 10:20:16,364:INFO:self.USI: 2705
2025-01-09 10:20:16,364:INFO:self._variable_keys: {'html_param', 'target_param', 'logging_param', 'seed', 'fold_shuffle_param', 'fix_imbalance', 'X', 'y', 'exp_name_log', '_available_plots', 'is_multiclass', 'idx', 'gpu_n_jobs_param', 'USI', 'memory', 'y_test', 'X_train', 'y_train', 'data', 'X_test', 'n_jobs_param', 'gpu_param', '_ml_usecase', 'exp_id', 'fold_generator', 'fold_groups_param', 'log_plots_param', 'pipeline'}
2025-01-09 10:20:16,364:INFO:Checking environment
2025-01-09 10:20:16,364:INFO:python_version: 3.11.9
2025-01-09 10:20:16,364:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2025-01-09 10:20:16,364:INFO:machine: AMD64
2025-01-09 10:20:16,364:INFO:platform: Windows-10-10.0.22631-SP0
2025-01-09 10:20:16,369:INFO:Memory: svmem(total=33837973504, available=16317874176, percent=51.8, used=17520099328, free=16317874176)
2025-01-09 10:20:16,370:INFO:Physical Core: 16
2025-01-09 10:20:16,370:INFO:Logical Core: 22
2025-01-09 10:20:16,370:INFO:Checking libraries
2025-01-09 10:20:16,370:INFO:System:
2025-01-09 10:20:16,370:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2025-01-09 10:20:16,370:INFO:executable: c:\wanted\envs\ml_practice\Scripts\python.exe
2025-01-09 10:20:16,370:INFO:   machine: Windows-10-10.0.22631-SP0
2025-01-09 10:20:16,370:INFO:PyCaret required dependencies:
2025-01-09 10:20:16,390:INFO:                 pip: 24.3.1
2025-01-09 10:20:16,390:INFO:          setuptools: 65.5.0
2025-01-09 10:20:16,390:INFO:             pycaret: 3.3.2
2025-01-09 10:20:16,390:INFO:             IPython: 8.31.0
2025-01-09 10:20:16,390:INFO:          ipywidgets: 8.1.5
2025-01-09 10:20:16,390:INFO:                tqdm: 4.67.1
2025-01-09 10:20:16,390:INFO:               numpy: 1.26.4
2025-01-09 10:20:16,390:INFO:              pandas: 2.1.4
2025-01-09 10:20:16,390:INFO:              jinja2: 3.1.5
2025-01-09 10:20:16,390:INFO:               scipy: 1.11.4
2025-01-09 10:20:16,390:INFO:              joblib: 1.3.2
2025-01-09 10:20:16,390:INFO:             sklearn: 1.4.2
2025-01-09 10:20:16,390:INFO:                pyod: 2.0.3
2025-01-09 10:20:16,390:INFO:            imblearn: 0.13.0
2025-01-09 10:20:16,390:INFO:   category_encoders: 2.7.0
2025-01-09 10:20:16,390:INFO:            lightgbm: 4.5.0
2025-01-09 10:20:16,390:INFO:               numba: 0.60.0
2025-01-09 10:20:16,390:INFO:            requests: 2.32.3
2025-01-09 10:20:16,390:INFO:          matplotlib: 3.7.5
2025-01-09 10:20:16,390:INFO:          scikitplot: 0.3.7
2025-01-09 10:20:16,390:INFO:         yellowbrick: 1.5
2025-01-09 10:20:16,391:INFO:              plotly: 5.24.1
2025-01-09 10:20:16,391:INFO:    plotly-resampler: Not installed
2025-01-09 10:20:16,391:INFO:             kaleido: 0.2.1
2025-01-09 10:20:16,391:INFO:           schemdraw: 0.15
2025-01-09 10:20:16,391:INFO:         statsmodels: 0.14.4
2025-01-09 10:20:16,391:INFO:              sktime: 0.26.0
2025-01-09 10:20:16,391:INFO:               tbats: 1.1.3
2025-01-09 10:20:16,391:INFO:            pmdarima: 2.0.4
2025-01-09 10:20:16,391:INFO:              psutil: 6.1.1
2025-01-09 10:20:16,391:INFO:          markupsafe: 3.0.2
2025-01-09 10:20:16,391:INFO:             pickle5: Not installed
2025-01-09 10:20:16,391:INFO:         cloudpickle: 3.1.0
2025-01-09 10:20:16,391:INFO:         deprecation: 2.1.0
2025-01-09 10:20:16,391:INFO:              xxhash: 3.5.0
2025-01-09 10:20:16,391:INFO:           wurlitzer: Not installed
2025-01-09 10:20:16,391:INFO:PyCaret optional dependencies:
2025-01-09 10:20:16,414:INFO:                shap: Not installed
2025-01-09 10:20:16,414:INFO:           interpret: Not installed
2025-01-09 10:20:16,414:INFO:                umap: Not installed
2025-01-09 10:20:16,414:INFO:     ydata_profiling: Not installed
2025-01-09 10:20:16,414:INFO:  explainerdashboard: Not installed
2025-01-09 10:20:16,414:INFO:             autoviz: Not installed
2025-01-09 10:20:16,414:INFO:           fairlearn: Not installed
2025-01-09 10:20:16,414:INFO:          deepchecks: Not installed
2025-01-09 10:20:16,414:INFO:             xgboost: 2.1.3
2025-01-09 10:20:16,414:INFO:            catboost: Not installed
2025-01-09 10:20:16,414:INFO:              kmodes: Not installed
2025-01-09 10:20:16,414:INFO:             mlxtend: Not installed
2025-01-09 10:20:16,414:INFO:       statsforecast: Not installed
2025-01-09 10:20:16,414:INFO:        tune_sklearn: Not installed
2025-01-09 10:20:16,414:INFO:                 ray: Not installed
2025-01-09 10:20:16,414:INFO:            hyperopt: Not installed
2025-01-09 10:20:16,414:INFO:              optuna: Not installed
2025-01-09 10:20:16,414:INFO:               skopt: Not installed
2025-01-09 10:20:16,414:INFO:              mlflow: Not installed
2025-01-09 10:20:16,414:INFO:              gradio: Not installed
2025-01-09 10:20:16,414:INFO:             fastapi: Not installed
2025-01-09 10:20:16,414:INFO:             uvicorn: Not installed
2025-01-09 10:20:16,414:INFO:              m2cgen: Not installed
2025-01-09 10:20:16,414:INFO:           evidently: Not installed
2025-01-09 10:20:16,414:INFO:               fugue: Not installed
2025-01-09 10:20:16,414:INFO:           streamlit: Not installed
2025-01-09 10:20:16,414:INFO:             prophet: Not installed
2025-01-09 10:20:16,415:INFO:None
2025-01-09 10:20:16,415:INFO:Set up data.
2025-01-09 10:20:16,420:INFO:Set up folding strategy.
2025-01-09 10:20:16,420:INFO:Set up train/test split.
2025-01-09 10:20:16,593:INFO:Set up index.
2025-01-09 10:20:16,593:INFO:Assigning column types.
2025-01-09 10:20:16,596:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-01-09 10:20:16,621:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-01-09 10:20:16,624:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:20:16,642:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:16,643:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:16,668:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-01-09 10:20:16,669:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:20:16,687:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:16,689:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:16,689:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-01-09 10:20:16,713:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:20:16,728:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:16,730:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:16,756:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:20:16,770:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:16,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:16,772:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-01-09 10:20:16,810:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:16,812:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:16,852:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:16,855:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:16,857:INFO:Preparing preprocessing pipeline...
2025-01-09 10:20:16,858:INFO:Set up simple imputation.
2025-01-09 10:20:16,859:INFO:Set up encoding of ordinal features.
2025-01-09 10:20:16,860:INFO:Set up encoding of categorical features.
2025-01-09 10:20:16,898:INFO:Finished creating preprocessing pipeline.
2025-01-09 10:20:16,906:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Age', 'SibSp', 'Parch',
                                             'Fare'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Transf...
                                                               mapping=[{'col': 'Sex',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None, include=['Embarked'],
                                    transformer=OneHotEncoder(cols=['Embarked'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-01-09 10:20:16,906:INFO:Creating final display dataframe.
2025-01-09 10:20:16,999:INFO:Setup _display_container:                     Description             Value
0                    Session id              2061
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 12)
4        Transformed data shape         (891, 10)
5   Transformed train set shape         (623, 10)
6    Transformed test set shape         (268, 10)
7               Ignore features                 4
8              Numeric features                 5
9          Categorical features                 2
10     Rows with missing values             79.5%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Fold Generator   StratifiedKFold
18                  Fold Number                10
19                     CPU Jobs                -1
20                      Use GPU             False
21               Log Experiment             False
22              Experiment Name  clf-default-name
23                          USI              2705
2025-01-09 10:20:17,049:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:17,050:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:17,089:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:20:17,094:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:20:17,095:INFO:setup() successfully completed in 0.74s...............
2025-01-09 10:21:59,940:INFO:Initializing compare_models()
2025-01-09 10:21:59,941:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-01-09 10:21:59,941:INFO:Checking exceptions
2025-01-09 10:21:59,944:INFO:Preparing display monitor
2025-01-09 10:21:59,959:INFO:Initializing Logistic Regression
2025-01-09 10:21:59,960:INFO:Total runtime is 1.6518433888753255e-05 minutes
2025-01-09 10:21:59,962:INFO:SubProcess create_model() called ==================================
2025-01-09 10:21:59,963:INFO:Initializing create_model()
2025-01-09 10:21:59,963:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:21:59,963:INFO:Checking exceptions
2025-01-09 10:21:59,963:INFO:Importing libraries
2025-01-09 10:21:59,963:INFO:Copying training dataset
2025-01-09 10:21:59,966:INFO:Defining folds
2025-01-09 10:21:59,966:INFO:Declaring metric variables
2025-01-09 10:21:59,969:INFO:Importing untrained model
2025-01-09 10:21:59,972:INFO:Logistic Regression Imported successfully
2025-01-09 10:21:59,978:INFO:Starting cross validation
2025-01-09 10:21:59,980:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:02,657:INFO:Calculating mean and std
2025-01-09 10:22:02,659:INFO:Creating metrics dataframe
2025-01-09 10:22:02,662:INFO:Uploading results into container
2025-01-09 10:22:02,663:INFO:Uploading model into container now
2025-01-09 10:22:02,663:INFO:_master_model_container: 1
2025-01-09 10:22:02,663:INFO:_display_container: 2
2025-01-09 10:22:02,664:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2061, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-01-09 10:22:02,664:INFO:create_model() successfully completed......................................
2025-01-09 10:22:02,751:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:02,751:INFO:Creating metrics dataframe
2025-01-09 10:22:02,754:INFO:Initializing K Neighbors Classifier
2025-01-09 10:22:02,754:INFO:Total runtime is 0.046585214138031 minutes
2025-01-09 10:22:02,756:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:02,757:INFO:Initializing create_model()
2025-01-09 10:22:02,757:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:02,757:INFO:Checking exceptions
2025-01-09 10:22:02,757:INFO:Importing libraries
2025-01-09 10:22:02,757:INFO:Copying training dataset
2025-01-09 10:22:02,759:INFO:Defining folds
2025-01-09 10:22:02,759:INFO:Declaring metric variables
2025-01-09 10:22:02,762:INFO:Importing untrained model
2025-01-09 10:22:02,765:INFO:K Neighbors Classifier Imported successfully
2025-01-09 10:22:02,769:INFO:Starting cross validation
2025-01-09 10:22:02,769:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:04,636:INFO:Calculating mean and std
2025-01-09 10:22:04,637:INFO:Creating metrics dataframe
2025-01-09 10:22:04,638:INFO:Uploading results into container
2025-01-09 10:22:04,639:INFO:Uploading model into container now
2025-01-09 10:22:04,639:INFO:_master_model_container: 2
2025-01-09 10:22:04,639:INFO:_display_container: 2
2025-01-09 10:22:04,639:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-01-09 10:22:04,639:INFO:create_model() successfully completed......................................
2025-01-09 10:22:04,715:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:04,715:INFO:Creating metrics dataframe
2025-01-09 10:22:04,720:INFO:Initializing Naive Bayes
2025-01-09 10:22:04,720:INFO:Total runtime is 0.07933963934580485 minutes
2025-01-09 10:22:04,721:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:04,721:INFO:Initializing create_model()
2025-01-09 10:22:04,722:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:04,722:INFO:Checking exceptions
2025-01-09 10:22:04,722:INFO:Importing libraries
2025-01-09 10:22:04,722:INFO:Copying training dataset
2025-01-09 10:22:04,724:INFO:Defining folds
2025-01-09 10:22:04,724:INFO:Declaring metric variables
2025-01-09 10:22:04,726:INFO:Importing untrained model
2025-01-09 10:22:04,728:INFO:Naive Bayes Imported successfully
2025-01-09 10:22:04,731:INFO:Starting cross validation
2025-01-09 10:22:04,731:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:05,922:INFO:Calculating mean and std
2025-01-09 10:22:05,924:INFO:Creating metrics dataframe
2025-01-09 10:22:05,926:INFO:Uploading results into container
2025-01-09 10:22:05,926:INFO:Uploading model into container now
2025-01-09 10:22:05,927:INFO:_master_model_container: 3
2025-01-09 10:22:05,927:INFO:_display_container: 2
2025-01-09 10:22:05,927:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-01-09 10:22:05,927:INFO:create_model() successfully completed......................................
2025-01-09 10:22:06,000:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:06,000:INFO:Creating metrics dataframe
2025-01-09 10:22:06,005:INFO:Initializing Decision Tree Classifier
2025-01-09 10:22:06,005:INFO:Total runtime is 0.10076087315877279 minutes
2025-01-09 10:22:06,007:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:06,007:INFO:Initializing create_model()
2025-01-09 10:22:06,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:06,007:INFO:Checking exceptions
2025-01-09 10:22:06,007:INFO:Importing libraries
2025-01-09 10:22:06,007:INFO:Copying training dataset
2025-01-09 10:22:06,009:INFO:Defining folds
2025-01-09 10:22:06,009:INFO:Declaring metric variables
2025-01-09 10:22:06,011:INFO:Importing untrained model
2025-01-09 10:22:06,012:INFO:Decision Tree Classifier Imported successfully
2025-01-09 10:22:06,016:INFO:Starting cross validation
2025-01-09 10:22:06,017:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:06,097:INFO:Calculating mean and std
2025-01-09 10:22:06,098:INFO:Creating metrics dataframe
2025-01-09 10:22:06,099:INFO:Uploading results into container
2025-01-09 10:22:06,099:INFO:Uploading model into container now
2025-01-09 10:22:06,099:INFO:_master_model_container: 4
2025-01-09 10:22:06,099:INFO:_display_container: 2
2025-01-09 10:22:06,100:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2061, splitter='best')
2025-01-09 10:22:06,100:INFO:create_model() successfully completed......................................
2025-01-09 10:22:06,169:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:06,169:INFO:Creating metrics dataframe
2025-01-09 10:22:06,174:INFO:Initializing SVM - Linear Kernel
2025-01-09 10:22:06,174:INFO:Total runtime is 0.10358562072118123 minutes
2025-01-09 10:22:06,177:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:06,177:INFO:Initializing create_model()
2025-01-09 10:22:06,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:06,177:INFO:Checking exceptions
2025-01-09 10:22:06,177:INFO:Importing libraries
2025-01-09 10:22:06,177:INFO:Copying training dataset
2025-01-09 10:22:06,179:INFO:Defining folds
2025-01-09 10:22:06,180:INFO:Declaring metric variables
2025-01-09 10:22:06,182:INFO:Importing untrained model
2025-01-09 10:22:06,184:INFO:SVM - Linear Kernel Imported successfully
2025-01-09 10:22:06,187:INFO:Starting cross validation
2025-01-09 10:22:06,188:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:06,243:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:06,269:INFO:Calculating mean and std
2025-01-09 10:22:06,270:INFO:Creating metrics dataframe
2025-01-09 10:22:06,271:INFO:Uploading results into container
2025-01-09 10:22:06,271:INFO:Uploading model into container now
2025-01-09 10:22:06,272:INFO:_master_model_container: 5
2025-01-09 10:22:06,272:INFO:_display_container: 2
2025-01-09 10:22:06,272:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2061, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-01-09 10:22:06,272:INFO:create_model() successfully completed......................................
2025-01-09 10:22:06,344:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:06,344:INFO:Creating metrics dataframe
2025-01-09 10:22:06,349:INFO:Initializing Ridge Classifier
2025-01-09 10:22:06,349:INFO:Total runtime is 0.10648854573567708 minutes
2025-01-09 10:22:06,352:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:06,352:INFO:Initializing create_model()
2025-01-09 10:22:06,352:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:06,352:INFO:Checking exceptions
2025-01-09 10:22:06,352:INFO:Importing libraries
2025-01-09 10:22:06,352:INFO:Copying training dataset
2025-01-09 10:22:06,355:INFO:Defining folds
2025-01-09 10:22:06,355:INFO:Declaring metric variables
2025-01-09 10:22:06,357:INFO:Importing untrained model
2025-01-09 10:22:06,359:INFO:Ridge Classifier Imported successfully
2025-01-09 10:22:06,363:INFO:Starting cross validation
2025-01-09 10:22:06,364:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:06,445:INFO:Calculating mean and std
2025-01-09 10:22:06,446:INFO:Creating metrics dataframe
2025-01-09 10:22:06,447:INFO:Uploading results into container
2025-01-09 10:22:06,447:INFO:Uploading model into container now
2025-01-09 10:22:06,448:INFO:_master_model_container: 6
2025-01-09 10:22:06,448:INFO:_display_container: 2
2025-01-09 10:22:06,448:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2061, solver='auto',
                tol=0.0001)
2025-01-09 10:22:06,448:INFO:create_model() successfully completed......................................
2025-01-09 10:22:06,516:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:06,516:INFO:Creating metrics dataframe
2025-01-09 10:22:06,522:INFO:Initializing Random Forest Classifier
2025-01-09 10:22:06,522:INFO:Total runtime is 0.10938334465026855 minutes
2025-01-09 10:22:06,524:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:06,525:INFO:Initializing create_model()
2025-01-09 10:22:06,525:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:06,525:INFO:Checking exceptions
2025-01-09 10:22:06,525:INFO:Importing libraries
2025-01-09 10:22:06,525:INFO:Copying training dataset
2025-01-09 10:22:06,528:INFO:Defining folds
2025-01-09 10:22:06,528:INFO:Declaring metric variables
2025-01-09 10:22:06,530:INFO:Importing untrained model
2025-01-09 10:22:06,532:INFO:Random Forest Classifier Imported successfully
2025-01-09 10:22:06,536:INFO:Starting cross validation
2025-01-09 10:22:06,538:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:06,812:INFO:Calculating mean and std
2025-01-09 10:22:06,813:INFO:Creating metrics dataframe
2025-01-09 10:22:06,814:INFO:Uploading results into container
2025-01-09 10:22:06,814:INFO:Uploading model into container now
2025-01-09 10:22:06,815:INFO:_master_model_container: 7
2025-01-09 10:22:06,815:INFO:_display_container: 2
2025-01-09 10:22:06,815:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2061, verbose=0,
                       warm_start=False)
2025-01-09 10:22:06,815:INFO:create_model() successfully completed......................................
2025-01-09 10:22:06,891:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:06,892:INFO:Creating metrics dataframe
2025-01-09 10:22:06,896:INFO:Initializing Quadratic Discriminant Analysis
2025-01-09 10:22:06,896:INFO:Total runtime is 0.11561719179153443 minutes
2025-01-09 10:22:06,900:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:06,900:INFO:Initializing create_model()
2025-01-09 10:22:06,900:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:06,900:INFO:Checking exceptions
2025-01-09 10:22:06,900:INFO:Importing libraries
2025-01-09 10:22:06,900:INFO:Copying training dataset
2025-01-09 10:22:06,903:INFO:Defining folds
2025-01-09 10:22:06,903:INFO:Declaring metric variables
2025-01-09 10:22:06,905:INFO:Importing untrained model
2025-01-09 10:22:06,907:INFO:Quadratic Discriminant Analysis Imported successfully
2025-01-09 10:22:06,910:INFO:Starting cross validation
2025-01-09 10:22:06,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:06,945:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,946:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,947:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,948:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,948:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,956:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,958:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,959:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,960:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,964:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:22:06,990:INFO:Calculating mean and std
2025-01-09 10:22:06,991:INFO:Creating metrics dataframe
2025-01-09 10:22:06,992:INFO:Uploading results into container
2025-01-09 10:22:06,992:INFO:Uploading model into container now
2025-01-09 10:22:06,993:INFO:_master_model_container: 8
2025-01-09 10:22:06,993:INFO:_display_container: 2
2025-01-09 10:22:06,993:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-01-09 10:22:06,993:INFO:create_model() successfully completed......................................
2025-01-09 10:22:07,063:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:07,063:INFO:Creating metrics dataframe
2025-01-09 10:22:07,068:INFO:Initializing Ada Boost Classifier
2025-01-09 10:22:07,068:INFO:Total runtime is 0.11847048997879028 minutes
2025-01-09 10:22:07,071:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:07,071:INFO:Initializing create_model()
2025-01-09 10:22:07,071:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:07,071:INFO:Checking exceptions
2025-01-09 10:22:07,071:INFO:Importing libraries
2025-01-09 10:22:07,071:INFO:Copying training dataset
2025-01-09 10:22:07,075:INFO:Defining folds
2025-01-09 10:22:07,075:INFO:Declaring metric variables
2025-01-09 10:22:07,077:INFO:Importing untrained model
2025-01-09 10:22:07,079:INFO:Ada Boost Classifier Imported successfully
2025-01-09 10:22:07,083:INFO:Starting cross validation
2025-01-09 10:22:07,084:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:07,114:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,118:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,120:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,120:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,122:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,124:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,124:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,126:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,128:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,133:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:22:07,237:INFO:Calculating mean and std
2025-01-09 10:22:07,237:INFO:Creating metrics dataframe
2025-01-09 10:22:07,238:INFO:Uploading results into container
2025-01-09 10:22:07,239:INFO:Uploading model into container now
2025-01-09 10:22:07,239:INFO:_master_model_container: 9
2025-01-09 10:22:07,239:INFO:_display_container: 2
2025-01-09 10:22:07,240:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=2061)
2025-01-09 10:22:07,240:INFO:create_model() successfully completed......................................
2025-01-09 10:22:07,311:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:07,311:INFO:Creating metrics dataframe
2025-01-09 10:22:07,317:INFO:Initializing Gradient Boosting Classifier
2025-01-09 10:22:07,317:INFO:Total runtime is 0.1226265549659729 minutes
2025-01-09 10:22:07,320:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:07,320:INFO:Initializing create_model()
2025-01-09 10:22:07,320:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:07,320:INFO:Checking exceptions
2025-01-09 10:22:07,320:INFO:Importing libraries
2025-01-09 10:22:07,320:INFO:Copying training dataset
2025-01-09 10:22:07,324:INFO:Defining folds
2025-01-09 10:22:07,324:INFO:Declaring metric variables
2025-01-09 10:22:07,326:INFO:Importing untrained model
2025-01-09 10:22:07,328:INFO:Gradient Boosting Classifier Imported successfully
2025-01-09 10:22:07,331:INFO:Starting cross validation
2025-01-09 10:22:07,332:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:07,516:INFO:Calculating mean and std
2025-01-09 10:22:07,517:INFO:Creating metrics dataframe
2025-01-09 10:22:07,518:INFO:Uploading results into container
2025-01-09 10:22:07,518:INFO:Uploading model into container now
2025-01-09 10:22:07,519:INFO:_master_model_container: 10
2025-01-09 10:22:07,519:INFO:_display_container: 2
2025-01-09 10:22:07,519:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2061, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:22:07,519:INFO:create_model() successfully completed......................................
2025-01-09 10:22:07,584:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:07,584:INFO:Creating metrics dataframe
2025-01-09 10:22:07,590:INFO:Initializing Linear Discriminant Analysis
2025-01-09 10:22:07,590:INFO:Total runtime is 0.12718399365743002 minutes
2025-01-09 10:22:07,592:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:07,592:INFO:Initializing create_model()
2025-01-09 10:22:07,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:07,593:INFO:Checking exceptions
2025-01-09 10:22:07,593:INFO:Importing libraries
2025-01-09 10:22:07,593:INFO:Copying training dataset
2025-01-09 10:22:07,595:INFO:Defining folds
2025-01-09 10:22:07,595:INFO:Declaring metric variables
2025-01-09 10:22:07,596:INFO:Importing untrained model
2025-01-09 10:22:07,598:INFO:Linear Discriminant Analysis Imported successfully
2025-01-09 10:22:07,601:INFO:Starting cross validation
2025-01-09 10:22:07,602:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:07,682:INFO:Calculating mean and std
2025-01-09 10:22:07,683:INFO:Creating metrics dataframe
2025-01-09 10:22:07,684:INFO:Uploading results into container
2025-01-09 10:22:07,684:INFO:Uploading model into container now
2025-01-09 10:22:07,685:INFO:_master_model_container: 11
2025-01-09 10:22:07,685:INFO:_display_container: 2
2025-01-09 10:22:07,685:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-01-09 10:22:07,685:INFO:create_model() successfully completed......................................
2025-01-09 10:22:07,753:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:07,754:INFO:Creating metrics dataframe
2025-01-09 10:22:07,761:INFO:Initializing Extra Trees Classifier
2025-01-09 10:22:07,762:INFO:Total runtime is 0.13004496494928997 minutes
2025-01-09 10:22:07,764:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:07,764:INFO:Initializing create_model()
2025-01-09 10:22:07,764:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:07,764:INFO:Checking exceptions
2025-01-09 10:22:07,764:INFO:Importing libraries
2025-01-09 10:22:07,764:INFO:Copying training dataset
2025-01-09 10:22:07,767:INFO:Defining folds
2025-01-09 10:22:07,768:INFO:Declaring metric variables
2025-01-09 10:22:07,769:INFO:Importing untrained model
2025-01-09 10:22:07,772:INFO:Extra Trees Classifier Imported successfully
2025-01-09 10:22:07,776:INFO:Starting cross validation
2025-01-09 10:22:07,777:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:08,022:INFO:Calculating mean and std
2025-01-09 10:22:08,024:INFO:Creating metrics dataframe
2025-01-09 10:22:08,025:INFO:Uploading results into container
2025-01-09 10:22:08,025:INFO:Uploading model into container now
2025-01-09 10:22:08,026:INFO:_master_model_container: 12
2025-01-09 10:22:08,026:INFO:_display_container: 2
2025-01-09 10:22:08,026:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2061, verbose=0,
                     warm_start=False)
2025-01-09 10:22:08,026:INFO:create_model() successfully completed......................................
2025-01-09 10:22:08,093:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:08,094:INFO:Creating metrics dataframe
2025-01-09 10:22:08,100:INFO:Initializing Extreme Gradient Boosting
2025-01-09 10:22:08,100:INFO:Total runtime is 0.13568394978841147 minutes
2025-01-09 10:22:08,102:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:08,102:INFO:Initializing create_model()
2025-01-09 10:22:08,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:08,102:INFO:Checking exceptions
2025-01-09 10:22:08,102:INFO:Importing libraries
2025-01-09 10:22:08,102:INFO:Copying training dataset
2025-01-09 10:22:08,106:INFO:Defining folds
2025-01-09 10:22:08,106:INFO:Declaring metric variables
2025-01-09 10:22:08,108:INFO:Importing untrained model
2025-01-09 10:22:08,110:INFO:Extreme Gradient Boosting Imported successfully
2025-01-09 10:22:08,113:INFO:Starting cross validation
2025-01-09 10:22:08,114:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:10,554:INFO:Calculating mean and std
2025-01-09 10:22:10,554:INFO:Creating metrics dataframe
2025-01-09 10:22:10,556:INFO:Uploading results into container
2025-01-09 10:22:10,556:INFO:Uploading model into container now
2025-01-09 10:22:10,557:INFO:_master_model_container: 13
2025-01-09 10:22:10,557:INFO:_display_container: 2
2025-01-09 10:22:10,557:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-01-09 10:22:10,557:INFO:create_model() successfully completed......................................
2025-01-09 10:22:10,628:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:10,628:INFO:Creating metrics dataframe
2025-01-09 10:22:10,635:INFO:Initializing Light Gradient Boosting Machine
2025-01-09 10:22:10,635:INFO:Total runtime is 0.1779224514961243 minutes
2025-01-09 10:22:10,637:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:10,637:INFO:Initializing create_model()
2025-01-09 10:22:10,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:10,638:INFO:Checking exceptions
2025-01-09 10:22:10,638:INFO:Importing libraries
2025-01-09 10:22:10,638:INFO:Copying training dataset
2025-01-09 10:22:10,640:INFO:Defining folds
2025-01-09 10:22:10,640:INFO:Declaring metric variables
2025-01-09 10:22:10,641:INFO:Importing untrained model
2025-01-09 10:22:10,643:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:22:10,647:INFO:Starting cross validation
2025-01-09 10:22:10,648:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:11,843:INFO:Calculating mean and std
2025-01-09 10:22:11,843:INFO:Creating metrics dataframe
2025-01-09 10:22:11,843:INFO:Uploading results into container
2025-01-09 10:22:11,846:INFO:Uploading model into container now
2025-01-09 10:22:11,847:INFO:_master_model_container: 14
2025-01-09 10:22:11,847:INFO:_display_container: 2
2025-01-09 10:22:11,847:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2061, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:22:11,847:INFO:create_model() successfully completed......................................
2025-01-09 10:22:11,927:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:11,928:INFO:Creating metrics dataframe
2025-01-09 10:22:11,935:INFO:Initializing Dummy Classifier
2025-01-09 10:22:11,935:INFO:Total runtime is 0.19959282477696738 minutes
2025-01-09 10:22:11,937:INFO:SubProcess create_model() called ==================================
2025-01-09 10:22:11,937:INFO:Initializing create_model()
2025-01-09 10:22:11,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13F1B210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:11,937:INFO:Checking exceptions
2025-01-09 10:22:11,937:INFO:Importing libraries
2025-01-09 10:22:11,937:INFO:Copying training dataset
2025-01-09 10:22:11,939:INFO:Defining folds
2025-01-09 10:22:11,940:INFO:Declaring metric variables
2025-01-09 10:22:11,941:INFO:Importing untrained model
2025-01-09 10:22:11,943:INFO:Dummy Classifier Imported successfully
2025-01-09 10:22:11,946:INFO:Starting cross validation
2025-01-09 10:22:11,947:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:22:11,991:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:11,993:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:11,996:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:11,996:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,000:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,012:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,012:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,012:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,013:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,019:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:22:12,025:INFO:Calculating mean and std
2025-01-09 10:22:12,026:INFO:Creating metrics dataframe
2025-01-09 10:22:12,027:INFO:Uploading results into container
2025-01-09 10:22:12,027:INFO:Uploading model into container now
2025-01-09 10:22:12,028:INFO:_master_model_container: 15
2025-01-09 10:22:12,028:INFO:_display_container: 2
2025-01-09 10:22:12,028:INFO:DummyClassifier(constant=None, random_state=2061, strategy='prior')
2025-01-09 10:22:12,028:INFO:create_model() successfully completed......................................
2025-01-09 10:22:12,095:INFO:SubProcess create_model() end ==================================
2025-01-09 10:22:12,095:INFO:Creating metrics dataframe
2025-01-09 10:22:12,101:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-01-09 10:22:12,108:INFO:Initializing create_model()
2025-01-09 10:22:12,108:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2061, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:22:12,108:INFO:Checking exceptions
2025-01-09 10:22:12,109:INFO:Importing libraries
2025-01-09 10:22:12,109:INFO:Copying training dataset
2025-01-09 10:22:12,111:INFO:Defining folds
2025-01-09 10:22:12,111:INFO:Declaring metric variables
2025-01-09 10:22:12,111:INFO:Importing untrained model
2025-01-09 10:22:12,111:INFO:Declaring custom model
2025-01-09 10:22:12,111:INFO:Gradient Boosting Classifier Imported successfully
2025-01-09 10:22:12,112:INFO:Cross validation set to False
2025-01-09 10:22:12,112:INFO:Fitting Model
2025-01-09 10:22:12,201:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2061, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:22:12,201:INFO:create_model() successfully completed......................................
2025-01-09 10:22:12,285:INFO:_master_model_container: 15
2025-01-09 10:22:12,285:INFO:_display_container: 2
2025-01-09 10:22:12,286:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2061, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:22:12,286:INFO:compare_models() successfully completed......................................
2025-01-09 10:24:23,987:INFO:Initializing set_config()
2025-01-09 10:24:23,987:INFO:set_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F1370EF10>, variable=seed, value=11, kwargs={})
2025-01-09 10:24:23,987:INFO:Global variable: seed updated to 11
2025-01-09 10:24:23,987:INFO:set_config() successfully completed......................................
2025-01-09 10:24:23,989:INFO:PyCaret ClassificationExperiment
2025-01-09 10:24:23,989:INFO:Logging name: clf-default-name
2025-01-09 10:24:23,989:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-01-09 10:24:23,989:INFO:version 3.3.2
2025-01-09 10:24:23,989:INFO:Initializing setup()
2025-01-09 10:24:23,989:INFO:self.USI: 7e78
2025-01-09 10:24:23,989:INFO:self._variable_keys: {'html_param', 'target_param', 'logging_param', 'seed', 'fold_shuffle_param', 'fix_imbalance', 'X', 'y', 'exp_name_log', '_available_plots', 'is_multiclass', 'idx', 'gpu_n_jobs_param', 'USI', 'memory', 'y_test', 'X_train', 'y_train', 'data', 'X_test', 'n_jobs_param', 'gpu_param', '_ml_usecase', 'exp_id', 'fold_generator', 'fold_groups_param', 'log_plots_param', 'pipeline'}
2025-01-09 10:24:23,989:INFO:Checking environment
2025-01-09 10:24:23,989:INFO:python_version: 3.11.9
2025-01-09 10:24:23,989:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2025-01-09 10:24:23,989:INFO:machine: AMD64
2025-01-09 10:24:23,989:INFO:platform: Windows-10-10.0.22631-SP0
2025-01-09 10:24:23,996:INFO:Memory: svmem(total=33837973504, available=11865325568, percent=64.9, used=21972647936, free=11865325568)
2025-01-09 10:24:23,996:INFO:Physical Core: 16
2025-01-09 10:24:23,996:INFO:Logical Core: 22
2025-01-09 10:24:23,996:INFO:Checking libraries
2025-01-09 10:24:23,996:INFO:System:
2025-01-09 10:24:23,996:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2025-01-09 10:24:23,996:INFO:executable: c:\wanted\envs\ml_practice\Scripts\python.exe
2025-01-09 10:24:23,996:INFO:   machine: Windows-10-10.0.22631-SP0
2025-01-09 10:24:23,996:INFO:PyCaret required dependencies:
2025-01-09 10:24:23,996:INFO:                 pip: 24.3.1
2025-01-09 10:24:23,996:INFO:          setuptools: 65.5.0
2025-01-09 10:24:23,996:INFO:             pycaret: 3.3.2
2025-01-09 10:24:23,996:INFO:             IPython: 8.31.0
2025-01-09 10:24:23,996:INFO:          ipywidgets: 8.1.5
2025-01-09 10:24:23,996:INFO:                tqdm: 4.67.1
2025-01-09 10:24:23,996:INFO:               numpy: 1.26.4
2025-01-09 10:24:23,996:INFO:              pandas: 2.1.4
2025-01-09 10:24:23,996:INFO:              jinja2: 3.1.5
2025-01-09 10:24:23,996:INFO:               scipy: 1.11.4
2025-01-09 10:24:23,996:INFO:              joblib: 1.3.2
2025-01-09 10:24:23,996:INFO:             sklearn: 1.4.2
2025-01-09 10:24:23,996:INFO:                pyod: 2.0.3
2025-01-09 10:24:23,996:INFO:            imblearn: 0.13.0
2025-01-09 10:24:23,996:INFO:   category_encoders: 2.7.0
2025-01-09 10:24:23,996:INFO:            lightgbm: 4.5.0
2025-01-09 10:24:23,996:INFO:               numba: 0.60.0
2025-01-09 10:24:23,996:INFO:            requests: 2.32.3
2025-01-09 10:24:23,996:INFO:          matplotlib: 3.7.5
2025-01-09 10:24:23,996:INFO:          scikitplot: 0.3.7
2025-01-09 10:24:23,996:INFO:         yellowbrick: 1.5
2025-01-09 10:24:23,997:INFO:              plotly: 5.24.1
2025-01-09 10:24:23,997:INFO:    plotly-resampler: Not installed
2025-01-09 10:24:23,997:INFO:             kaleido: 0.2.1
2025-01-09 10:24:23,997:INFO:           schemdraw: 0.15
2025-01-09 10:24:23,997:INFO:         statsmodels: 0.14.4
2025-01-09 10:24:23,997:INFO:              sktime: 0.26.0
2025-01-09 10:24:23,997:INFO:               tbats: 1.1.3
2025-01-09 10:24:23,997:INFO:            pmdarima: 2.0.4
2025-01-09 10:24:23,997:INFO:              psutil: 6.1.1
2025-01-09 10:24:23,997:INFO:          markupsafe: 3.0.2
2025-01-09 10:24:23,997:INFO:             pickle5: Not installed
2025-01-09 10:24:23,997:INFO:         cloudpickle: 3.1.0
2025-01-09 10:24:23,997:INFO:         deprecation: 2.1.0
2025-01-09 10:24:23,997:INFO:              xxhash: 3.5.0
2025-01-09 10:24:23,997:INFO:           wurlitzer: Not installed
2025-01-09 10:24:23,997:INFO:PyCaret optional dependencies:
2025-01-09 10:24:23,997:INFO:                shap: Not installed
2025-01-09 10:24:23,997:INFO:           interpret: Not installed
2025-01-09 10:24:23,997:INFO:                umap: Not installed
2025-01-09 10:24:23,997:INFO:     ydata_profiling: Not installed
2025-01-09 10:24:23,997:INFO:  explainerdashboard: Not installed
2025-01-09 10:24:23,997:INFO:             autoviz: Not installed
2025-01-09 10:24:23,997:INFO:           fairlearn: Not installed
2025-01-09 10:24:23,997:INFO:          deepchecks: Not installed
2025-01-09 10:24:23,997:INFO:             xgboost: 2.1.3
2025-01-09 10:24:23,997:INFO:            catboost: Not installed
2025-01-09 10:24:23,997:INFO:              kmodes: Not installed
2025-01-09 10:24:23,997:INFO:             mlxtend: Not installed
2025-01-09 10:24:23,997:INFO:       statsforecast: Not installed
2025-01-09 10:24:23,997:INFO:        tune_sklearn: Not installed
2025-01-09 10:24:23,997:INFO:                 ray: Not installed
2025-01-09 10:24:23,997:INFO:            hyperopt: Not installed
2025-01-09 10:24:23,997:INFO:              optuna: Not installed
2025-01-09 10:24:23,997:INFO:               skopt: Not installed
2025-01-09 10:24:23,997:INFO:              mlflow: Not installed
2025-01-09 10:24:23,997:INFO:              gradio: Not installed
2025-01-09 10:24:23,997:INFO:             fastapi: Not installed
2025-01-09 10:24:23,997:INFO:             uvicorn: Not installed
2025-01-09 10:24:23,997:INFO:              m2cgen: Not installed
2025-01-09 10:24:23,997:INFO:           evidently: Not installed
2025-01-09 10:24:23,997:INFO:               fugue: Not installed
2025-01-09 10:24:23,997:INFO:           streamlit: Not installed
2025-01-09 10:24:23,997:INFO:             prophet: Not installed
2025-01-09 10:24:23,997:INFO:None
2025-01-09 10:24:23,997:INFO:Set up data.
2025-01-09 10:24:24,002:INFO:Set up folding strategy.
2025-01-09 10:24:24,002:INFO:Set up train/test split.
2025-01-09 10:24:24,006:INFO:Set up index.
2025-01-09 10:24:24,006:INFO:Assigning column types.
2025-01-09 10:24:24,008:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-01-09 10:24:24,032:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-01-09 10:24:24,032:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:24:24,048:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,050:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,073:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-01-09 10:24:24,074:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:24:24,089:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,091:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,091:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-01-09 10:24:24,116:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:24:24,132:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,133:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,159:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-01-09 10:24:24,175:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,176:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,176:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-01-09 10:24:24,216:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,217:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,259:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,262:INFO:Preparing preprocessing pipeline...
2025-01-09 10:24:24,262:INFO:Set up simple imputation.
2025-01-09 10:24:24,263:INFO:Set up encoding of ordinal features.
2025-01-09 10:24:24,264:INFO:Set up encoding of categorical features.
2025-01-09 10:24:24,299:INFO:Finished creating preprocessing pipeline.
2025-01-09 10:24:24,307:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Age', 'SibSp', 'Parch',
                                             'Fare'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Transf...
                                                               mapping=[{'col': 'Sex',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None, include=['Embarked'],
                                    transformer=OneHotEncoder(cols=['Embarked'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-01-09 10:24:24,307:INFO:Creating final display dataframe.
2025-01-09 10:24:24,405:INFO:Setup _display_container:                     Description             Value
0                    Session id              5001
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 12)
4        Transformed data shape         (891, 10)
5   Transformed train set shape         (623, 10)
6    Transformed test set shape         (268, 10)
7               Ignore features                 4
8              Numeric features                 5
9          Categorical features                 2
10     Rows with missing values             79.5%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Fold Generator   StratifiedKFold
18                  Fold Number                10
19                     CPU Jobs                -1
20                      Use GPU             False
21               Log Experiment             False
22              Experiment Name  clf-default-name
23                          USI              7e78
2025-01-09 10:24:24,449:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,451:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,494:INFO:Soft dependency imported: xgboost: 2.1.3
2025-01-09 10:24:24,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-01-09 10:24:24,496:INFO:setup() successfully completed in 0.51s...............
2025-01-09 10:24:31,371:INFO:Initializing compare_models()
2025-01-09 10:24:31,372:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-01-09 10:24:31,372:INFO:Checking exceptions
2025-01-09 10:24:31,375:INFO:Preparing display monitor
2025-01-09 10:24:31,389:INFO:Initializing Logistic Regression
2025-01-09 10:24:31,389:INFO:Total runtime is 0.0 minutes
2025-01-09 10:24:31,391:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:31,391:INFO:Initializing create_model()
2025-01-09 10:24:31,391:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:31,391:INFO:Checking exceptions
2025-01-09 10:24:31,391:INFO:Importing libraries
2025-01-09 10:24:31,392:INFO:Copying training dataset
2025-01-09 10:24:31,393:INFO:Defining folds
2025-01-09 10:24:31,393:INFO:Declaring metric variables
2025-01-09 10:24:31,395:INFO:Importing untrained model
2025-01-09 10:24:31,397:INFO:Logistic Regression Imported successfully
2025-01-09 10:24:31,401:INFO:Starting cross validation
2025-01-09 10:24:31,402:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:31,516:INFO:Calculating mean and std
2025-01-09 10:24:31,516:INFO:Creating metrics dataframe
2025-01-09 10:24:31,517:INFO:Uploading results into container
2025-01-09 10:24:31,518:INFO:Uploading model into container now
2025-01-09 10:24:31,518:INFO:_master_model_container: 1
2025-01-09 10:24:31,518:INFO:_display_container: 2
2025-01-09 10:24:31,518:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5001, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-01-09 10:24:31,518:INFO:create_model() successfully completed......................................
2025-01-09 10:24:31,594:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:31,594:INFO:Creating metrics dataframe
2025-01-09 10:24:31,598:INFO:Initializing K Neighbors Classifier
2025-01-09 10:24:31,598:INFO:Total runtime is 0.003487185637156169 minutes
2025-01-09 10:24:31,601:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:31,601:INFO:Initializing create_model()
2025-01-09 10:24:31,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:31,601:INFO:Checking exceptions
2025-01-09 10:24:31,601:INFO:Importing libraries
2025-01-09 10:24:31,601:INFO:Copying training dataset
2025-01-09 10:24:31,604:INFO:Defining folds
2025-01-09 10:24:31,604:INFO:Declaring metric variables
2025-01-09 10:24:31,606:INFO:Importing untrained model
2025-01-09 10:24:31,608:INFO:K Neighbors Classifier Imported successfully
2025-01-09 10:24:31,612:INFO:Starting cross validation
2025-01-09 10:24:31,613:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:31,736:INFO:Calculating mean and std
2025-01-09 10:24:31,736:INFO:Creating metrics dataframe
2025-01-09 10:24:31,737:INFO:Uploading results into container
2025-01-09 10:24:31,737:INFO:Uploading model into container now
2025-01-09 10:24:31,738:INFO:_master_model_container: 2
2025-01-09 10:24:31,738:INFO:_display_container: 2
2025-01-09 10:24:31,738:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-01-09 10:24:31,738:INFO:create_model() successfully completed......................................
2025-01-09 10:24:31,811:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:31,811:INFO:Creating metrics dataframe
2025-01-09 10:24:31,816:INFO:Initializing Naive Bayes
2025-01-09 10:24:31,816:INFO:Total runtime is 0.007118598620096842 minutes
2025-01-09 10:24:31,819:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:31,819:INFO:Initializing create_model()
2025-01-09 10:24:31,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:31,819:INFO:Checking exceptions
2025-01-09 10:24:31,819:INFO:Importing libraries
2025-01-09 10:24:31,819:INFO:Copying training dataset
2025-01-09 10:24:31,821:INFO:Defining folds
2025-01-09 10:24:31,821:INFO:Declaring metric variables
2025-01-09 10:24:31,824:INFO:Importing untrained model
2025-01-09 10:24:31,826:INFO:Naive Bayes Imported successfully
2025-01-09 10:24:31,830:INFO:Starting cross validation
2025-01-09 10:24:31,831:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:31,910:INFO:Calculating mean and std
2025-01-09 10:24:31,910:INFO:Creating metrics dataframe
2025-01-09 10:24:31,911:INFO:Uploading results into container
2025-01-09 10:24:31,911:INFO:Uploading model into container now
2025-01-09 10:24:31,912:INFO:_master_model_container: 3
2025-01-09 10:24:31,912:INFO:_display_container: 2
2025-01-09 10:24:31,912:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-01-09 10:24:31,912:INFO:create_model() successfully completed......................................
2025-01-09 10:24:31,984:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:31,984:INFO:Creating metrics dataframe
2025-01-09 10:24:31,988:INFO:Initializing Decision Tree Classifier
2025-01-09 10:24:31,989:INFO:Total runtime is 0.010001262029012043 minutes
2025-01-09 10:24:31,991:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:32,000:INFO:Initializing create_model()
2025-01-09 10:24:32,000:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:32,000:INFO:Checking exceptions
2025-01-09 10:24:32,000:INFO:Importing libraries
2025-01-09 10:24:32,000:INFO:Copying training dataset
2025-01-09 10:24:32,002:INFO:Defining folds
2025-01-09 10:24:32,002:INFO:Declaring metric variables
2025-01-09 10:24:32,003:INFO:Importing untrained model
2025-01-09 10:24:32,006:INFO:Decision Tree Classifier Imported successfully
2025-01-09 10:24:32,010:INFO:Starting cross validation
2025-01-09 10:24:32,012:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:32,083:INFO:Calculating mean and std
2025-01-09 10:24:32,083:INFO:Creating metrics dataframe
2025-01-09 10:24:32,084:INFO:Uploading results into container
2025-01-09 10:24:32,084:INFO:Uploading model into container now
2025-01-09 10:24:32,085:INFO:_master_model_container: 4
2025-01-09 10:24:32,085:INFO:_display_container: 2
2025-01-09 10:24:32,085:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5001, splitter='best')
2025-01-09 10:24:32,085:INFO:create_model() successfully completed......................................
2025-01-09 10:24:32,159:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:32,160:INFO:Creating metrics dataframe
2025-01-09 10:24:32,165:INFO:Initializing SVM - Linear Kernel
2025-01-09 10:24:32,165:INFO:Total runtime is 0.012937140464782714 minutes
2025-01-09 10:24:32,167:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:32,167:INFO:Initializing create_model()
2025-01-09 10:24:32,167:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:32,167:INFO:Checking exceptions
2025-01-09 10:24:32,167:INFO:Importing libraries
2025-01-09 10:24:32,167:INFO:Copying training dataset
2025-01-09 10:24:32,169:INFO:Defining folds
2025-01-09 10:24:32,169:INFO:Declaring metric variables
2025-01-09 10:24:32,171:INFO:Importing untrained model
2025-01-09 10:24:32,173:INFO:SVM - Linear Kernel Imported successfully
2025-01-09 10:24:32,176:INFO:Starting cross validation
2025-01-09 10:24:32,178:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:32,245:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:32,251:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:32,258:INFO:Calculating mean and std
2025-01-09 10:24:32,259:INFO:Creating metrics dataframe
2025-01-09 10:24:32,260:INFO:Uploading results into container
2025-01-09 10:24:32,260:INFO:Uploading model into container now
2025-01-09 10:24:32,260:INFO:_master_model_container: 5
2025-01-09 10:24:32,260:INFO:_display_container: 2
2025-01-09 10:24:32,261:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5001, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-01-09 10:24:32,261:INFO:create_model() successfully completed......................................
2025-01-09 10:24:32,337:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:32,337:INFO:Creating metrics dataframe
2025-01-09 10:24:32,342:INFO:Initializing Ridge Classifier
2025-01-09 10:24:32,342:INFO:Total runtime is 0.015880461533864337 minutes
2025-01-09 10:24:32,346:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:32,346:INFO:Initializing create_model()
2025-01-09 10:24:32,346:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:32,346:INFO:Checking exceptions
2025-01-09 10:24:32,346:INFO:Importing libraries
2025-01-09 10:24:32,347:INFO:Copying training dataset
2025-01-09 10:24:32,348:INFO:Defining folds
2025-01-09 10:24:32,349:INFO:Declaring metric variables
2025-01-09 10:24:32,350:INFO:Importing untrained model
2025-01-09 10:24:32,352:INFO:Ridge Classifier Imported successfully
2025-01-09 10:24:32,355:INFO:Starting cross validation
2025-01-09 10:24:32,356:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:32,437:INFO:Calculating mean and std
2025-01-09 10:24:32,438:INFO:Creating metrics dataframe
2025-01-09 10:24:32,440:INFO:Uploading results into container
2025-01-09 10:24:32,440:INFO:Uploading model into container now
2025-01-09 10:24:32,440:INFO:_master_model_container: 6
2025-01-09 10:24:32,440:INFO:_display_container: 2
2025-01-09 10:24:32,441:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5001, solver='auto',
                tol=0.0001)
2025-01-09 10:24:32,441:INFO:create_model() successfully completed......................................
2025-01-09 10:24:32,513:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:32,514:INFO:Creating metrics dataframe
2025-01-09 10:24:32,519:INFO:Initializing Random Forest Classifier
2025-01-09 10:24:32,519:INFO:Total runtime is 0.01883862018585205 minutes
2025-01-09 10:24:32,522:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:32,523:INFO:Initializing create_model()
2025-01-09 10:24:32,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:32,523:INFO:Checking exceptions
2025-01-09 10:24:32,523:INFO:Importing libraries
2025-01-09 10:24:32,523:INFO:Copying training dataset
2025-01-09 10:24:32,525:INFO:Defining folds
2025-01-09 10:24:32,525:INFO:Declaring metric variables
2025-01-09 10:24:32,527:INFO:Importing untrained model
2025-01-09 10:24:32,530:INFO:Random Forest Classifier Imported successfully
2025-01-09 10:24:32,533:INFO:Starting cross validation
2025-01-09 10:24:32,534:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:32,810:INFO:Calculating mean and std
2025-01-09 10:24:32,811:INFO:Creating metrics dataframe
2025-01-09 10:24:32,812:INFO:Uploading results into container
2025-01-09 10:24:32,812:INFO:Uploading model into container now
2025-01-09 10:24:32,812:INFO:_master_model_container: 7
2025-01-09 10:24:32,812:INFO:_display_container: 2
2025-01-09 10:24:32,813:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5001, verbose=0,
                       warm_start=False)
2025-01-09 10:24:32,813:INFO:create_model() successfully completed......................................
2025-01-09 10:24:32,889:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:32,889:INFO:Creating metrics dataframe
2025-01-09 10:24:32,896:INFO:Initializing Quadratic Discriminant Analysis
2025-01-09 10:24:32,896:INFO:Total runtime is 0.025124935309092204 minutes
2025-01-09 10:24:32,898:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:32,899:INFO:Initializing create_model()
2025-01-09 10:24:32,899:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:32,899:INFO:Checking exceptions
2025-01-09 10:24:32,899:INFO:Importing libraries
2025-01-09 10:24:32,899:INFO:Copying training dataset
2025-01-09 10:24:32,901:INFO:Defining folds
2025-01-09 10:24:32,902:INFO:Declaring metric variables
2025-01-09 10:24:32,903:INFO:Importing untrained model
2025-01-09 10:24:32,904:INFO:Quadratic Discriminant Analysis Imported successfully
2025-01-09 10:24:32,907:INFO:Starting cross validation
2025-01-09 10:24:32,908:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:32,940:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,940:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,942:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,943:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,943:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,950:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,951:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,952:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,957:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,961:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:32,968:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:32,987:INFO:Calculating mean and std
2025-01-09 10:24:32,989:INFO:Creating metrics dataframe
2025-01-09 10:24:32,990:INFO:Uploading results into container
2025-01-09 10:24:32,991:INFO:Uploading model into container now
2025-01-09 10:24:32,991:INFO:_master_model_container: 8
2025-01-09 10:24:32,991:INFO:_display_container: 2
2025-01-09 10:24:32,991:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-01-09 10:24:32,991:INFO:create_model() successfully completed......................................
2025-01-09 10:24:33,062:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:33,062:INFO:Creating metrics dataframe
2025-01-09 10:24:33,067:INFO:Initializing Ada Boost Classifier
2025-01-09 10:24:33,067:INFO:Total runtime is 0.02796717087427775 minutes
2025-01-09 10:24:33,069:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:33,070:INFO:Initializing create_model()
2025-01-09 10:24:33,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:33,070:INFO:Checking exceptions
2025-01-09 10:24:33,070:INFO:Importing libraries
2025-01-09 10:24:33,070:INFO:Copying training dataset
2025-01-09 10:24:33,072:INFO:Defining folds
2025-01-09 10:24:33,072:INFO:Declaring metric variables
2025-01-09 10:24:33,073:INFO:Importing untrained model
2025-01-09 10:24:33,074:INFO:Ada Boost Classifier Imported successfully
2025-01-09 10:24:33,079:INFO:Starting cross validation
2025-01-09 10:24:33,080:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:33,110:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,111:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,115:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,117:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,122:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,123:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,123:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,123:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,131:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,132:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:33,224:INFO:Calculating mean and std
2025-01-09 10:24:33,225:INFO:Creating metrics dataframe
2025-01-09 10:24:33,226:INFO:Uploading results into container
2025-01-09 10:24:33,227:INFO:Uploading model into container now
2025-01-09 10:24:33,227:INFO:_master_model_container: 9
2025-01-09 10:24:33,227:INFO:_display_container: 2
2025-01-09 10:24:33,227:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5001)
2025-01-09 10:24:33,227:INFO:create_model() successfully completed......................................
2025-01-09 10:24:33,300:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:33,300:INFO:Creating metrics dataframe
2025-01-09 10:24:33,305:INFO:Initializing Gradient Boosting Classifier
2025-01-09 10:24:33,306:INFO:Total runtime is 0.03195104598999023 minutes
2025-01-09 10:24:33,308:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:33,308:INFO:Initializing create_model()
2025-01-09 10:24:33,308:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:33,308:INFO:Checking exceptions
2025-01-09 10:24:33,308:INFO:Importing libraries
2025-01-09 10:24:33,308:INFO:Copying training dataset
2025-01-09 10:24:33,312:INFO:Defining folds
2025-01-09 10:24:33,312:INFO:Declaring metric variables
2025-01-09 10:24:33,314:INFO:Importing untrained model
2025-01-09 10:24:33,316:INFO:Gradient Boosting Classifier Imported successfully
2025-01-09 10:24:33,320:INFO:Starting cross validation
2025-01-09 10:24:33,321:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:33,494:INFO:Calculating mean and std
2025-01-09 10:24:33,495:INFO:Creating metrics dataframe
2025-01-09 10:24:33,496:INFO:Uploading results into container
2025-01-09 10:24:33,497:INFO:Uploading model into container now
2025-01-09 10:24:33,497:INFO:_master_model_container: 10
2025-01-09 10:24:33,497:INFO:_display_container: 2
2025-01-09 10:24:33,498:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:24:33,498:INFO:create_model() successfully completed......................................
2025-01-09 10:24:33,572:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:33,572:INFO:Creating metrics dataframe
2025-01-09 10:24:33,577:INFO:Initializing Linear Discriminant Analysis
2025-01-09 10:24:33,578:INFO:Total runtime is 0.03648381630579631 minutes
2025-01-09 10:24:33,581:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:33,581:INFO:Initializing create_model()
2025-01-09 10:24:33,581:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:33,581:INFO:Checking exceptions
2025-01-09 10:24:33,581:INFO:Importing libraries
2025-01-09 10:24:33,581:INFO:Copying training dataset
2025-01-09 10:24:33,583:INFO:Defining folds
2025-01-09 10:24:33,583:INFO:Declaring metric variables
2025-01-09 10:24:33,584:INFO:Importing untrained model
2025-01-09 10:24:33,586:INFO:Linear Discriminant Analysis Imported successfully
2025-01-09 10:24:33,589:INFO:Starting cross validation
2025-01-09 10:24:33,590:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:33,663:INFO:Calculating mean and std
2025-01-09 10:24:33,664:INFO:Creating metrics dataframe
2025-01-09 10:24:33,665:INFO:Uploading results into container
2025-01-09 10:24:33,665:INFO:Uploading model into container now
2025-01-09 10:24:33,665:INFO:_master_model_container: 11
2025-01-09 10:24:33,665:INFO:_display_container: 2
2025-01-09 10:24:33,665:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-01-09 10:24:33,665:INFO:create_model() successfully completed......................................
2025-01-09 10:24:33,742:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:33,742:INFO:Creating metrics dataframe
2025-01-09 10:24:33,747:INFO:Initializing Extra Trees Classifier
2025-01-09 10:24:33,747:INFO:Total runtime is 0.039302698771158856 minutes
2025-01-09 10:24:33,749:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:33,749:INFO:Initializing create_model()
2025-01-09 10:24:33,749:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:33,749:INFO:Checking exceptions
2025-01-09 10:24:33,749:INFO:Importing libraries
2025-01-09 10:24:33,749:INFO:Copying training dataset
2025-01-09 10:24:33,751:INFO:Defining folds
2025-01-09 10:24:33,751:INFO:Declaring metric variables
2025-01-09 10:24:33,753:INFO:Importing untrained model
2025-01-09 10:24:33,754:INFO:Extra Trees Classifier Imported successfully
2025-01-09 10:24:33,759:INFO:Starting cross validation
2025-01-09 10:24:33,759:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:34,015:INFO:Calculating mean and std
2025-01-09 10:24:34,016:INFO:Creating metrics dataframe
2025-01-09 10:24:34,017:INFO:Uploading results into container
2025-01-09 10:24:34,017:INFO:Uploading model into container now
2025-01-09 10:24:34,017:INFO:_master_model_container: 12
2025-01-09 10:24:34,018:INFO:_display_container: 2
2025-01-09 10:24:34,018:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5001, verbose=0,
                     warm_start=False)
2025-01-09 10:24:34,018:INFO:create_model() successfully completed......................................
2025-01-09 10:24:34,091:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:34,091:INFO:Creating metrics dataframe
2025-01-09 10:24:34,099:INFO:Initializing Extreme Gradient Boosting
2025-01-09 10:24:34,099:INFO:Total runtime is 0.045166293780008956 minutes
2025-01-09 10:24:34,101:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:34,101:INFO:Initializing create_model()
2025-01-09 10:24:34,101:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:34,101:INFO:Checking exceptions
2025-01-09 10:24:34,101:INFO:Importing libraries
2025-01-09 10:24:34,101:INFO:Copying training dataset
2025-01-09 10:24:34,103:INFO:Defining folds
2025-01-09 10:24:34,103:INFO:Declaring metric variables
2025-01-09 10:24:34,104:INFO:Importing untrained model
2025-01-09 10:24:34,106:INFO:Extreme Gradient Boosting Imported successfully
2025-01-09 10:24:34,109:INFO:Starting cross validation
2025-01-09 10:24:34,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:34,823:INFO:Calculating mean and std
2025-01-09 10:24:34,824:INFO:Creating metrics dataframe
2025-01-09 10:24:34,825:INFO:Uploading results into container
2025-01-09 10:24:34,826:INFO:Uploading model into container now
2025-01-09 10:24:34,826:INFO:_master_model_container: 13
2025-01-09 10:24:34,826:INFO:_display_container: 2
2025-01-09 10:24:34,827:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-01-09 10:24:34,827:INFO:create_model() successfully completed......................................
2025-01-09 10:24:34,908:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:34,908:INFO:Creating metrics dataframe
2025-01-09 10:24:34,913:INFO:Initializing Light Gradient Boosting Machine
2025-01-09 10:24:34,913:INFO:Total runtime is 0.058739276727040615 minutes
2025-01-09 10:24:34,915:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:34,915:INFO:Initializing create_model()
2025-01-09 10:24:34,915:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:34,915:INFO:Checking exceptions
2025-01-09 10:24:34,915:INFO:Importing libraries
2025-01-09 10:24:34,915:INFO:Copying training dataset
2025-01-09 10:24:34,917:INFO:Defining folds
2025-01-09 10:24:34,917:INFO:Declaring metric variables
2025-01-09 10:24:34,919:INFO:Importing untrained model
2025-01-09 10:24:34,921:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:24:34,924:INFO:Starting cross validation
2025-01-09 10:24:34,925:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:36,092:INFO:Calculating mean and std
2025-01-09 10:24:36,092:INFO:Creating metrics dataframe
2025-01-09 10:24:36,094:INFO:Uploading results into container
2025-01-09 10:24:36,094:INFO:Uploading model into container now
2025-01-09 10:24:36,095:INFO:_master_model_container: 14
2025-01-09 10:24:36,095:INFO:_display_container: 2
2025-01-09 10:24:36,095:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:24:36,095:INFO:create_model() successfully completed......................................
2025-01-09 10:24:36,182:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:36,182:INFO:Creating metrics dataframe
2025-01-09 10:24:36,189:INFO:Initializing Dummy Classifier
2025-01-09 10:24:36,189:INFO:Total runtime is 0.08000914255777995 minutes
2025-01-09 10:24:36,191:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:36,192:INFO:Initializing create_model()
2025-01-09 10:24:36,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15591650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:36,192:INFO:Checking exceptions
2025-01-09 10:24:36,192:INFO:Importing libraries
2025-01-09 10:24:36,192:INFO:Copying training dataset
2025-01-09 10:24:36,195:INFO:Defining folds
2025-01-09 10:24:36,195:INFO:Declaring metric variables
2025-01-09 10:24:36,197:INFO:Importing untrained model
2025-01-09 10:24:36,199:INFO:Dummy Classifier Imported successfully
2025-01-09 10:24:36,203:INFO:Starting cross validation
2025-01-09 10:24:36,204:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:36,253:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,254:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,258:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,260:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,260:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,261:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,263:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,268:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,270:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,270:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:36,282:INFO:Calculating mean and std
2025-01-09 10:24:36,282:INFO:Creating metrics dataframe
2025-01-09 10:24:36,283:INFO:Uploading results into container
2025-01-09 10:24:36,283:INFO:Uploading model into container now
2025-01-09 10:24:36,284:INFO:_master_model_container: 15
2025-01-09 10:24:36,284:INFO:_display_container: 2
2025-01-09 10:24:36,284:INFO:DummyClassifier(constant=None, random_state=5001, strategy='prior')
2025-01-09 10:24:36,284:INFO:create_model() successfully completed......................................
2025-01-09 10:24:36,359:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:36,359:INFO:Creating metrics dataframe
2025-01-09 10:24:36,367:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-01-09 10:24:36,372:INFO:Initializing create_model()
2025-01-09 10:24:36,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:36,373:INFO:Checking exceptions
2025-01-09 10:24:36,373:INFO:Importing libraries
2025-01-09 10:24:36,373:INFO:Copying training dataset
2025-01-09 10:24:36,375:INFO:Defining folds
2025-01-09 10:24:36,375:INFO:Declaring metric variables
2025-01-09 10:24:36,375:INFO:Importing untrained model
2025-01-09 10:24:36,375:INFO:Declaring custom model
2025-01-09 10:24:36,376:INFO:Gradient Boosting Classifier Imported successfully
2025-01-09 10:24:36,377:INFO:Cross validation set to False
2025-01-09 10:24:36,377:INFO:Fitting Model
2025-01-09 10:24:36,466:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:24:36,466:INFO:create_model() successfully completed......................................
2025-01-09 10:24:36,556:INFO:_master_model_container: 15
2025-01-09 10:24:36,556:INFO:_display_container: 2
2025-01-09 10:24:36,556:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:24:36,556:INFO:compare_models() successfully completed......................................
2025-01-09 10:24:43,620:INFO:Initializing compare_models()
2025-01-09 10:24:43,620:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-01-09 10:24:43,620:INFO:Checking exceptions
2025-01-09 10:24:43,621:INFO:Preparing display monitor
2025-01-09 10:24:43,635:INFO:Initializing Logistic Regression
2025-01-09 10:24:43,635:INFO:Total runtime is 0.0 minutes
2025-01-09 10:24:43,637:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:43,637:INFO:Initializing create_model()
2025-01-09 10:24:43,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:43,638:INFO:Checking exceptions
2025-01-09 10:24:43,638:INFO:Importing libraries
2025-01-09 10:24:43,638:INFO:Copying training dataset
2025-01-09 10:24:43,640:INFO:Defining folds
2025-01-09 10:24:43,640:INFO:Declaring metric variables
2025-01-09 10:24:43,642:INFO:Importing untrained model
2025-01-09 10:24:43,645:INFO:Logistic Regression Imported successfully
2025-01-09 10:24:43,649:INFO:Starting cross validation
2025-01-09 10:24:43,650:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:43,763:INFO:Calculating mean and std
2025-01-09 10:24:43,763:INFO:Creating metrics dataframe
2025-01-09 10:24:43,764:INFO:Uploading results into container
2025-01-09 10:24:43,764:INFO:Uploading model into container now
2025-01-09 10:24:43,765:INFO:_master_model_container: 16
2025-01-09 10:24:43,765:INFO:_display_container: 3
2025-01-09 10:24:43,765:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5001, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-01-09 10:24:43,765:INFO:create_model() successfully completed......................................
2025-01-09 10:24:43,841:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:43,841:INFO:Creating metrics dataframe
2025-01-09 10:24:43,845:INFO:Initializing K Neighbors Classifier
2025-01-09 10:24:43,845:INFO:Total runtime is 0.0034986098607381185 minutes
2025-01-09 10:24:43,847:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:43,848:INFO:Initializing create_model()
2025-01-09 10:24:43,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:43,848:INFO:Checking exceptions
2025-01-09 10:24:43,848:INFO:Importing libraries
2025-01-09 10:24:43,848:INFO:Copying training dataset
2025-01-09 10:24:43,849:INFO:Defining folds
2025-01-09 10:24:43,849:INFO:Declaring metric variables
2025-01-09 10:24:43,851:INFO:Importing untrained model
2025-01-09 10:24:43,854:INFO:K Neighbors Classifier Imported successfully
2025-01-09 10:24:43,858:INFO:Starting cross validation
2025-01-09 10:24:43,858:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:43,979:INFO:Calculating mean and std
2025-01-09 10:24:43,980:INFO:Creating metrics dataframe
2025-01-09 10:24:43,981:INFO:Uploading results into container
2025-01-09 10:24:43,981:INFO:Uploading model into container now
2025-01-09 10:24:43,981:INFO:_master_model_container: 17
2025-01-09 10:24:43,981:INFO:_display_container: 3
2025-01-09 10:24:43,981:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-01-09 10:24:43,981:INFO:create_model() successfully completed......................................
2025-01-09 10:24:44,054:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:44,054:INFO:Creating metrics dataframe
2025-01-09 10:24:44,058:INFO:Initializing Naive Bayes
2025-01-09 10:24:44,058:INFO:Total runtime is 0.0070503950119018555 minutes
2025-01-09 10:24:44,062:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:44,062:INFO:Initializing create_model()
2025-01-09 10:24:44,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:44,062:INFO:Checking exceptions
2025-01-09 10:24:44,063:INFO:Importing libraries
2025-01-09 10:24:44,063:INFO:Copying training dataset
2025-01-09 10:24:44,064:INFO:Defining folds
2025-01-09 10:24:44,064:INFO:Declaring metric variables
2025-01-09 10:24:44,066:INFO:Importing untrained model
2025-01-09 10:24:44,068:INFO:Naive Bayes Imported successfully
2025-01-09 10:24:44,072:INFO:Starting cross validation
2025-01-09 10:24:44,074:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:44,153:INFO:Calculating mean and std
2025-01-09 10:24:44,153:INFO:Creating metrics dataframe
2025-01-09 10:24:44,154:INFO:Uploading results into container
2025-01-09 10:24:44,154:INFO:Uploading model into container now
2025-01-09 10:24:44,155:INFO:_master_model_container: 18
2025-01-09 10:24:44,155:INFO:_display_container: 3
2025-01-09 10:24:44,155:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-01-09 10:24:44,155:INFO:create_model() successfully completed......................................
2025-01-09 10:24:44,228:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:44,229:INFO:Creating metrics dataframe
2025-01-09 10:24:44,233:INFO:Initializing Decision Tree Classifier
2025-01-09 10:24:44,233:INFO:Total runtime is 0.009969532489776611 minutes
2025-01-09 10:24:44,235:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:44,235:INFO:Initializing create_model()
2025-01-09 10:24:44,235:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:44,235:INFO:Checking exceptions
2025-01-09 10:24:44,235:INFO:Importing libraries
2025-01-09 10:24:44,235:INFO:Copying training dataset
2025-01-09 10:24:44,238:INFO:Defining folds
2025-01-09 10:24:44,238:INFO:Declaring metric variables
2025-01-09 10:24:44,239:INFO:Importing untrained model
2025-01-09 10:24:44,242:INFO:Decision Tree Classifier Imported successfully
2025-01-09 10:24:44,247:INFO:Starting cross validation
2025-01-09 10:24:44,247:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:44,326:INFO:Calculating mean and std
2025-01-09 10:24:44,326:INFO:Creating metrics dataframe
2025-01-09 10:24:44,328:INFO:Uploading results into container
2025-01-09 10:24:44,328:INFO:Uploading model into container now
2025-01-09 10:24:44,328:INFO:_master_model_container: 19
2025-01-09 10:24:44,328:INFO:_display_container: 3
2025-01-09 10:24:44,329:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5001, splitter='best')
2025-01-09 10:24:44,329:INFO:create_model() successfully completed......................................
2025-01-09 10:24:44,402:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:44,403:INFO:Creating metrics dataframe
2025-01-09 10:24:44,408:INFO:Initializing SVM - Linear Kernel
2025-01-09 10:24:44,408:INFO:Total runtime is 0.012873812516530355 minutes
2025-01-09 10:24:44,411:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:44,412:INFO:Initializing create_model()
2025-01-09 10:24:44,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:44,412:INFO:Checking exceptions
2025-01-09 10:24:44,412:INFO:Importing libraries
2025-01-09 10:24:44,412:INFO:Copying training dataset
2025-01-09 10:24:44,414:INFO:Defining folds
2025-01-09 10:24:44,414:INFO:Declaring metric variables
2025-01-09 10:24:44,415:INFO:Importing untrained model
2025-01-09 10:24:44,418:INFO:SVM - Linear Kernel Imported successfully
2025-01-09 10:24:44,422:INFO:Starting cross validation
2025-01-09 10:24:44,423:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:44,486:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:44,486:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:44,504:INFO:Calculating mean and std
2025-01-09 10:24:44,504:INFO:Creating metrics dataframe
2025-01-09 10:24:44,505:INFO:Uploading results into container
2025-01-09 10:24:44,505:INFO:Uploading model into container now
2025-01-09 10:24:44,505:INFO:_master_model_container: 20
2025-01-09 10:24:44,505:INFO:_display_container: 3
2025-01-09 10:24:44,506:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5001, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-01-09 10:24:44,506:INFO:create_model() successfully completed......................................
2025-01-09 10:24:44,577:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:44,577:INFO:Creating metrics dataframe
2025-01-09 10:24:44,583:INFO:Initializing Ridge Classifier
2025-01-09 10:24:44,583:INFO:Total runtime is 0.015800213813781737 minutes
2025-01-09 10:24:44,585:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:44,585:INFO:Initializing create_model()
2025-01-09 10:24:44,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:44,585:INFO:Checking exceptions
2025-01-09 10:24:44,585:INFO:Importing libraries
2025-01-09 10:24:44,585:INFO:Copying training dataset
2025-01-09 10:24:44,587:INFO:Defining folds
2025-01-09 10:24:44,587:INFO:Declaring metric variables
2025-01-09 10:24:44,589:INFO:Importing untrained model
2025-01-09 10:24:44,591:INFO:Ridge Classifier Imported successfully
2025-01-09 10:24:44,595:INFO:Starting cross validation
2025-01-09 10:24:44,596:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:44,666:INFO:Calculating mean and std
2025-01-09 10:24:44,667:INFO:Creating metrics dataframe
2025-01-09 10:24:44,668:INFO:Uploading results into container
2025-01-09 10:24:44,668:INFO:Uploading model into container now
2025-01-09 10:24:44,669:INFO:_master_model_container: 21
2025-01-09 10:24:44,669:INFO:_display_container: 3
2025-01-09 10:24:44,669:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5001, solver='auto',
                tol=0.0001)
2025-01-09 10:24:44,669:INFO:create_model() successfully completed......................................
2025-01-09 10:24:44,742:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:44,742:INFO:Creating metrics dataframe
2025-01-09 10:24:44,748:INFO:Initializing Random Forest Classifier
2025-01-09 10:24:44,748:INFO:Total runtime is 0.018546565373738604 minutes
2025-01-09 10:24:44,750:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:44,750:INFO:Initializing create_model()
2025-01-09 10:24:44,750:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:44,750:INFO:Checking exceptions
2025-01-09 10:24:44,750:INFO:Importing libraries
2025-01-09 10:24:44,750:INFO:Copying training dataset
2025-01-09 10:24:44,754:INFO:Defining folds
2025-01-09 10:24:44,754:INFO:Declaring metric variables
2025-01-09 10:24:44,756:INFO:Importing untrained model
2025-01-09 10:24:44,758:INFO:Random Forest Classifier Imported successfully
2025-01-09 10:24:44,762:INFO:Starting cross validation
2025-01-09 10:24:44,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:45,037:INFO:Calculating mean and std
2025-01-09 10:24:45,038:INFO:Creating metrics dataframe
2025-01-09 10:24:45,039:INFO:Uploading results into container
2025-01-09 10:24:45,039:INFO:Uploading model into container now
2025-01-09 10:24:45,040:INFO:_master_model_container: 22
2025-01-09 10:24:45,040:INFO:_display_container: 3
2025-01-09 10:24:45,040:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5001, verbose=0,
                       warm_start=False)
2025-01-09 10:24:45,040:INFO:create_model() successfully completed......................................
2025-01-09 10:24:45,113:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:45,113:INFO:Creating metrics dataframe
2025-01-09 10:24:45,119:INFO:Initializing Quadratic Discriminant Analysis
2025-01-09 10:24:45,119:INFO:Total runtime is 0.024729390939076737 minutes
2025-01-09 10:24:45,121:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:45,121:INFO:Initializing create_model()
2025-01-09 10:24:45,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:45,121:INFO:Checking exceptions
2025-01-09 10:24:45,121:INFO:Importing libraries
2025-01-09 10:24:45,121:INFO:Copying training dataset
2025-01-09 10:24:45,123:INFO:Defining folds
2025-01-09 10:24:45,123:INFO:Declaring metric variables
2025-01-09 10:24:45,125:INFO:Importing untrained model
2025-01-09 10:24:45,127:INFO:Quadratic Discriminant Analysis Imported successfully
2025-01-09 10:24:45,131:INFO:Starting cross validation
2025-01-09 10:24:45,131:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:45,164:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,167:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,168:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,169:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,169:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,170:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,174:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,174:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,175:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,177:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-01-09 10:24:45,198:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:45,211:INFO:Calculating mean and std
2025-01-09 10:24:45,212:INFO:Creating metrics dataframe
2025-01-09 10:24:45,213:INFO:Uploading results into container
2025-01-09 10:24:45,214:INFO:Uploading model into container now
2025-01-09 10:24:45,214:INFO:_master_model_container: 23
2025-01-09 10:24:45,214:INFO:_display_container: 3
2025-01-09 10:24:45,214:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-01-09 10:24:45,214:INFO:create_model() successfully completed......................................
2025-01-09 10:24:45,290:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:45,290:INFO:Creating metrics dataframe
2025-01-09 10:24:45,296:INFO:Initializing Ada Boost Classifier
2025-01-09 10:24:45,296:INFO:Total runtime is 0.027685332298278804 minutes
2025-01-09 10:24:45,298:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:45,298:INFO:Initializing create_model()
2025-01-09 10:24:45,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:45,298:INFO:Checking exceptions
2025-01-09 10:24:45,299:INFO:Importing libraries
2025-01-09 10:24:45,299:INFO:Copying training dataset
2025-01-09 10:24:45,303:INFO:Defining folds
2025-01-09 10:24:45,303:INFO:Declaring metric variables
2025-01-09 10:24:45,304:INFO:Importing untrained model
2025-01-09 10:24:45,306:INFO:Ada Boost Classifier Imported successfully
2025-01-09 10:24:45,310:INFO:Starting cross validation
2025-01-09 10:24:45,311:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:45,342:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,342:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,345:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,350:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,352:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,352:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,354:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,354:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,358:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,359:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-01-09 10:24:45,454:INFO:Calculating mean and std
2025-01-09 10:24:45,455:INFO:Creating metrics dataframe
2025-01-09 10:24:45,456:INFO:Uploading results into container
2025-01-09 10:24:45,456:INFO:Uploading model into container now
2025-01-09 10:24:45,457:INFO:_master_model_container: 24
2025-01-09 10:24:45,457:INFO:_display_container: 3
2025-01-09 10:24:45,457:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5001)
2025-01-09 10:24:45,457:INFO:create_model() successfully completed......................................
2025-01-09 10:24:45,527:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:45,527:INFO:Creating metrics dataframe
2025-01-09 10:24:45,534:INFO:Initializing Gradient Boosting Classifier
2025-01-09 10:24:45,534:INFO:Total runtime is 0.031649661064147946 minutes
2025-01-09 10:24:45,536:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:45,536:INFO:Initializing create_model()
2025-01-09 10:24:45,536:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:45,536:INFO:Checking exceptions
2025-01-09 10:24:45,536:INFO:Importing libraries
2025-01-09 10:24:45,536:INFO:Copying training dataset
2025-01-09 10:24:45,538:INFO:Defining folds
2025-01-09 10:24:45,538:INFO:Declaring metric variables
2025-01-09 10:24:45,540:INFO:Importing untrained model
2025-01-09 10:24:45,542:INFO:Gradient Boosting Classifier Imported successfully
2025-01-09 10:24:45,547:INFO:Starting cross validation
2025-01-09 10:24:45,548:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:45,723:INFO:Calculating mean and std
2025-01-09 10:24:45,723:INFO:Creating metrics dataframe
2025-01-09 10:24:45,724:INFO:Uploading results into container
2025-01-09 10:24:45,725:INFO:Uploading model into container now
2025-01-09 10:24:45,725:INFO:_master_model_container: 25
2025-01-09 10:24:45,725:INFO:_display_container: 3
2025-01-09 10:24:45,725:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:24:45,725:INFO:create_model() successfully completed......................................
2025-01-09 10:24:45,802:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:45,802:INFO:Creating metrics dataframe
2025-01-09 10:24:45,806:INFO:Initializing Linear Discriminant Analysis
2025-01-09 10:24:45,807:INFO:Total runtime is 0.03619653383890788 minutes
2025-01-09 10:24:45,809:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:45,809:INFO:Initializing create_model()
2025-01-09 10:24:45,809:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:45,809:INFO:Checking exceptions
2025-01-09 10:24:45,809:INFO:Importing libraries
2025-01-09 10:24:45,809:INFO:Copying training dataset
2025-01-09 10:24:45,812:INFO:Defining folds
2025-01-09 10:24:45,812:INFO:Declaring metric variables
2025-01-09 10:24:45,814:INFO:Importing untrained model
2025-01-09 10:24:45,816:INFO:Linear Discriminant Analysis Imported successfully
2025-01-09 10:24:45,819:INFO:Starting cross validation
2025-01-09 10:24:45,820:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:45,900:INFO:Calculating mean and std
2025-01-09 10:24:45,901:INFO:Creating metrics dataframe
2025-01-09 10:24:45,902:INFO:Uploading results into container
2025-01-09 10:24:45,903:INFO:Uploading model into container now
2025-01-09 10:24:45,903:INFO:_master_model_container: 26
2025-01-09 10:24:45,903:INFO:_display_container: 3
2025-01-09 10:24:45,903:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-01-09 10:24:45,903:INFO:create_model() successfully completed......................................
2025-01-09 10:24:45,978:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:45,978:INFO:Creating metrics dataframe
2025-01-09 10:24:45,985:INFO:Initializing Extra Trees Classifier
2025-01-09 10:24:45,985:INFO:Total runtime is 0.03915689388910929 minutes
2025-01-09 10:24:45,987:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:45,987:INFO:Initializing create_model()
2025-01-09 10:24:45,987:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:45,987:INFO:Checking exceptions
2025-01-09 10:24:45,987:INFO:Importing libraries
2025-01-09 10:24:45,987:INFO:Copying training dataset
2025-01-09 10:24:45,989:INFO:Defining folds
2025-01-09 10:24:45,989:INFO:Declaring metric variables
2025-01-09 10:24:45,991:INFO:Importing untrained model
2025-01-09 10:24:45,992:INFO:Extra Trees Classifier Imported successfully
2025-01-09 10:24:45,997:INFO:Starting cross validation
2025-01-09 10:24:45,997:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:46,241:INFO:Calculating mean and std
2025-01-09 10:24:46,242:INFO:Creating metrics dataframe
2025-01-09 10:24:46,242:INFO:Uploading results into container
2025-01-09 10:24:46,243:INFO:Uploading model into container now
2025-01-09 10:24:46,243:INFO:_master_model_container: 27
2025-01-09 10:24:46,243:INFO:_display_container: 3
2025-01-09 10:24:46,244:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5001, verbose=0,
                     warm_start=False)
2025-01-09 10:24:46,244:INFO:create_model() successfully completed......................................
2025-01-09 10:24:46,323:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:46,323:INFO:Creating metrics dataframe
2025-01-09 10:24:46,330:INFO:Initializing Extreme Gradient Boosting
2025-01-09 10:24:46,330:INFO:Total runtime is 0.044907999038696286 minutes
2025-01-09 10:24:46,332:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:46,332:INFO:Initializing create_model()
2025-01-09 10:24:46,332:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:46,332:INFO:Checking exceptions
2025-01-09 10:24:46,332:INFO:Importing libraries
2025-01-09 10:24:46,332:INFO:Copying training dataset
2025-01-09 10:24:46,334:INFO:Defining folds
2025-01-09 10:24:46,334:INFO:Declaring metric variables
2025-01-09 10:24:46,335:INFO:Importing untrained model
2025-01-09 10:24:46,337:INFO:Extreme Gradient Boosting Imported successfully
2025-01-09 10:24:46,341:INFO:Starting cross validation
2025-01-09 10:24:46,341:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:47,106:INFO:Calculating mean and std
2025-01-09 10:24:47,107:INFO:Creating metrics dataframe
2025-01-09 10:24:47,108:INFO:Uploading results into container
2025-01-09 10:24:47,108:INFO:Uploading model into container now
2025-01-09 10:24:47,109:INFO:_master_model_container: 28
2025-01-09 10:24:47,109:INFO:_display_container: 3
2025-01-09 10:24:47,109:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-01-09 10:24:47,110:INFO:create_model() successfully completed......................................
2025-01-09 10:24:47,186:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:47,186:INFO:Creating metrics dataframe
2025-01-09 10:24:47,191:INFO:Initializing Light Gradient Boosting Machine
2025-01-09 10:24:47,193:INFO:Total runtime is 0.059297025203704834 minutes
2025-01-09 10:24:47,194:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:47,195:INFO:Initializing create_model()
2025-01-09 10:24:47,195:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:47,195:INFO:Checking exceptions
2025-01-09 10:24:47,195:INFO:Importing libraries
2025-01-09 10:24:47,195:INFO:Copying training dataset
2025-01-09 10:24:47,197:INFO:Defining folds
2025-01-09 10:24:47,197:INFO:Declaring metric variables
2025-01-09 10:24:47,198:INFO:Importing untrained model
2025-01-09 10:24:47,200:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:24:47,204:INFO:Starting cross validation
2025-01-09 10:24:47,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:48,373:INFO:Calculating mean and std
2025-01-09 10:24:48,373:INFO:Creating metrics dataframe
2025-01-09 10:24:48,373:INFO:Uploading results into container
2025-01-09 10:24:48,376:INFO:Uploading model into container now
2025-01-09 10:24:48,377:INFO:_master_model_container: 29
2025-01-09 10:24:48,377:INFO:_display_container: 3
2025-01-09 10:24:48,377:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:24:48,377:INFO:create_model() successfully completed......................................
2025-01-09 10:24:48,466:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:48,466:INFO:Creating metrics dataframe
2025-01-09 10:24:48,472:INFO:Initializing Dummy Classifier
2025-01-09 10:24:48,472:INFO:Total runtime is 0.08061701456705729 minutes
2025-01-09 10:24:48,475:INFO:SubProcess create_model() called ==================================
2025-01-09 10:24:48,475:INFO:Initializing create_model()
2025-01-09 10:24:48,475:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F15473890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:48,475:INFO:Checking exceptions
2025-01-09 10:24:48,475:INFO:Importing libraries
2025-01-09 10:24:48,475:INFO:Copying training dataset
2025-01-09 10:24:48,477:INFO:Defining folds
2025-01-09 10:24:48,477:INFO:Declaring metric variables
2025-01-09 10:24:48,479:INFO:Importing untrained model
2025-01-09 10:24:48,482:INFO:Dummy Classifier Imported successfully
2025-01-09 10:24:48,486:INFO:Starting cross validation
2025-01-09 10:24:48,486:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:24:48,529:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,530:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,539:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,542:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,543:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,544:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,545:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,548:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,549:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,550:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-01-09 10:24:48,554:INFO:Calculating mean and std
2025-01-09 10:24:48,554:INFO:Creating metrics dataframe
2025-01-09 10:24:48,555:INFO:Uploading results into container
2025-01-09 10:24:48,555:INFO:Uploading model into container now
2025-01-09 10:24:48,556:INFO:_master_model_container: 30
2025-01-09 10:24:48,556:INFO:_display_container: 3
2025-01-09 10:24:48,556:INFO:DummyClassifier(constant=None, random_state=5001, strategy='prior')
2025-01-09 10:24:48,556:INFO:create_model() successfully completed......................................
2025-01-09 10:24:48,629:INFO:SubProcess create_model() end ==================================
2025-01-09 10:24:48,629:INFO:Creating metrics dataframe
2025-01-09 10:24:48,635:WARNING:c:\wanted\envs\ml_practice\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-01-09 10:24:48,642:INFO:Initializing create_model()
2025-01-09 10:24:48,642:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:24:48,642:INFO:Checking exceptions
2025-01-09 10:24:48,644:INFO:Importing libraries
2025-01-09 10:24:48,644:INFO:Copying training dataset
2025-01-09 10:24:48,646:INFO:Defining folds
2025-01-09 10:24:48,646:INFO:Declaring metric variables
2025-01-09 10:24:48,646:INFO:Importing untrained model
2025-01-09 10:24:48,646:INFO:Declaring custom model
2025-01-09 10:24:48,646:INFO:Gradient Boosting Classifier Imported successfully
2025-01-09 10:24:48,647:INFO:Cross validation set to False
2025-01-09 10:24:48,647:INFO:Fitting Model
2025-01-09 10:24:48,739:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:24:48,739:INFO:create_model() successfully completed......................................
2025-01-09 10:24:48,830:INFO:_master_model_container: 30
2025-01-09 10:24:48,830:INFO:_display_container: 3
2025-01-09 10:24:48,830:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5001, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-01-09 10:24:48,830:INFO:compare_models() successfully completed......................................
2025-01-09 10:36:42,134:INFO:Initializing create_model()
2025-01-09 10:36:42,134:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:36:42,134:INFO:Checking exceptions
2025-01-09 10:36:42,144:INFO:Importing libraries
2025-01-09 10:36:42,144:INFO:Copying training dataset
2025-01-09 10:36:42,147:INFO:Defining folds
2025-01-09 10:36:42,147:INFO:Declaring metric variables
2025-01-09 10:36:42,149:INFO:Importing untrained model
2025-01-09 10:36:42,152:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:36:42,157:INFO:Starting cross validation
2025-01-09 10:36:42,158:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:36:45,203:INFO:Calculating mean and std
2025-01-09 10:36:45,204:INFO:Creating metrics dataframe
2025-01-09 10:36:45,207:INFO:Finalizing model
2025-01-09 10:36:45,237:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2025-01-09 10:36:45,238:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000381 seconds.
2025-01-09 10:36:45,238:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-01-09 10:36:45,238:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-01-09 10:36:45,238:INFO:[LightGBM] [Info] Total Bins 188
2025-01-09 10:36:45,238:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 9
2025-01-09 10:36:45,238:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2025-01-09 10:36:45,238:INFO:[LightGBM] [Info] Start training from score -0.474179
2025-01-09 10:36:45,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:45,429:INFO:Uploading results into container
2025-01-09 10:36:45,430:INFO:Uploading model into container now
2025-01-09 10:36:45,437:INFO:_master_model_container: 31
2025-01-09 10:36:45,437:INFO:_display_container: 4
2025-01-09 10:36:45,438:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:36:45,438:INFO:create_model() successfully completed......................................
2025-01-09 10:36:45,537:INFO:Initializing evaluate_model()
2025-01-09 10:36:45,537:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-01-09 10:36:45,545:INFO:Initializing plot_model()
2025-01-09 10:36:45,545:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2025-01-09 10:36:45,545:INFO:Checking exceptions
2025-01-09 10:36:45,546:INFO:Preloading libraries
2025-01-09 10:36:45,552:INFO:Copying training dataset
2025-01-09 10:36:45,552:INFO:Plot type: pipeline
2025-01-09 10:36:45,716:INFO:Visual Rendered Successfully
2025-01-09 10:36:45,794:INFO:plot_model() successfully completed......................................
2025-01-09 10:36:51,217:INFO:Initializing tune_model()
2025-01-09 10:36:51,217:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-01-09 10:36:51,217:INFO:Checking exceptions
2025-01-09 10:36:51,229:INFO:Copying training dataset
2025-01-09 10:36:51,232:INFO:Checking base model
2025-01-09 10:36:51,232:INFO:Base model : Light Gradient Boosting Machine
2025-01-09 10:36:51,234:INFO:Declaring metric variables
2025-01-09 10:36:51,237:INFO:Defining Hyperparameters
2025-01-09 10:36:51,337:INFO:Tuning with n_jobs=-1
2025-01-09 10:36:51,338:INFO:Initializing RandomizedSearchCV
2025-01-09 10:36:58,385:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 1, 'actual_estimator__num_leaves': 80, 'actual_estimator__n_estimators': 180, 'actual_estimator__min_split_gain': 0.2, 'actual_estimator__min_child_samples': 1, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 0.9, 'actual_estimator__bagging_freq': 7, 'actual_estimator__bagging_fraction': 1.0}
2025-01-09 10:36:58,386:INFO:Hyperparameter search completed
2025-01-09 10:36:58,386:INFO:SubProcess create_model() called ==================================
2025-01-09 10:36:58,387:INFO:Initializing create_model()
2025-01-09 10:36:58,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F13EFFC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 1, 'num_leaves': 80, 'n_estimators': 180, 'min_split_gain': 0.2, 'min_child_samples': 1, 'learning_rate': 0.05, 'feature_fraction': 0.9, 'bagging_freq': 7, 'bagging_fraction': 1.0})
2025-01-09 10:36:58,387:INFO:Checking exceptions
2025-01-09 10:36:58,387:INFO:Importing libraries
2025-01-09 10:36:58,387:INFO:Copying training dataset
2025-01-09 10:36:58,390:INFO:Defining folds
2025-01-09 10:36:58,390:INFO:Declaring metric variables
2025-01-09 10:36:58,392:INFO:Importing untrained model
2025-01-09 10:36:58,392:INFO:Declaring custom model
2025-01-09 10:36:58,395:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:36:58,399:INFO:Starting cross validation
2025-01-09 10:36:58,400:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:36:59,320:INFO:Calculating mean and std
2025-01-09 10:36:59,320:INFO:Creating metrics dataframe
2025-01-09 10:36:59,325:INFO:Finalizing model
2025-01-09 10:36:59,349:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2025-01-09 10:36:59,349:INFO:[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0
2025-01-09 10:36:59,350:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2025-01-09 10:36:59,351:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2025-01-09 10:36:59,351:INFO:[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0
2025-01-09 10:36:59,351:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2025-01-09 10:36:59,351:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2025-01-09 10:36:59,352:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2025-01-09 10:36:59,352:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-01-09 10:36:59,352:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-01-09 10:36:59,352:INFO:[LightGBM] [Info] Total Bins 188
2025-01-09 10:36:59,352:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 9
2025-01-09 10:36:59,352:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2025-01-09 10:36:59,352:INFO:[LightGBM] [Info] Start training from score -0.474179
2025-01-09 10:36:59,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,500:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,504:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,511:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:36:59,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:36:59,520:INFO:Uploading results into container
2025-01-09 10:36:59,522:INFO:Uploading model into container now
2025-01-09 10:36:59,522:INFO:_master_model_container: 32
2025-01-09 10:36:59,522:INFO:_display_container: 5
2025-01-09 10:36:59,522:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:36:59,522:INFO:create_model() successfully completed......................................
2025-01-09 10:36:59,612:INFO:SubProcess create_model() end ==================================
2025-01-09 10:36:59,612:INFO:choose_better activated
2025-01-09 10:36:59,614:INFO:SubProcess create_model() called ==================================
2025-01-09 10:36:59,614:INFO:Initializing create_model()
2025-01-09 10:36:59,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:36:59,614:INFO:Checking exceptions
2025-01-09 10:36:59,616:INFO:Importing libraries
2025-01-09 10:36:59,616:INFO:Copying training dataset
2025-01-09 10:36:59,618:INFO:Defining folds
2025-01-09 10:36:59,618:INFO:Declaring metric variables
2025-01-09 10:36:59,618:INFO:Importing untrained model
2025-01-09 10:36:59,618:INFO:Declaring custom model
2025-01-09 10:36:59,619:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:36:59,619:INFO:Starting cross validation
2025-01-09 10:36:59,620:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:37:00,654:INFO:Calculating mean and std
2025-01-09 10:37:00,655:INFO:Creating metrics dataframe
2025-01-09 10:37:00,655:INFO:Finalizing model
2025-01-09 10:37:00,685:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2025-01-09 10:37:00,686:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000428 seconds.
2025-01-09 10:37:00,686:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-01-09 10:37:00,686:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-01-09 10:37:00,686:INFO:[LightGBM] [Info] Total Bins 188
2025-01-09 10:37:00,686:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 9
2025-01-09 10:37:00,687:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2025-01-09 10:37:00,687:INFO:[LightGBM] [Info] Start training from score -0.474179
2025-01-09 10:37:00,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:00,848:INFO:Uploading results into container
2025-01-09 10:37:00,848:INFO:Uploading model into container now
2025-01-09 10:37:00,849:INFO:_master_model_container: 33
2025-01-09 10:37:00,849:INFO:_display_container: 6
2025-01-09 10:37:00,849:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:37:00,849:INFO:create_model() successfully completed......................................
2025-01-09 10:37:00,929:INFO:SubProcess create_model() end ==================================
2025-01-09 10:37:00,929:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.7913
2025-01-09 10:37:00,930:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8186
2025-01-09 10:37:00,930:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2025-01-09 10:37:00,930:INFO:choose_better completed
2025-01-09 10:37:00,936:INFO:_master_model_container: 33
2025-01-09 10:37:00,936:INFO:_display_container: 5
2025-01-09 10:37:00,936:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:37:00,936:INFO:tune_model() successfully completed......................................
2025-01-09 10:37:17,812:INFO:Initializing tune_model()
2025-01-09 10:37:17,812:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-01-09 10:37:17,812:INFO:Checking exceptions
2025-01-09 10:37:17,822:INFO:Copying training dataset
2025-01-09 10:37:17,825:INFO:Checking base model
2025-01-09 10:37:17,825:INFO:Base model : Light Gradient Boosting Machine
2025-01-09 10:37:17,827:INFO:Declaring metric variables
2025-01-09 10:37:17,829:INFO:Defining Hyperparameters
2025-01-09 10:37:17,908:INFO:Tuning with n_jobs=-1
2025-01-09 10:37:17,908:INFO:Initializing RandomizedSearchCV
2025-01-09 10:37:22,294:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 1, 'actual_estimator__num_leaves': 80, 'actual_estimator__n_estimators': 180, 'actual_estimator__min_split_gain': 0.2, 'actual_estimator__min_child_samples': 1, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 0.9, 'actual_estimator__bagging_freq': 7, 'actual_estimator__bagging_fraction': 1.0}
2025-01-09 10:37:22,296:INFO:Hyperparameter search completed
2025-01-09 10:37:22,296:INFO:SubProcess create_model() called ==================================
2025-01-09 10:37:22,297:INFO:Initializing create_model()
2025-01-09 10:37:22,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018F155DA9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 1, 'num_leaves': 80, 'n_estimators': 180, 'min_split_gain': 0.2, 'min_child_samples': 1, 'learning_rate': 0.05, 'feature_fraction': 0.9, 'bagging_freq': 7, 'bagging_fraction': 1.0})
2025-01-09 10:37:22,297:INFO:Checking exceptions
2025-01-09 10:37:22,297:INFO:Importing libraries
2025-01-09 10:37:22,297:INFO:Copying training dataset
2025-01-09 10:37:22,300:INFO:Defining folds
2025-01-09 10:37:22,300:INFO:Declaring metric variables
2025-01-09 10:37:22,303:INFO:Importing untrained model
2025-01-09 10:37:22,304:INFO:Declaring custom model
2025-01-09 10:37:22,306:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:37:22,311:INFO:Starting cross validation
2025-01-09 10:37:22,313:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:37:23,253:INFO:Calculating mean and std
2025-01-09 10:37:23,255:INFO:Creating metrics dataframe
2025-01-09 10:37:23,259:INFO:Finalizing model
2025-01-09 10:37:23,285:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2025-01-09 10:37:23,285:INFO:[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0
2025-01-09 10:37:23,285:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2025-01-09 10:37:23,286:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2025-01-09 10:37:23,286:INFO:[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0
2025-01-09 10:37:23,286:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2025-01-09 10:37:23,286:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2025-01-09 10:37:23,287:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.
2025-01-09 10:37:23,287:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-01-09 10:37:23,287:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-01-09 10:37:23,287:INFO:[LightGBM] [Info] Total Bins 188
2025-01-09 10:37:23,288:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 9
2025-01-09 10:37:23,288:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2025-01-09 10:37:23,288:INFO:[LightGBM] [Info] Start training from score -0.474179
2025-01-09 10:37:23,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:23,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-01-09 10:37:23,463:INFO:Uploading results into container
2025-01-09 10:37:23,464:INFO:Uploading model into container now
2025-01-09 10:37:23,464:INFO:_master_model_container: 34
2025-01-09 10:37:23,464:INFO:_display_container: 6
2025-01-09 10:37:23,465:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:37:23,465:INFO:create_model() successfully completed......................................
2025-01-09 10:37:23,552:INFO:SubProcess create_model() end ==================================
2025-01-09 10:37:23,552:INFO:choose_better activated
2025-01-09 10:37:23,555:INFO:SubProcess create_model() called ==================================
2025-01-09 10:37:23,555:INFO:Initializing create_model()
2025-01-09 10:37:23,555:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000018F13C17CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-01-09 10:37:23,555:INFO:Checking exceptions
2025-01-09 10:37:23,557:INFO:Importing libraries
2025-01-09 10:37:23,557:INFO:Copying training dataset
2025-01-09 10:37:23,559:INFO:Defining folds
2025-01-09 10:37:23,559:INFO:Declaring metric variables
2025-01-09 10:37:23,560:INFO:Importing untrained model
2025-01-09 10:37:23,560:INFO:Declaring custom model
2025-01-09 10:37:23,560:INFO:Light Gradient Boosting Machine Imported successfully
2025-01-09 10:37:23,560:INFO:Starting cross validation
2025-01-09 10:37:23,561:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-01-09 10:37:24,623:INFO:Calculating mean and std
2025-01-09 10:37:24,623:INFO:Creating metrics dataframe
2025-01-09 10:37:24,624:INFO:Finalizing model
2025-01-09 10:37:24,650:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2025-01-09 10:37:24,650:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000362 seconds.
2025-01-09 10:37:24,650:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-01-09 10:37:24,650:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-01-09 10:37:24,650:INFO:[LightGBM] [Info] Total Bins 188
2025-01-09 10:37:24,651:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 9
2025-01-09 10:37:24,651:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2025-01-09 10:37:24,651:INFO:[LightGBM] [Info] Start training from score -0.474179
2025-01-09 10:37:24,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-01-09 10:37:24,834:INFO:Uploading results into container
2025-01-09 10:37:24,835:INFO:Uploading model into container now
2025-01-09 10:37:24,835:INFO:_master_model_container: 35
2025-01-09 10:37:24,835:INFO:_display_container: 7
2025-01-09 10:37:24,835:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:37:24,835:INFO:create_model() successfully completed......................................
2025-01-09 10:37:24,944:INFO:SubProcess create_model() end ==================================
2025-01-09 10:37:24,944:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5001, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.7913
2025-01-09 10:37:24,945:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8186
2025-01-09 10:37:24,945:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2025-01-09 10:37:24,945:INFO:choose_better completed
2025-01-09 10:37:24,951:INFO:_master_model_container: 35
2025-01-09 10:37:24,951:INFO:_display_container: 6
2025-01-09 10:37:24,951:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-01-09 10:37:24,951:INFO:tune_model() successfully completed......................................
2025-01-09 10:38:05,495:INFO:Initializing save_model()
2025-01-09 10:38:05,495:INFO:save_model(model=LGBMClassifier(bagging_fraction=1.0, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=1, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=5001, reg_alpha=1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=tuned_lgbm_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Age', 'SibSp', 'Parch',
                                             'Fare'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 Transf...
                                                               mapping=[{'col': 'Sex',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None, include=['Embarked'],
                                    transformer=OneHotEncoder(cols=['Embarked'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-01-09 10:38:05,495:INFO:Adding model into prep_pipe
2025-01-09 10:38:05,504:INFO:tuned_lgbm_model.pkl saved in current working directory
2025-01-09 10:38:05,514:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Age', 'SibSp', 'Parch',
                                             'Fare'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=['Sex...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.9,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=1,
                                min_child_weight=0.001, min_split_gain=0.2,
                                n_estimators=180, n_jobs=-1, num_leaves=80,
                                objective=None, random_state=5001, reg_alpha=1,
                                reg_lambda=2, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-01-09 10:38:05,514:INFO:save_model() successfully completed......................................
2025-01-09 10:38:10,300:INFO:Initializing load_model()
2025-01-09 10:38:10,300:INFO:load_model(model_name=tuned_lgbm_model, platform=None, authentication=None, verbose=True)
