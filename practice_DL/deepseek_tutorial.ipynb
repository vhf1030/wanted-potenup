{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-ai/DeepSeek-R1\n",
      "deepseek-ai/Janus-Pro-7B\n",
      "deepseek-ai/DeepSeek-V3\n",
      "unsloth/DeepSeek-R1-GGUF\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "deepseek-ai/deepseek-vl2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# API ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "api = HfApi()\n",
    "\n",
    "# ëª¨ë¸ ê²€ìƒ‰ (ì˜ˆ: 'deepseek' ê´€ë ¨ ëª¨ë¸ ì°¾ê¸°)\n",
    "models = api.list_models(search=\"deepseek\", limit=10)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for model in models:\n",
    "    print(model.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "# # model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "# $ huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir ./models/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "\n",
    "# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì§€ì •\n",
    "local_model_path = \"../models/\"\n",
    "model_name = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# ë¡œì»¬ì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path + model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path + model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's weather is going to be tricky. You have a weather app that you need to check. You have a weather app that you need to check. You have a weather app that you need to check. You have a weather app that you\n"
     ]
    }
   ],
   "source": [
    "# ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "prompt = \"Today's weather is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # ì…ë ¥ ë°ì´í„°ë„ GPUë¡œ ì´ë™\n",
    "\n",
    "# ëª¨ë¸ì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=50,  # ìƒì„±í•  ìµœëŒ€ í† í° ê¸¸ì´\n",
    "        num_return_sequences=1,  # ìƒì„±í•  ë¬¸ì¥ ìˆ˜\n",
    "        do_sample=True,  # ìƒ˜í”Œë§ ì—¬ë¶€\n",
    "        top_p=0.95,  # nucleus sampling\n",
    "        top_k=50     # top-k ìƒ˜í”Œë§\n",
    "    )\n",
    "\n",
    "# ê²°ê³¼ ë””ì½”ë”© ë° ì¶œë ¥\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Chatbot: Tell me a joke about AI. It needs to be surprising and have something to do with AI. Maybe it's about how AI is changing jobs? Or something like that.\n",
      "\n",
      "Okay, I'm thinking about something funny. Maybe a joke that plays on the idea that AI is moving jobs around, but the punchline is that AI is so good at it that it doesn't need the jobs anymore. Or maybe it's about how AI is just as human as humans and is constantly evolving. Or\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì…ë ¥ ì˜ˆì‹œ\n",
    "user_input = \"Tell me a joke about AI.\"\n",
    "\n",
    "# ì…ë ¥ì„ í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=100,  # ë‹µë³€ ê¸¸ì´ ì œí•œ\n",
    "        temperature=0.8,  # ë‹¤ì–‘ì„± ì¡°ì ˆ\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "# ì‘ë‹µ ì¶œë ¥\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"ğŸ¤– Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ Review Summary: \n",
      "This phone has a great camera and an amazing battery life. \n",
      "However, the screen sometimes flickers, which is quite annoying. \n",
      "Overall, it is a good purchase for the price.\n",
      "But the problem is that the battery life is not\n"
     ]
    }
   ],
   "source": [
    "# ê¸´ ë¦¬ë·° ì…ë ¥\n",
    "review_text = \"\"\"\n",
    "This phone has a great camera and an amazing battery life. \n",
    "However, the screen sometimes flickers, which is quite annoying. \n",
    "Overall, it is a good purchase for the price.\n",
    "\"\"\"\n",
    "\n",
    "# ì…ë ¥ì„ í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(review_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "# ìš”ì•½ëœ í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"ğŸ“¢ Review Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‡°ğŸ‡· Translated: Translate this to Korean: The restaurant serves delicious pasta with fresh ingredients. The chef is really talented. The food is prepared with traditional techniques.\n",
      "\n",
      "Okay, I have a problem with this. The problem is that the restaurant is so busy, and I can't get a seat. I need to figure out a way to get a table, but I don't know the restaurant's seating arrangements.\n",
      "\n",
      "The manager is a bit unprofessional. I don't think they pay attention to the customer's needs.\n"
     ]
    }
   ],
   "source": [
    "# ì˜ì–´ ë¬¸ì¥ ì…ë ¥\n",
    "english_text = \"The restaurant serves delicious pasta with fresh ingredients.\"\n",
    "\n",
    "# ì…ë ¥ì„ í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(\"Translate this to Korean: \" + english_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ëª¨ë¸ë¡œ ë²ˆì—­ ìƒì„±\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "# ë²ˆì—­ëœ ê²°ê³¼ ì¶œë ¥\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"ğŸ‡°ğŸ‡· Translated:\", translated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
