딥러닝 학습
epoch, learning rate
손실함수: 모델 생성 단계(ex. MSE, cross entropy error)
최적화함수 - (ex. 경사하강법(GD), Adam)
활성함수 - 

Train / Test
Train / Validation - 과적합 방지를 위해 가중치 업데이트에 사용하지 않는 데이터
batch: 효율적인 연산을 위해 데이터를 쪼개는 단위

epoch
1. torch.no_grad(): 손실함수의 미분값 초기화
2. model(x): yhat 계산 (순전파)
3. loss 계산
4. loss.backward(): loss의 미분값 계산
5. optimizer.step(): 가중치 업데이트


### FAST API

fastapi, postman, ngrok